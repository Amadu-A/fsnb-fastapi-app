{
  "meta": {
    "generated_at_utc": "2025-12-18T07:58:12Z",
    "root": "/home/main/PycharmProjects/fsnb-fastapi-app",
    "tool": "dump_project.py",
    "max_file_bytes": 500000,
    "total_files": 107
  },
  "context": {
    "project_description": "Краткое описание проекта, архитектура, стек, цели.",
    "static_instructions": [
      "Это команды для обсуждения, rewie, улучшения, оптимизации, модернизации и исправления кода для распознавания pdf-файлов с помощью парсинга и обучения YOLO модели и подготовке на основе этих данных отчетов в excel.",
      "Проанализируй весь код проекта, разберись, что и с чем связано.",
      "Каждый файл, который ты подготовишь, должен быть заполнен.",
      "каждый кусок кода , который ты покажешь должен быть строго указан к какому файлу он принадлежит.",
      "Имена переменных и функций — PEP8.",
      "Писать код с упором на низкое потребление памяти и работу с большими данными.",
      "Единый кастомный JSON-логгер через LoggerAdapter (время, уровень, имя функции, сообщение).",
      "Документировать функции и ключевые участки кода.",
      "Все изменения кода подписывай определенным файлом, в котором мы делаем изменения.",
      "Не приводи допущений, где я сам должен что-то понять и довести дело до конца.",
      "Объясни каждую строку кода.",
      "Предлагай улучшения таким образом, чтобы не порушить существующую логику, которая уже работает."
    ],
    "current_objectives": []
  },
  "structure_text": "./\n  .dockerignore\n  .env.examlpe\n  .gitignore\n  Dockerfile\n  Readme.md\n  __init__.py\n  alembic.ini\n  docker-compose.yml\n  dump_project.py\n  poetry.lock\n  pyproject.toml\n  static/\n    static/js/\n      app.js\n      avatar-preview.js\n      fsnb_matcher.js\n    static/css/\n      style.css\n    static/uploads/\n      static/uploads/avatars/\n    static/img/\n  FSNB-2022_28_08_25/\n    База ТГ.xml\n    ГЭСН.xml\n    ГЭСНм.xml\n    ГЭСНмр.xml\n    ГЭСНп.xml\n    ГЭСНр.xml\n    Ключи перехода ТГ.xml\n    ФСБЦ_Мат&Оборуд.xml\n    ФСБЦ_Маш.xml\n  weights/\n    weights/Giga-Embeddings-instruct/\n      .gitattributes\n      README.md\n      config.json\n      config_sentence_transformers.json\n      configuration_gigarembed.py\n      model-00001-of-00003.safetensors\n      model-00002-of-00003.safetensors\n      model-00003-of-00003.safetensors\n      model.safetensors.index.json\n      modeling_gigarembed.py\n      modules.json\n      sentence_bert_config.json\n      special_tokens_map.json\n      tokenizer.json\n      tokenizer_config.json\n      weights/Giga-Embeddings-instruct/1_Pooling/\n        config.json\n    weights/hf-cache/\n      weights/hf-cache/modules/\n        __init__.py\n        weights/hf-cache/modules/transformers_modules/\n          __init__.py\n          weights/hf-cache/modules/transformers_modules/Giga_hyphen_Embeddings_hyphen_instruct/\n            __init__.py\n            configuration_gigarembed.py\n            modeling_gigarembed.py\n  src/\n    __init__.py\n    admin.py\n    app_logging.py\n    main.py\n    manage.py\n    src/crud/\n      __init__.py\n      item_repository.py\n      permission_repository.py\n      profile_repository.py\n      user_repository.py\n    src/core/\n      __init__.py\n      config.py\n      dependencies.py\n      email_tokens.py\n      security.py\n      src/core/utils/\n        __init__.py\n        case_converter.py\n      src/core/schemas/\n        __init__.py\n        permission.py\n        profile.py\n        user.py\n      src/core/mailing/\n        __init__.py\n        email.py\n      src/core/api/\n        __init__.py\n        src/core/api/api_v1/\n          __init__.py\n          auth.py\n          users.py\n      src/core/services/\n        __init__.py\n        auth_service.py\n      src/core/views/\n        __init__.py\n        admin.py\n        auth.py\n        web.py\n    src/fsnb_matcher/\n      __init__.py\n      src/fsnb_matcher/utils/\n        __init__.py\n      src/fsnb_matcher/schemas/\n        __init__.py\n        item.py\n      src/fsnb_matcher/api/\n        __init__.py\n        src/fsnb_matcher/api/api_v1/\n          __init__.py\n          match.py\n      src/fsnb_matcher/services/\n        __init__.py\n        fsnb_xml_parser.py\n        index_qdrant.py\n        ingest.py\n        ingest_items.py\n        matcher_service.py\n        parser.py\n        qdr.py\n      src/fsnb_matcher/views/\n        __init__.py\n      src/fsnb_matcher/embeddings/\n        __init__.py\n        model_giga.py\n    src/templates/\n      src/templates/core/\n        _header.html\n        base.html\n        index.html\n        login.html\n        profile.html\n        register.html\n      src/templates/users/\n        list.html\n      src/templates/admin/\n        index.html\n        login.html\n        model_edit.html\n        model_list.html\n        perm_edit.html\n        profile_edit.html\n        user_edit.html\n        users.html\n      src/templates/fsnb_matcher/\n        widget.html\n    src/scripts/\n      create_fsnb_pg.py\n      init_vector_db.py\n      superuser.py\n  alembic/\n    README\n    env.py\n    script.py.mako\n    alembic/versions/\n      2025_11_10_1130-c1e86fa06fba_create_users_table.py\n      2025_11_10_1138-cff667a73384_update_users_table.py\n      2025_12_09_1325-33aa3a368fa8_users_profile_permission.py\n      2025_12_09_1626-655d3e998655_make_username_nullable.py\n      2025_12_09_1632-e133393acc3f_expand_activation_key_to_255.py\n      2025_12_09_1702-e8c5f30d93ed_drop_uq_users_foo_bar.py\n      2025_12_10_1436-87f35af16311_cascade_deletes_trigger_profile_user.py\n      2025_12_16_1347-940ec7a6bbce_fsnb_matcher_add_items.py\n      2025_12_16_1359-e306c008d156_fsnb_matcher_create_items_table.py",
  "files": [
    {
      "path": "Dockerfile",
      "language": "docker",
      "size_bytes": 1558,
      "sha256": "2660125fcc3826f6ac6c4d0fcba4de09a8e1773a52d7b0701b656e471ccdc433",
      "content": "# path: Dockerfile\nFROM python:3.12-slim AS base\n\nENV PYTHONDONTWRITEBYTECODE=1 \\\n    PYTHONUNBUFFERED=1 \\\n    PIP_NO_CACHE_DIR=1 \\\n    POETRY_VERSION=1.8.3 \\\n    PYTHONPATH=/app/src \\\n    HF_HOME=/app/weights/hf-cache \\\n    TRANSFORMERS_CACHE=/app/weights/hf-cache \\\n    SENTENCE_TRANSFORMERS_HOME=/app/weights/hf-cache \\\n    PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n\n# Системные зависимости для сборки некоторых Python-пакетов (если нужно)\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    build-essential curl git ca-certificates locales \\\n && rm -rf /var/lib/apt/lists/*\n\n# Poetry\nRUN pip install --no-cache-dir \"poetry==${POETRY_VERSION}\" && poetry --version\n\nWORKDIR /app\n\n# Сначала зависимости (чтобы лучше работал Docker cache)\nCOPY pyproject.toml poetry.lock ./\nRUN poetry config virtualenvs.create false \\\n && poetry install --only main --no-interaction --no-ansi\n\n# PyTorch (как у тебя было, CUDA 12.1)\nRUN pip install --no-cache-dir --index-url https://download.pytorch.org/whl/cu121 \\\n    torch torchvision torchaudio\n\n# Копируем только исходники проекта.\n# Шаблоны у тебя в src/templates/, поэтому отдельный COPY templates НЕ нужен.\nCOPY src ./src\n\nEXPOSE 8000\n\n# ВАЖНО: точка входа у тебя src.main:main_app, а не src.core.main:app\nCMD [\"poetry\", \"run\", \"uvicorn\", \"src.main:main_app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"
    },
    {
      "path": "Readme.md",
      "language": "markdown",
      "size_bytes": 18363,
      "sha256": "77948307cdb1cc6536a0c7d2830821a404b75ca3cb46b898eb7e1a3dbae7ab83",
      "content": "\n# fsnb-fastapi — базовый каркас веб‑приложения на FastAPI\n\nЛёгкий, но «живой» стартовый проект: веб‑шаблоны (Jinja2), сессии и формы, JWT‑авторизация для API, пользователи/профили/права, загрузка аватара, а также простая админ‑панель с регистрацией моделей «как в Django». Стек — **FastAPI + SQLAlchemy (async) + Alembic + Jinja2 + OAuth2/JWT**.\n\n---\n\n## Содержание\n\n- [Возможности](#возможности)\n- [Архитектура и стек](#архитектура-и-стек)\n- [Структура проекта](#структура-проекта)\n- [Быстрый старт](#быстрый-старт)\n  - [Требования](#требования)\n  - [Переменные окружения](#переменные-окружения)\n  - [Инициализация БД](#инициализация-бд)\n  - [Запуск dev‑сервера](#запуск-dev-сервера)\n  - [Создание суперпользователя](#создание-суперпользователя)\n- [Веб‑интерфейс](#веб-интерфейс)\n  - [/ – главная](#--главная)\n  - [/login, /register – вход/регистрация](#login-register--входрегистрация)\n  - [/profile – профиль и аватар](#profile--профиль-и-аватар)\n  - [/admin – админ‑панель](#admin--админ-панель)\n- [API (OAuth2/JWT)](#api-oauth2jwt)\n  - [/api/v1/auth/token](#apiv1authtoken)\n  - [/api/v1/users](#apiv1users)\n- [Модели и права](#модели-и-права)\n- [Регистрация моделей в админке (аналог django admin.py)](#регистрация-моделей-в-админке-аналог-django-adminpy)\n- [Загрузка файлов и ограничения](#загрузка-файлов-и-ограничения)\n- [Расширение и доработка](#расширение-и-доработка)\n- [Подсказки и устранение проблем](#подсказки-и-устранение-проблем)\n\n---\n\n## Возможности\n\n- Пользователи с профилем и правами (**User ↔ Profile ↔ Permission**).\n- Авторизация:\n  - веб‑часть — через **сессии и формы**;\n  - API — через **OAuth2 Password** и **JWT**.\n- Профиль пользователя: редактирование полей + **загрузка/предпросмотр аватара** (кнопка «Удалить» и дефолтная картинка).\n- Ограничения на файл аватара: **только изображения**, **≤ 3 МБ**, **минимум 40×40 px**; при замене старый файл удаляется.\n- Простая **админ‑панель**:\n  - вход только для `is_superadmin` или `is_admin`;\n  - список зарегистрированных моделей, просмотр/редактирование полей с ограничениями read‑only;\n  - управление правами пользователей (админ не может менять флаги супер‑админа).\n- «Django‑like» **регистрация моделей** для админки в `base_app/admin.py` (аналог `admin.site.register()`).\n\n---\n\n## Архитектура и стек\n\n- **FastAPI** (асинхронный веб‑фреймворк).\n- **SQLAlchemy (async)** + **Alembic** (ORM и миграции).\n- **Jinja2** (шаблоны), **Starlette sessions** (сессии).\n- **Passlib (bcrypt_sha256)** для хеширования паролей.\n- **python-jose** для JWT.\n- **Poetry** для зависимостей (см. `pyproject.toml`).\n\nОтдельная настройка: `base_app/core/config.py` — Pydantic Settings с префиксом переменных окружения `APP_CONFIG__…`.\n\n---\n\n## Структура проекта\n\nГлавные узлы (примерно):\n\n```\nFastAPIbase/\n├─ src/\n│  ├─ main.py\n│  ├─ manage.py               # CLI (create_superuser и пр.)\n│  ├─ admin.py                # реестр моделей для админки\n│  ├─ logging.py\n│  ├─ core/\n│  │  ├─ views/\n│  │  │  ├─ web.py\n│  │  │  ├─ auth.py\n│  │  │  └─ admin.py          # (или admin_views.py)\n│  │  ├─ api/\n│  │  │  └─ api_v1/\n│  │  │     ├─ __init__.py    # сборка роутеров v1\n│  │  │     ├─ users.py\n│  │  │     └─ auth.py\n│  │  ├─ models/\n│  │  │  ├─ __init__.py\n│  │  │  ├─ base.py\n│  │  │  ├─ db_helper.py\n│  │  │  ├─ permission.py\n│  │  │  ├─ profile.py\n│  │  │  └─ user.py\n│  │  ├─ schemas/\n│  │  │  ├─ __init__.py\n│  │  │  ├─ permission.py\n│  │  │  ├─ profile.py\n│  │  │  └─ user.py\n│  │  ├─ services/\n│  │  │  ├─ __init__.py\n│  │  │  └─ auth_service.py\n│  │  ├─ mailing/\n│  │  │  ├─ __init__.py\n│  │  │  └─ mail.py\n│  │  ├─ utils/\n│  │  │  ├─ __init__.py\n│  │  │  └─ case_converter.py\n│  │  ├─ config.py\n│  │  ├─ email_tokens.py\n│  │  ├─ security.py\n│  │  └─ dependencies.py      # DI-провайдеры (возвращают crud)\n│  ├─ crud/\n│  │  ├─ user_repository.py\n│  │  ├─ profile_repository.py\n│  │  └─ permission_repository.py\n│  ├─ templates/\n│  │  ├─ core/{base,_header,login,register,profile,index}.html\n│  │  ├─ users/list.html\n│  │  └─ admin/{index,login,users,user_edit,profile_edit,perm_edit,model_list,model_edit}.html\n│  └─ scripts/\n│     └─ superuser.py\n├─ static/\n│  ├─ css/style.css\n│  ├─ js/{app.js,avatar-preview.js}\n│  ├─ img/\n│  └─ uploads/avatars/\n├─ alembic/\n│  ├─ env.py\n│  └─ versions/*.py\n├─ docker-compose.yml\n├─ poetry.lock\n├─ Readme.md\n├─ alembic.ini\n├─ .gitignore\n├─ .env\n├─ .env.example\n└─ pyproject.toml\n```\n\n---\n\n## Быстрый старт\n\n### Требования\n\n- Python 3.12+\n- PostgreSQL 14+ (или совместимый)\n- Poetry\n\n### Переменные окружения\n\nКонфиг читается через Pydantic Settings с префиксом `APP_CONFIG__` и разделителем `__` (см. `base_app/core/config.py`). Обязательное:\n\n```bash\nAPP_CONFIG__DB__URL=postgresql+asyncpg://user:password@localhost:5432/dbname\n```\n\nРекомендуется задать секреты:\n\n```bash\nAPP_CONFIG__AUTH__SECRET_KEY=change_me\nAPP_CONFIG__AUTH__EMAIL_VERIFY_SECRET=change_me_too\n```\n\n> В корне используйте корректный `.env` или `.env.example` (проверьте отсутствие опечаток в имени файла).\n\n\n## Start project services with Compose\n\nFrom the project root (where `docker-compose.yml` lives):\n\n```bash\n# Launch in background\ndocker compose up -d\n# See container status\ndocker compose ps\n# Stream logs (select services as needed)\ndocker compose logs -f postgres adminer mail\n```\n## Services & default addresses\n\nBelow are the **typical** defaults used in this project. If you changed ports or service names in `docker-compose.yml`, adjust accordingly.\n\n### PostgreSQL (DB)\n\n- **Internal service name (Compose network):** `postgres` (sometimes `db`)  \n- **Internal port:** `5432`  \n- **Published host port (example):** `5436`  \n- **Default credentials (match your app’s DSN):**\n  - **DB name:** `shop`\n  - **User:** `user`\n  - **Password:** `password`\n\n#### Connect from your host (psql / GUI):\n```bash\npsql \"postgresql://user:password@localhost:5436/shop\"\n```\n### Adminer (DB UI)\n\n- **URL (browser):** http://localhost:8091\n\nAt the login screen choose:\n\n| Field    | Value (inside Docker network)  | Alternative (through host port map) |\n|----------|--------------------------------|--------------------------------------|\n| System   | PostgreSQL                     | PostgreSQL                           |\n| Server   | `postgres`  *(or `db`)*        | `host.docker.internal:5436` *(Linux may use `172.17.0.1:5436`)* |\n| Username | `user`                         | `user`                               |\n| Password | `password`                     | `password`                           |\n| Database | `shop`                         | `shop`                               |\n\n> **Tip:** If Adminer can’t connect using `postgres` (or `db`), check the actual service name in `docker-compose.yml`.  \n> When using the **host port** variant, make sure Postgres is published on that port (e.g., `5436:5432`).\n\n\n### Инициализация БД\n\n```bash\npoetry install\npoetry run alembic upgrade head\n```\n\nМиграции находятся в `alembic/versions` и создают таблицы `users`, `profiles`, `permissions` с каскадными связями и триггером удаления.\n\n### Запуск dev‑сервера\n\n```bash\npoetry run python -m base_app.main\n# или:\n# poetry run uvicorn base_app.main:app --reload --port 8015\n```\n\nПараметры хоста/порта берутся из `base_app/core/config.py` (`run.host`, `run.port`).\n\n### Создание суперпользователя\n\nЕсть два пути:\n\n1) **CLI-обёртка**:\n\n```bash\npoetry run python -m base_app.manage --create_superuser\n```\n\nСкрипт интерактивно спросит `username`, `password`, `email` и выставит ключевые флаги прав (is_superadmin/is_admin и т. п.).\n\n2) **Прямая утилита**:\n\n```bash\npoetry run python -m base_app.scripts.superuser\n```\n\n---\n\n## Веб‑интерфейс\n\n### `/` — главная\n\nБазовая страница на Jinja2 (`templates/core/index.html`). Шапка из `_header.html` показывает «Профиль/Выйти» для авторизованных, «Вход/Регистрация» — для гостя.\n\n### `/login`, `/register` — вход/регистрация\n\nФормы находятся в `templates/core/{login,register}.html`, обработчики — `base_app/views/auth.py`. После входа создаётся сессионная авторизация для веб‑части.\n\n### `/profile` — профиль и аватар\n\n- Шаблон: `templates/core/profile.html`.\n- JS‑предпросмотр: `static/js/avatar-preview.js` (подключается через `base.html` с `defer`).\n- Серверная валидация:\n  - только **image/***;\n  - размер файла ≤ **3 МБ**;\n  - **минимум 40×40 px** (иначе будет alert/ошибка);\n  - при загрузке **старый аватар удаляется** (не копим мусор).\n- Хранение: `static/uploads/avatars/`; кнопка «Удалить аватар» — под мини‑превью справа от заголовка «Профиль».\n\nПоля профиля: `nickname`, `first_name`, `second_name`, `phone`, `email`, `tg_id`, `tg_nickname`, `session`, `verification` и др.\n\n### `/admin` — админ‑панель\n\n- Вход по `/admin/login` (CSRF в форме), хранится отдельная админ‑сессия.\n- Допускаются пользователи, у кого в связанной записи `Permission` стоит `is_superadmin=True` **или** `is_admin=True`.\n- Индекс показывает доступные модели (из реестра): `Users`, `Profiles`, `Permissions`.\n- Список/редактирование:\n  - списки: `templates/admin/users.html`, `model_list.html`;\n  - редактирование: `templates/admin/{user_edit,profile_edit,perm_edit,model_edit}.html`.\n- Логика прав:\n  - супер‑админ может менять всем всё;\n  - обычный админ **не может** изменять флаги супер‑админа;\n  - `is_superadmin` виден всем, но менять его может только супер‑админ.\n\n---\n\n## API (OAuth2/JWT)\n\n### `/api/v1/auth/token`\n\n`POST x-www-form-urlencoded` (OAuth2 Password: `username`, `password`) → `{\"access_token\": \"...\", \"token_type\": \"bearer\"}`. Реализация — `base_app/api/api_v1/auth.py`, JWT в `base_app/core/security.py`.\n\n> Убедитесь, что `tokenUrl` в зависимостях (`base_app/core/dependencies.py`) совпадает с фактическим префиксом роутера.\n\n### `/api/v1/users`\n\n- `GET` — список пользователей (`UserRead`), репозиторий: `base_app/crud/user_repository.py:get_all_users`.\n- `POST` — создать пользователя (`UserCreate` → User + Profile + базовый Permission).\n\n---\n\n## Модели и права\n\n- **User**: `email` (уникальный), `username` (опц.), `hashed_password`, `is_active`, `activation_key` (+служебные поля).\n- **Profile**: 1:1 к `User`; контактные/публичные поля + `verification`.\n- **Permission**: связь к `Profile` (практически 1:1), флаги:\n  - `is_superadmin`, `is_admin`, `is_staff`, `is_updater`, `is_reader`, `is_user`.\n\nВ БД настроены каскадные удаления, а также триггер, удаляющий `User` при удалении связанного `Profile`.\n\n---\n\n## Регистрация моделей в админке (аналог `django admin.py`)\n\nФайл `src/admin.py` содержит минимальный «реестр» моделей:\n\n```python\nfrom src.admin import admin_site\nfrom src.core.models.user import User\n\nadmin_site.register(\n    User,\n    slug=\"users\",\n    list_display=[\"id\", \"email\", \"username\", \"is_active\"],\n    form_fields=[\"email\", \"username\", \"is_active\", \"activation_key\"],\n    readonly_fields=[\"id\"],\n    search_fields=[\"email\", \"username\"],\n    can_create=False,\n    can_delete=False,\n)\n```\n\n- **`slug`** определяет часть URL (например, `/admin/m/users`).\n- **`list_display`** — колонки списка, **`form_fields`** — редактируемые поля, **`readonly_fields`** — только просмотр.\n- Аналогично регистрируются `Profile` и `Permission`.\n\n---\n\n## Загрузка файлов и ограничения\n\n- Каталог: `static/uploads/avatars/`.\n- Серверная валидация:\n  - MIME‑тип должен начинаться с `image/`;\n  - размер ≤ **3 МБ**;\n  - геометрия не меньше **40×40 px**.\n- При загрузке **предыдущий файл аватара удаляется**.\n- На фронте `static/js/avatar-preview.js` показывает превью, имя файла и валидирует ограничения **до** отправки формы; изменения применяются в UI сразу, но сохраняются только после нажатия «Сохранить».\n\n---\n\n\n\n\n## Расширение и доработка\n\n- Добавляйте модели в БД через Alembic, регистрируйте их в `base_app/admin.py` — они появятся в админ‑меню.\n- Политики прав можно расширять новыми флагами в `Permission` и проверками во вьюхах.\n- Веб‑часть масштабируется через новые шаблоны в `templates/` и статику в `static/`.\n- API — подключайте новые роутеры под `base_app/api/api_v1/`, включайте их в `__init__.py`.\n\n---\n\n## Подсказки и устранение проблем\n\n- **.env**: используйте `.env` или `.env.example` без опечаток.\n- **Маршруты/префиксы**: следите, чтобы `tokenUrl` и префиксы роутеров совпадали (во избежание «двойного префикса»).\n- **CSRF в админке**: логин/формы используют CSRF‑токен; при 400/403 проверьте скрытое поле и сессию.\n- **Права не сохраняются**: `is_superadmin` может менять только супер‑админ.\n- **Аватар/превью**: проверяйте подключение `static/js/avatar-preview.js` (через `defer`) и кеш браузера.\n\n\n# Сервис получения ВОР\n\nСоздать суперпользователя\n```\ndocker compose exec app bash -lc \"python -m src.manage --create_superuser\"\n```\nЗалить ФСНБ в Postgres\n```\ndocker compose exec app bash -lc \"python -m src.scripts.create_fsnb_pg\"\n```\nПроверить Postgres (таблицы + количество + примеры)\n```\ndocker compose exec pg bash -lc \"psql -U user -d shop -c '\\dt'\"\n\ndocker compose exec pg bash -lc \"psql -U user -d shop -c 'select count(*) from items;'\"\n\ndocker compose exec pg bash -lc \"psql -U user -d shop -c \\\"select type, count(*) from items group by type order by 2 desc;\\\"\"\n\ndocker compose exec pg bash -lc \"psql -U user -d shop -c \\\"select id, code, left(name,120) as name, unit, type from items order by id limit 5;\\\"\"\n```\nИндексация Qdrant\n```\ndocker compose exec app bash -lc \"python -m src.scripts.init_vector_db\"\n```\nПроверить Qdrant (с хоста)\n### список коллекций (на хосте qdrant проброшен как 6335)\n```\ncurl -s http://127.0.0.1:6335/collections | head\n```\n### count по коллекции (пример: fsnb_giga)\n```\ncurl -s -X POST \"http://127.0.0.1:6335/collections/fsnb_giga/points/count\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"exact\": true}' | head\n```\n"
    },
    {
      "path": "__init__.py",
      "language": "python",
      "size_bytes": 0,
      "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
      "content": ""
    },
    {
      "path": "alembic/env.py",
      "language": "python",
      "size_bytes": 2532,
      "sha256": "fda52c2147dd56f10a3d0cacdaefde5daf116d2ccf2661c619b84fb7f0610b9c",
      "content": "import asyncio\nfrom logging.config import fileConfig\n\nfrom sqlalchemy import pool\nfrom sqlalchemy.engine import Connection\nfrom sqlalchemy.ext.asyncio import async_engine_from_config\n\nfrom alembic import context\n\nfrom src.core.config import settings\nfrom src.core.models import Base\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = Base.metadata\n\n# other values from the config, defined by the needs of env.py,\n# can be acquired:\n# my_important_option = config.get_main_option(\"my_important_option\")\n# ... etc.\nconfig.set_main_option(\"sqlalchemy.url\", str(settings.db.url))\n\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef do_run_migrations(connection: Connection) -> None:\n    context.configure(connection=connection, target_metadata=target_metadata)\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\nasync def run_async_migrations() -> None:\n    \"\"\"In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n\n    connectable = async_engine_from_config(\n        config.get_section(config.config_ini_section, {}),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    async with connectable.connect() as connection:\n        await connection.run_sync(do_run_migrations)\n\n    await connectable.dispose()\n\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\"\"\"\n\n    asyncio.run(run_async_migrations())\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n"
    },
    {
      "path": "alembic/versions/2025_11_10_1130-c1e86fa06fba_create_users_table.py",
      "language": "python",
      "size_bytes": 1049,
      "sha256": "84ed820467e60ebbe596be951192a6672d3daa045d255bd286b8e8e877a59647",
      "content": "\"\"\"create users table\n\nRevision ID: c1e86fa06fba\nRevises:\nCreate Date: 2025-11-10 11:30:00.602050\n\n\"\"\"\n\nfrom typing import Sequence, Union\n\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision: str = \"c1e86fa06fba\"\ndown_revision: Union[str, Sequence[str], None] = None\nbranch_labels: Union[str, Sequence[str], None] = None\ndepends_on: Union[str, Sequence[str], None] = None\n\n\ndef upgrade() -> None:\n    \"\"\"Upgrade schema.\"\"\"\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"users\",\n        sa.Column(\"username\", sa.String(), nullable=False),\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\", name=op.f(\"pk_users\")),\n        sa.UniqueConstraint(\"username\", name=op.f(\"uq_users_username\")),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    \"\"\"Downgrade schema.\"\"\"\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"users\")\n    # ### end Alembic commands ###\n"
    },
    {
      "path": "alembic/versions/2025_11_10_1138-cff667a73384_update_users_table.py",
      "language": "python",
      "size_bytes": 1142,
      "sha256": "d5a1292949ab9d2ff6421a8830c604b7dadfc4c6378f8bd2c8d986a98136c960",
      "content": "\"\"\"update users table\n\nRevision ID: cff667a73384\nRevises: c1e86fa06fba\nCreate Date: 2025-11-10 11:38:35.346057\n\n\"\"\"\n\nfrom typing import Sequence, Union\n\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision: str = \"cff667a73384\"\ndown_revision: Union[str, Sequence[str], None] = \"c1e86fa06fba\"\nbranch_labels: Union[str, Sequence[str], None] = None\ndepends_on: Union[str, Sequence[str], None] = None\n\n\ndef upgrade() -> None:\n    \"\"\"Upgrade schema.\"\"\"\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"users\", sa.Column(\"foo\", sa.Integer(), nullable=False))\n    op.add_column(\"users\", sa.Column(\"bar\", sa.Integer(), nullable=False))\n    op.create_unique_constraint(\n        op.f(\"uq_users_foo_bar\"), \"users\", [\"foo\", \"bar\"]\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    \"\"\"Downgrade schema.\"\"\"\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_constraint(op.f(\"uq_users_foo_bar\"), \"users\", type_=\"unique\")\n    op.drop_column(\"users\", \"bar\")\n    op.drop_column(\"users\", \"foo\")\n    # ### end Alembic commands ###\n"
    },
    {
      "path": "alembic/versions/2025_12_09_1325-33aa3a368fa8_users_profile_permission.py",
      "language": "python",
      "size_bytes": 7449,
      "sha256": "2e854faff454db587f8552bf469ca57c932d2c62b879c1da1fc039b5deaa88e7",
      "content": "# /alembic/versions/2025_12_09_1325-33aa3a368fa8_users_profile_permission.py\nfrom __future__ import annotations\n\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"33aa3a368fa8\"\ndown_revision = \"cff667a73384\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    bind = op.get_bind()\n    insp = sa.inspect(bind)\n\n    def has_table(name: str) -> bool:\n        return insp.has_table(name)\n\n    def has_column(table: str, column: str) -> bool:\n        try:\n            return column in {c[\"name\"] for c in insp.get_columns(table)}\n        except Exception:\n            return False\n\n    # --- USERS ---\n    if not has_table(\"users\"):\n        # создаём таблицу users с актуальной структурой\n        op.create_table(\n            \"users\",\n            sa.Column(\"id\", sa.Integer(), primary_key=True, autoincrement=True),\n            sa.Column(\"email\", sa.String(length=255), nullable=False),\n            sa.Column(\"username\", sa.String(length=64), nullable=True),\n            sa.Column(\"hashed_password\", sa.String(length=255), nullable=True),\n            sa.Column(\"is_active\", sa.Boolean(), nullable=False, server_default=sa.true()),\n            sa.Column(\"activation_key\", sa.String(length=64), nullable=True),\n            sa.Column(\"activation_sent_at\", sa.DateTime(timezone=True), nullable=True),\n            sa.Column(\"foo\", sa.Integer(), nullable=False, server_default=\"0\"),\n            sa.Column(\"bar\", sa.Integer(), nullable=False, server_default=\"0\"),\n            sa.UniqueConstraint(\"email\", name=\"uq_users_email\"),\n            sa.UniqueConstraint(\"username\", name=\"uq_users_username\"),\n        )\n    else:\n        # добавляем недостающие колонки по одной\n        if not has_column(\"users\", \"email\"):\n            op.add_column(\"users\", sa.Column(\"email\", sa.String(length=255), nullable=True))\n            # бэкофилл email -> NOT NULL -> UNIQUE\n            bind.execute(\n                sa.text(\n                    \"\"\"\n                    UPDATE users\n                    SET email = COALESCE(\n                        email,\n                        CASE\n                            WHEN username IS NOT NULL AND username <> '' THEN username || '@local.invalid'\n                            ELSE 'user_' || id::text || '@local.invalid'\n                        END\n                    )\n                    \"\"\"\n                )\n            )\n            op.alter_column(\"users\", \"email\", existing_type=sa.String(length=255), nullable=False)\n            # создаём уникальный констрейнт, если его нет\n            uqs = insp.get_unique_constraints(\"users\")\n            if \"uq_users_email\" not in {uq[\"name\"] for uq in uqs}:\n                op.create_unique_constraint(\"uq_users_email\", \"users\", [\"email\"])\n\n        if not has_column(\"users\", \"username\"):\n            op.add_column(\"users\", sa.Column(\"username\", sa.String(length=64), nullable=True))\n            uqs = insp.get_unique_constraints(\"users\")\n            if \"uq_users_username\" not in {uq[\"name\"] for uq in uqs}:\n                op.create_unique_constraint(\"uq_users_username\", \"users\", [\"username\"])\n\n        if not has_column(\"users\", \"hashed_password\"):\n            op.add_column(\"users\", sa.Column(\"hashed_password\", sa.String(length=255), nullable=True))\n        if not has_column(\"users\", \"is_active\"):\n            op.add_column(\"users\", sa.Column(\"is_active\", sa.Boolean(), nullable=False, server_default=sa.true()))\n        if not has_column(\"users\", \"activation_key\"):\n            op.add_column(\"users\", sa.Column(\"activation_key\", sa.String(length=64), nullable=True))\n        if not has_column(\"users\", \"activation_sent_at\"):\n            op.add_column(\"users\", sa.Column(\"activation_sent_at\", sa.DateTime(timezone=True), nullable=True))\n        if not has_column(\"users\", \"foo\"):\n            op.add_column(\"users\", sa.Column(\"foo\", sa.Integer(), nullable=False, server_default=\"0\"))\n        if not has_column(\"users\", \"bar\"):\n            op.add_column(\"users\", sa.Column(\"bar\", sa.Integer(), nullable=False, server_default=\"0\"))\n\n    # --- PROFILES ---\n    if not has_table(\"profiles\"):\n        op.create_table(\n            \"profiles\",\n            sa.Column(\"id\", sa.Integer(), autoincrement=True, nullable=False),\n            sa.Column(\"nickname\", sa.String(length=64), nullable=True),\n            sa.Column(\"avatar\", sa.String(length=255), nullable=True),\n            sa.Column(\"first_name\", sa.String(length=48), nullable=True),\n            sa.Column(\"second_name\", sa.String(length=48), nullable=True),\n            sa.Column(\"phone\", sa.String(length=32), nullable=True),\n            sa.Column(\"email\", sa.String(length=255), nullable=True),\n            sa.Column(\"tg_id\", sa.BigInteger(), nullable=True),\n            sa.Column(\"tg_nickname\", sa.String(length=64), nullable=True),\n            sa.Column(\"verification\", sa.Boolean(), nullable=False, server_default=sa.false()),\n            sa.Column(\"session\", sa.String(length=255), nullable=True),\n            sa.Column(\"user_id\", sa.Integer(), nullable=False),\n            sa.ForeignKeyConstraint([\"user_id\"], [\"users.id\"]),\n            sa.PrimaryKeyConstraint(\"id\"),\n            sa.UniqueConstraint(\"user_id\", name=\"uq_profiles_user_id\"),\n        )\n\n    # --- PERMISSIONS ---\n    if not has_table(\"permissions\"):\n        op.create_table(\n            \"permissions\",\n            sa.Column(\"id\", sa.Integer(), autoincrement=True, nullable=False),\n            sa.Column(\"profile_id\", sa.Integer(), nullable=False),\n            sa.Column(\"is_superadmin\", sa.Boolean(), nullable=False, server_default=sa.false()),\n            sa.Column(\"is_admin\", sa.Boolean(), nullable=False, server_default=sa.false()),\n            sa.Column(\"is_staff\", sa.Boolean(), nullable=False, server_default=sa.false()),\n            sa.Column(\"is_updater\", sa.Boolean(), nullable=False, server_default=sa.false()),\n            sa.Column(\"is_reader\", sa.Boolean(), nullable=False, server_default=sa.false()),\n            sa.Column(\"is_user\", sa.Boolean(), nullable=False, server_default=sa.false()),\n            sa.ForeignKeyConstraint([\"profile_id\"], [\"profiles.id\"]),\n            sa.PrimaryKeyConstraint(\"id\"),\n        )\n\n\ndef downgrade() -> None:\n    bind = op.get_bind()\n    insp = sa.inspect(bind)\n\n    def has_table(name: str) -> bool:\n        return insp.has_table(name)\n\n    def has_uc(table: str, name: str) -> bool:\n        return name in {uc[\"name\"] for uc in insp.get_unique_constraints(table)}\n\n    # порядок важен: сначала зависимые\n    if has_table(\"permissions\"):\n        op.drop_table(\"permissions\")\n    if has_table(\"profiles\"):\n        op.drop_table(\"profiles\")\n\n    if has_table(\"users\"):\n        # мягкий откат: уберем констрейнты и колонки, которые добавляли\n        if has_uc(\"users\", \"uq_users_email\"):\n            op.drop_constraint(\"uq_users_email\", \"users\", type_=\"unique\")\n        if has_uc(\"users\", \"uq_users_username\"):\n            op.drop_constraint(\"uq_users_username\", \"users\", type_=\"unique\")\n\n        cols = {c[\"name\"] for c in insp.get_columns(\"users\")}\n        for col in (\"activation_sent_at\", \"activation_key\", \"is_active\", \"hashed_password\", \"email\", \"username\", \"foo\", \"bar\"):\n            if col in cols:\n                op.drop_column(\"users\", col)\n"
    },
    {
      "path": "alembic/versions/2025_12_09_1626-655d3e998655_make_username_nullable.py",
      "language": "python",
      "size_bytes": 1009,
      "sha256": "9ec8daa8515d176b8bc5ef9195b962f260ff09eee6387de7c0d32e9a1c9d1522",
      "content": "\"\"\"make username nullable\n\nRevision ID: 655d3e998655\nRevises: 33aa3a368fa8\nCreate Date: 2025-12-09 16:26:43.983839\n\n\"\"\"\n\nfrom typing import Sequence, Union\n\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision: str = \"655d3e998655\"\ndown_revision: Union[str, Sequence[str], None] = \"33aa3a368fa8\"\nbranch_labels: Union[str, Sequence[str], None] = None\ndepends_on: Union[str, Sequence[str], None] = None\n\n\ndef upgrade() -> None:\n    # Делаем username nullable=True\n    op.alter_column(\"users\", \"username\", existing_type=sa.String(length=64), nullable=True)\n\n\ndef downgrade() -> None:\n    # Откат осторожный: перед запретом NULL нужно заполнить пустые username\n    conn = op.get_bind()\n    conn.execute(sa.text(\"\"\"\n        UPDATE users\n        SET username = 'user_' || id::text\n        WHERE username IS NULL\n    \"\"\"))\n    op.alter_column(\"users\", \"username\", existing_type=sa.String(length=64), nullable=False)\n"
    },
    {
      "path": "alembic/versions/2025_12_09_1632-e133393acc3f_expand_activation_key_to_255.py",
      "language": "python",
      "size_bytes": 1208,
      "sha256": "efe87aa19126120c5bf6f67a7305b04aab03c96d3730579bb2a1cc267337b092",
      "content": "\"\"\"expand activation_key to 255\n\nRevision ID: e133393acc3f\nRevises: 655d3e998655\nCreate Date: 2025-12-09 16:32:43.704552\n\n\"\"\"\n\nfrom typing import Sequence, Union\n\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision: str = \"e133393acc3f\"\ndown_revision: Union[str, Sequence[str], None] = \"655d3e998655\"\nbranch_labels: Union[str, Sequence[str], None] = None\ndepends_on: Union[str, Sequence[str], None] = None\n\n\ndef upgrade() -> None:\n    # Было VARCHAR(64), делаем VARCHAR(255)\n    op.alter_column(\n        \"users\",\n        \"activation_key\",\n        existing_type=sa.String(length=64),\n        type_=sa.String(length=255),\n        existing_nullable=True,\n    )\n\n\ndef downgrade() -> None:\n    # ОСТОРОЖНО: при откате обрежем значения до 64 символов\n    conn = op.get_bind()\n    conn.execute(sa.text(\"\"\"\n        UPDATE users\n        SET activation_key = LEFT(activation_key, 64)\n        WHERE activation_key IS NOT NULL;\n    \"\"\"))\n    op.alter_column(\n        \"users\",\n        \"activation_key\",\n        existing_type=sa.String(length=255),\n        type_=sa.String(length=64),\n        existing_nullable=True,\n    )"
    },
    {
      "path": "alembic/versions/2025_12_09_1702-e8c5f30d93ed_drop_uq_users_foo_bar.py",
      "language": "python",
      "size_bytes": 786,
      "sha256": "33080c3add7ed7e239e7a5dca5c22a3565e5053e2c92959fdb4d6c965052ae2b",
      "content": "\"\"\"drop uq_users_foo_bar\n\nRevision ID: e8c5f30d93ed\nRevises: e133393acc3f\nCreate Date: 2025-12-09 17:02:20.420091\n\n\"\"\"\n\nfrom typing import Sequence, Union\n\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision: str = \"e8c5f30d93ed\"\ndown_revision: Union[str, Sequence[str], None] = \"e133393acc3f\"\nbranch_labels: Union[str, Sequence[str], None] = None\ndepends_on: Union[str, Sequence[str], None] = None\n\n\ndef upgrade() -> None:\n    # снимаем уникальность с (foo, bar)\n    op.drop_constraint(\"uq_users_foo_bar\", \"users\", type_=\"unique\")\n\n\ndef downgrade() -> None:\n    # возвращаем (если вдруг понадобится откат)\n    op.create_unique_constraint(\"uq_users_foo_bar\", \"users\", [\"foo\", \"bar\"])\n"
    },
    {
      "path": "alembic/versions/2025_12_10_1436-87f35af16311_cascade_deletes_trigger_profile_user.py",
      "language": "python",
      "size_bytes": 2741,
      "sha256": "83730473c7416a5256f39fde64c986effdd472344d9b7a3d871e98d258113ce9",
      "content": "\"\"\"cascade deletes + trigger profile->user\n\nRevision ID: 87f35af16311\nRevises: e8c5f30d93ed\nCreate Date: 2025-12-10 14:36:51.024796\n\n\"\"\"\n\nfrom typing import Sequence, Union\n\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision: str = \"87f35af16311\"\ndown_revision: Union[str, Sequence[str], None] = \"e8c5f30d93ed\"\nbranch_labels: Union[str, Sequence[str], None] = None\ndepends_on: Union[str, Sequence[str], None] = None\n\n\ndef upgrade():\n    # 1) Пересоздаем FK с ON DELETE CASCADE\n    op.drop_constraint(op.f(\"fk_profiles_user_id_users\"), \"profiles\", type_=\"foreignkey\")\n    op.create_foreign_key(\n        op.f(\"fk_profiles_user_id_users\"),\n        \"profiles\",\n        \"users\",\n        [\"user_id\"],\n        [\"id\"],\n        ondelete=\"CASCADE\",\n    )\n\n    op.drop_constraint(op.f(\"fk_permissions_profile_id_profiles\"), \"permissions\", type_=\"foreignkey\")\n    op.create_foreign_key(\n        op.f(\"fk_permissions_profile_id_profiles\"),\n        \"permissions\",\n        \"profiles\",\n        [\"profile_id\"],\n        [\"id\"],\n        ondelete=\"CASCADE\",\n    )\n\n    # 2) Функция триггера (одна команда — можно в одном op.execute)\n    op.execute(\n        \"\"\"\n        CREATE OR REPLACE FUNCTION delete_user_on_profile_delete()\n        RETURNS trigger AS $$\n        BEGIN\n            DELETE FROM users WHERE id = OLD.user_id;\n            RETURN OLD;\n        END;\n        $$ LANGUAGE plpgsql;\n        \"\"\"\n    )\n\n    # 3) Триггер — РАЗДЕЛЬНО (каждая команда в своем op.execute)\n    op.execute(\"DROP TRIGGER IF EXISTS trg_delete_user_on_profile_delete ON profiles;\")\n    op.execute(\n        \"\"\"\n        CREATE TRIGGER trg_delete_user_on_profile_delete\n        AFTER DELETE ON profiles\n        FOR EACH ROW\n        EXECUTE FUNCTION delete_user_on_profile_delete();\n        \"\"\"\n    )\n\n\ndef downgrade():\n    # Удаляем триггер и функцию\n    op.execute(\"DROP TRIGGER IF EXISTS trg_delete_user_on_profile_delete ON profiles;\")\n    op.execute(\"DROP FUNCTION IF EXISTS delete_user_on_profile_delete();\")\n\n    # Возвращаем FK без каскада\n    op.drop_constraint(op.f(\"fk_permissions_profile_id_profiles\"), \"permissions\", type_=\"foreignkey\")\n    op.create_foreign_key(\n        op.f(\"fk_permissions_profile_id_profiles\"),\n        \"permissions\",\n        \"profiles\",\n        [\"profile_id\"],\n        [\"id\"],\n        ondelete=None,\n    )\n\n    op.drop_constraint(op.f(\"fk_profiles_user_id_users\"), \"profiles\", type_=\"foreignkey\")\n    op.create_foreign_key(\n        op.f(\"fk_profiles_user_id_users\"),\n        \"profiles\",\n        \"users\",\n        [\"user_id\"],\n        [\"id\"],\n        ondelete=None,\n    )\n"
    },
    {
      "path": "alembic/versions/2025_12_16_1347-940ec7a6bbce_fsnb_matcher_add_items.py",
      "language": "python",
      "size_bytes": 1144,
      "sha256": "ac48b0f3b2064535c4b6f03f71bff87a78998b02e592dcaa9bcea06fb10c4f8f",
      "content": "\"\"\"fsnb_matcher: add items\n\nRevision ID: 940ec7a6bbce\nRevises: 87f35af16311\nCreate Date: 2025-12-16 13:47:56.185360\n\n\"\"\"\n\nfrom typing import Sequence, Union\n\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision: str = \"940ec7a6bbce\"\ndown_revision: Union[str, Sequence[str], None] = \"87f35af16311\"\nbranch_labels: Union[str, Sequence[str], None] = None\ndepends_on: Union[str, Sequence[str], None] = None\n\n\ndef upgrade() -> None:\n    \"\"\"Upgrade schema.\"\"\"\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_index(\n        op.f(\"ix_permissions_profile_id\"),\n        \"permissions\",\n        [\"profile_id\"],\n        unique=True,\n    )\n    op.create_index(\n        op.f(\"ix_profiles_user_id\"), \"profiles\", [\"user_id\"], unique=True\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    \"\"\"Downgrade schema.\"\"\"\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_profiles_user_id\"), table_name=\"profiles\")\n    op.drop_index(op.f(\"ix_permissions_profile_id\"), table_name=\"permissions\")\n    # ### end Alembic commands ###\n"
    },
    {
      "path": "alembic/versions/2025_12_16_1359-e306c008d156_fsnb_matcher_create_items_table.py",
      "language": "python",
      "size_bytes": 1148,
      "sha256": "f7fdc8ad1b8910c5a91762046bb05f1297f2492ca4c5e228fe366100e7236c9b",
      "content": "\"\"\"fsnb_matcher: create items table\n\nRevision ID: e306c008d156\nRevises: 940ec7a6bbce\nCreate Date: 2025-12-16 13:59:24.374547\n\n\"\"\"\n\nfrom typing import Sequence, Union\n\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision: str = \"e306c008d156\"\ndown_revision: Union[str, Sequence[str], None] = \"940ec7a6bbce\"\nbranch_labels: Union[str, Sequence[str], None] = None\ndepends_on: Union[str, Sequence[str], None] = None\n\n\ndef upgrade() -> None:\n    op.create_table(\n        \"items\",\n        sa.Column(\"id\", sa.Integer(), primary_key=True, autoincrement=True),\n        sa.Column(\"code\", sa.Text(), nullable=True),\n        sa.Column(\"name\", sa.Text(), nullable=False),\n        sa.Column(\"unit\", sa.Text(), nullable=True),\n        sa.Column(\"type\", sa.Text(), nullable=False),\n        sa.UniqueConstraint(\"code\", name=\"uq_items_code\"),\n        sa.CheckConstraint(\"type IN ('work','resource')\", name=\"chk_items_type\"),\n    )\n    op.create_index(op.f(\"ix_items_name\"), \"items\", [\"name\"], unique=False)\n\n\ndef downgrade() -> None:\n    op.drop_index(op.f(\"ix_items_name\"), table_name=\"items\")\n    op.drop_table(\"items\")"
    },
    {
      "path": "alembic.ini",
      "language": "ini",
      "size_bytes": 4797,
      "sha256": "08ebc0246133f9b8b256bda33e10fbe0a09eb68dbe435ad2887b0c9c8b9864ba",
      "content": "# A generic, single database configuration.\n\n[alembic]\n# path to migration scripts.\n# this is typically a path given in POSIX (e.g. forward slashes)\n# format, relative to the token %(here)s which refers to the location of this\n# ini file\nscript_location = %(here)s/alembic\n\n# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s\n# Uncomment the line below if you want the files to be prepended with date and time\n# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file\n# for all available tokens\nfile_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s\n\n# sys.path path, will be prepended to sys.path if present.\n# defaults to the current working directory.  for multiple paths, the path separator\n# is defined by \"path_separator\" below.\nprepend_sys_path = .\n\n# timezone to use when rendering the date within the migration file\n# as well as the filename.\n# If specified, requires the tzdata library which can be installed by adding\n# `alembic[tz]` to the pip requirements.\n# string value is passed to ZoneInfo()\n# leave blank for localtime\n# timezone =\n\n# max length of characters to apply to the \"slug\" field\n# truncate_slug_length = 40\n\n# set to 'true' to run the environment during\n# the 'revision' command, regardless of autogenerate\n# revision_environment = false\n\n# set to 'true' to allow .pyc and .pyo files without\n# a source .py file to be detected as revisions in the\n# versions/ directory\n# sourceless = false\n\n# version location specification; This defaults\n# to <script_location>/versions.  When using multiple version\n# directories, initial revisions must be specified with --version-path.\n# The path separator used here should be the separator specified by \"path_separator\"\n# below.\n# version_locations = %(here)s/bar:%(here)s/bat:%(here)s/alembic/versions\n\n# path_separator; This indicates what character is used to split lists of file\n# paths, including version_locations and prepend_sys_path within configparser\n# files such as alembic.ini.\n# The default rendered in new alembic.ini files is \"os\", which uses os.pathsep\n# to provide os-dependent path splitting.\n#\n# Note that in order to support legacy alembic.ini files, this default does NOT\n# take place if path_separator is not present in alembic.ini.  If this\n# option is omitted entirely, fallback logic is as follows:\n#\n# 1. Parsing of the version_locations option falls back to using the legacy\n#    \"version_path_separator\" key, which if absent then falls back to the legacy\n#    behavior of splitting on spaces and/or commas.\n# 2. Parsing of the prepend_sys_path option falls back to the legacy\n#    behavior of splitting on spaces, commas, or colons.\n#\n# Valid values for path_separator are:\n#\n# path_separator = :\n# path_separator = ;\n# path_separator = space\n# path_separator = newline\n#\n# Use os.pathsep. Default configuration used for new projects.\npath_separator = os\n\n\n# set to 'true' to search source files recursively\n# in each \"version_locations\" directory\n# new in Alembic version 1.10\n# recursive_version_locations = false\n\n# the output encoding used when revision files\n# are written from script.py.mako\n# output_encoding = utf-8\n\n# database URL.  This is consumed by the user-maintained env.py script only.\n# other means of configuring database URLs may be customized within the env.py\n# file.\nsqlalchemy.url = driver://user:pass@localhost/dbname\n\n\n[post_write_hooks]\n# post_write_hooks defines scripts or Python functions that are run\n# on newly generated revision scripts.  See the documentation for further\n# detail and examples\n\n# format using \"black\" - use the console_scripts runner, against the \"black\" entrypoint\nhooks = black\nblack.type = console_scripts\nblack.entrypoint = black\nblack.options = -l 79 REVISION_SCRIPT_FILENAME\n\n# lint with attempts to fix using \"ruff\" - use the module runner, against the \"ruff\" module\n# hooks = ruff\n# ruff.type = module\n# ruff.module = ruff\n# ruff.options = check --fix REVISION_SCRIPT_FILENAME\n\n# Alternatively, use the exec runner to execute a binary found on your PATH\n# hooks = ruff\n# ruff.type = exec\n# ruff.executable = ruff\n# ruff.options = check --fix REVISION_SCRIPT_FILENAME\n\n# Logging configuration.  This is also consumed by the user-maintained\n# env.py script only.\n[loggers]\nkeys = root,sqlalchemy,alembic\n\n[handlers]\nkeys = console\n\n[formatters]\nkeys = generic\n\n[logger_root]\nlevel = WARNING\nhandlers = console\nqualname =\n\n[logger_sqlalchemy]\nlevel = WARNING\nhandlers =\nqualname = sqlalchemy.engine\n\n[logger_alembic]\nlevel = INFO\nhandlers =\nqualname = alembic\n\n[handler_console]\nclass = StreamHandler\nargs = (sys.stderr,)\nlevel = NOTSET\nformatter = generic\n\n[formatter_generic]\nformat = %(levelname)-5.5s [%(name)s] %(message)s\ndatefmt = %H:%M:%S\n"
    },
    {
      "path": "docker-compose.yml",
      "language": "yaml",
      "size_bytes": 1916,
      "sha256": "b61c561942811638059dcba4d2a24fbd07fee9877d8ae14a5c45fa9bf567c54e",
      "content": "# path: docker-compose.yml\nservices:\n  pg:\n    image: postgres:16\n    environment:\n      POSTGRES_DB: shop\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    ports:\n      - \"5436:5432\"\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n\n  adminer:\n    image: adminer\n    ports:\n      - \"8091:8080\"\n    depends_on:\n      - pg\n\n  pgadmin:\n    image: dpage/pgadmin4\n    environment:\n      PGADMIN_DEFAULT_EMAIL: admin@admin.org\n      PGADMIN_DEFAULT_PASSWORD: admin\n      PGADMIN_CONFIG_SERVER_MODE: \"False\"\n    ports:\n      - \"5050:80\"\n    depends_on:\n      - pg\n\n  mailhog:\n    image: mailhog/mailhog\n    ports:\n      - \"8025:8025\"\n      - \"1025:1025\"\n\n  qdrant:\n    image: qdrant/qdrant:v1.12.1\n    ports:\n      - \"6335:6333\"   # REST на хосте\n      - \"6336:6334\"   # health/metrics на хосте\n    volumes:\n      - qdrant_data:/qdrant/storage\n\n  app:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    env_file:\n      - .env\n    environment:\n      # Внутри docker-сети ходим по именам сервисов, а не localhost\n      APP_CONFIG__DB__URL: postgresql+asyncpg://user:password@pg:5432/shop\n      APP_CONFIG__QDRANT__HOST: qdrant\n      APP_CONFIG__QDRANT__PORT: \"6333\"\n      # если в твоём конфиге почта берётся из APP_CONFIG__EMAIL__*, можешь не задавать\n      APP_CONFIG__FSNB__HF_EMBED_DEVICE: cuda\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n    depends_on:\n      - pg\n      - qdrant\n      - mailhog\n    ports:\n      - \"8015:8000\"\n    volumes:\n      # данные и веса монтируем, чтобы не тянуть в образ\n      - ./FSNB-2022_28_08_25:/app/FSNB-2022_28_08_25\n      - ./weights:/app/weights\n      - ./static:/app/static\n\nvolumes:\n  pgdata:\n  qdrant_data:\n"
    },
    {
      "path": "dump_project.py",
      "language": "python",
      "size_bytes": 13809,
      "sha256": "e9f69f517befeebb168e0bc8a5bcb22c04638d6fd606309063214cde0d50d1f5",
      "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\ndump_project.py — собирает код и структуру проекта в JSON для LLM.\n\nЗапуск:\n  python dump_project.py \\\n    --out project_dump.json \\\n    --max-bytes 800000 \\\n    --tree-depth 5\n\nПо умолчанию:\n  - сохраняет structure.txt (через `tree`, если доступен; иначе — питоновская обводка);\n  - игнорирует бинарные/медийные файлы и стандартные служебные директории;\n  - кладёт весь текстовый код в JSON с хэшами и размерами;\n  - добавляет поля context.static_instructions и context.current_objectives.\n\nСоветы:\n  - Для больших репозиториев увеличьте --max-bytes при необходимости.\n  - Если нужно включить доп. расширения — правьте TEXT_EXTENSIONS или передавайте --include-ext.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport argparse\nimport hashlib\nimport json\nimport os\nimport shutil\nimport subprocess\nimport sys\nfrom datetime import datetime\nfrom fnmatch import fnmatch\nfrom pathlib import Path\nfrom typing import Iterable, List, Dict, Any, Set\n\n# --- Настройки по умолчанию ---\n\nDEFAULT_IGNORE_GLOBS: List[str] = [\n    # из запроса\n    \"__pycache__\",\n    \"*.sample\",\n    \"*.txt\",  # можно убрать, если нужны README/тексты\n    \"*.log\",\n    \"*.pdf\",\n    \"*.jpg\", \"*.jpeg\", \"*.svg\", \"*.png\",\n    \"*.pt\",\n    \"venv\", \".venv\", \"env\", \".env\", \"media\",\n    \"objects\",  #  \"dataset\",\n    \"__init__.pyc\",\n\n    # типичные служебные/тяжёлые каталоги\n    \".git\", \".idea\", \".vscode\", \".mypy_cache\", \".pytest_cache\",\n    \".ruff_cache\", \".cache\", \".tox\",\n    \"node_modules\", \"dist\", \"build\", \".next\", \".nuxt\", \".turbo\",\n    \"*.egg-info\", \"models\",\n]\n\n# Каталоги, которые игнорируем только на верхнем уровне корня проекта\nROOT_ONLY_IGNORE_DIRS: Set[str] = {\"dataset\"}\n\n# Текстовые расширения, которые считаем безопасными для LLM\nTEXT_EXTENSIONS: Set[str] = {\n    # код\n    \".py\", \".pyi\", \".ipynb\",\n    \".js\", \".jsx\", \".ts\", \".tsx\",\n    \".html\", \".htm\", \".css\", \".scss\", \".sass\",\n    \".vue\", \".svelte\",\n    \".java\", \".kt\", \".kts\", \".swift\", \".go\", \".rs\",\n    \".c\", \".h\", \".cpp\", \".hpp\", \".cc\",\n    \".cs\", \".vb\",\n    \".php\", \".rb\", \".r\", \".m\", \".mm\", \".jl\", \".pl\", \".lua\",\n\n    # конфиги/данные\n    \".json\", \".jsonc\", \".yaml\", \".yml\", \".toml\", \".ini\", \".cfg\", \".conf\", \".env.example\",\n    \".sql\", \".graphql\",\n    \".sh\", \".bash\", \".zsh\", \".ps1\", \".bat\", \".cmd\", \"Dockerfile\", \".dockerfile\",\n    \".gitignore\", \".gitattributes\", \".editorconfig\",\n\n    # документация\n    \".md\", \".rst\", \".adoc\",\n}\n\n# Предельно допустимый размер текстового файла (байт) — чтобы не раздувать JSON\nDEFAULT_MAX_BYTES = 500_000\n\n\ndef parse_args() -> argparse.Namespace:\n    p = argparse.ArgumentParser(description=\"Собрать код и структуру проекта в JSON для LLM\")\n    p.add_argument(\"--root\", default=\".\", help=\"Корень проекта (по умолчанию текущая директория)\")\n    p.add_argument(\"--out\", default=\"project_dump.json\", help=\"Путь к JSON-выводу\")\n    p.add_argument(\"--max-bytes\", type=int, default=DEFAULT_MAX_BYTES, help=\"Макс. размер файла для включения\")\n    p.add_argument(\"--tree-depth\", type=int, default=5, help=\"Глубина для вывода структуры (tree)\")\n    p.add_argument(\"--no-structure\", action=\"store_true\", help=\"Не сохранять structure.txt\")\n    p.add_argument(\"--include-ext\", nargs=\"*\", default=None,\n                   help=\"Доп. расширения для включения (пример: .lock .txt)\")\n    p.add_argument(\"--extra-ignore\", nargs=\"*\", default=None,\n                   help=\"Доп. glob-шаблоны игнора (пример: secrets/* *.lock)\")\n    return p.parse_args()\n\n\ndef load_text(path: Path, max_bytes: int) -> str | None:\n    try:\n        if path.stat().st_size > max_bytes:\n            return None\n        # читаем как текст; невалидные байты заменяем\n        return path.read_text(encoding=\"utf-8\", errors=\"replace\")\n    except Exception:\n        return None\n\n\ndef sha256_text(s: str) -> str:\n    return hashlib.sha256(s.encode(\"utf-8\", errors=\"replace\")).hexdigest()\n\n\ndef is_ignored(path: Path, rel: str, ignore_globs: Iterable[str]) -> bool:\n    name = path.name\n    parts = Path(rel).parts\n    if parts and parts[0] in ROOT_ONLY_IGNORE_DIRS:\n        return True\n    for pattern in ignore_globs:\n        # Совпадение по имени файла/папки или по относительному пути\n        if fnmatch(name, pattern) or fnmatch(rel, pattern):\n            return True\n        # Для директорий допустим точное совпадение сегмента\n        # (например, 'venv' игнорирует любой каталог с таким именем)\n        # parts = Path(rel).parts\n        if pattern in parts:\n            return True\n    return False\n\n\ndef detect_language(ext: str, name: str) -> str:\n    # Очень грубая эвристика — достаточно для LLM-навигации\n    if name.lower() in {\"dockerfile\", \"compose.yaml\", \"compose.yml\"}:\n        return \"docker\"\n    MAP = {\n        \".py\": \"python\", \".js\": \"javascript\", \".ts\": \"typescript\", \".tsx\": \"typescriptreact\",\n        \".jsx\": \"javascriptreact\", \".html\": \"html\", \".css\": \"css\", \".scss\": \"scss\",\n        \".json\": \"json\", \".yaml\": \"yaml\", \".yml\": \"yaml\", \".toml\": \"toml\", \".ini\": \"ini\",\n        \".md\": \"markdown\", \".rst\": \"restructuredtext\", \".adoc\": \"asciidoc\",\n        \".sh\": \"bash\", \".bash\": \"bash\", \".zsh\": \"zsh\", \".ps1\": \"powershell\",\n        \".sql\": \"sql\", \".graphql\": \"graphql\", \".go\": \"go\", \".rs\": \"rust\",\n        \".c\": \"c\", \".h\": \"c\", \".cpp\": \"cpp\", \".hpp\": \"cpp\", \".cc\": \"cpp\",\n        \".java\": \"java\", \".kt\": \"kotlin\", \".kts\": \"kotlin\", \".swift\": \"swift\",\n        \".rb\": \"ruby\", \".php\": \"php\", \".cs\": \"csharp\", \".m\": \"objectivec\", \".mm\": \"objectivecpp\",\n        \".lua\": \"lua\", \".r\": \"r\", \".pl\": \"perl\", \".jl\": \"julia\",\n    }\n    return MAP.get(ext.lower(), ext.lower().lstrip(\".\") or \"text\")\n\n\ndef run_tree(root: Path, depth: int) -> str:\n    \"\"\"\n    Рендер структуры, уважающий DEFAULT_IGNORE_GLOBS и ROOT_ONLY_IGNORE_DIRS.\n    Специально НЕ используем внешнюю утилиту `tree`, чтобы корректно\n    игнорировать только корневые каталоги (например, dataset в корне, но не services/dataset).\n    \"\"\"\n    lines: List[str] = []\n    base = root.resolve()\n    max_depth = depth\n\n    for curr, dirs, files in os.walk(base):\n        rel = Path(curr).relative_to(base)\n        d = len(rel.parts)\n\n        # 1) На верхнем уровне — вырезаем только ROOT_ONLY_IGNORE_DIRS\n        if d == 0:\n            dirs[:] = [dn for dn in dirs if dn not in ROOT_ONLY_IGNORE_DIRS]\n\n        # 2) Вырезаем прочие игноры НА ВСЕХ уровнях (по нашим же правилам)\n        pruned_dirs: List[str] = []\n        for dn in dirs:\n            full = Path(curr) / dn\n            rel_dir = full.relative_to(base).as_posix()\n            if is_ignored(full, rel_dir, DEFAULT_IGNORE_GLOBS):\n                continue\n            pruned_dirs.append(dn)\n        dirs[:] = pruned_dirs\n\n        if d > max_depth:\n            dirs[:] = []\n            continue\n\n        indent = \"  \" * d\n        name = \".\" if rel == Path(\".\") else rel.as_posix()\n        lines.append(f\"{indent}{name}/\")\n\n        for f in sorted(files):\n            full = Path(curr) / f\n            rel_file = full.relative_to(base).as_posix()\n            if is_ignored(full, rel_file, DEFAULT_IGNORE_GLOBS):\n                continue\n            lines.append(f\"{indent}  {f}\")\n\n    return \"\\n\".join(lines)\n\n\ndef should_take_file(path: Path, include_ext: Set[str]) -> bool:\n    name = path.name\n    ext = path.suffix\n    if name.lower() == \"dockerfile\":\n        return True\n    return (ext in include_ext) or (ext in TEXT_EXTENSIONS)\n\n\ndef main() -> None:\n    args = parse_args()\n    root = Path(args.root).resolve()\n\n    ignore_globs = list(DEFAULT_IGNORE_GLOBS)\n    if args.extra_ignore:\n        ignore_globs.extend(args.extra_ignore)\n\n    include_ext: Set[str] = set()\n    if args.include_ext:\n        include_ext.update({e if e.startswith(\".\") else f\".{e}\" for e in args.include_ext})\n\n    files_out: List[Dict[str, Any]] = []\n\n    for path in sorted(root.rglob(\"*\")):\n        rel = path.relative_to(root).as_posix()\n\n        # игнор директорий целиком\n        if any(part.startswith(\".git\") for part in Path(rel).parts):\n            continue\n        if is_ignored(path, rel, ignore_globs):\n            if path.is_dir():\n                # пропускаем поддеревья через os.walk? rglob сам проглотит; этого достаточно\n                continue\n            else:\n                continue\n\n        if path.is_dir():\n            continue\n        if not path.is_file():\n            continue\n\n        if not should_take_file(path, include_ext):\n            continue\n\n        text = load_text(path, args.max_bytes)\n        if text is None:\n            continue\n\n        lang = detect_language(path.suffix, path.name)\n        entry = {\n            \"path\": rel,\n            \"language\": lang,\n            \"size_bytes\": len(text.encode(\"utf-8\", errors=\"replace\")),\n            \"sha256\": sha256_text(text),\n            \"content\": text,\n        }\n        files_out.append(entry)\n\n    # Структура проекта (текстом)\n    structure_text = None\n    if not args.no_structure:\n        structure_text = run_tree(root, args.tree_depth)\n        try:\n            Path(\"structure.txt\").write_text(structure_text or \"\", encoding=\"utf-8\")\n        except Exception:\n            pass\n\n    # Итоговый JSON для LLM\n    payload: Dict[str, Any] = {\n        \"meta\": {\n            \"generated_at_utc\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n            \"root\": str(root),\n            \"tool\": \"dump_project.py\",\n            \"max_file_bytes\": args.max_bytes,\n            \"total_files\": len(files_out),\n        },\n        \"context\": {\n            # Заполните эти поля один раз и держите «статикой»\n            \"project_description\": \"Краткое описание проекта, архитектура, стек, цели.\",\n            \"static_instructions\": [\n                \"Это команды для обсуждения, rewie, улучшения, оптимизации, модернизации и исправления кода для распознавания pdf-файлов с помощью парсинга и обучения YOLO модели и подготовке на основе этих данных отчетов в excel.\",\n                \"Проанализируй весь код проекта, разберись, что и с чем связано.\",\n                \"Каждый файл, который ты подготовишь, должен быть заполнен.\",\n                \"каждый кусок кода , который ты покажешь должен быть строго указан к какому файлу он принадлежит.\",\n                \"Имена переменных и функций — PEP8.\",\n                \"Писать код с упором на низкое потребление памяти и работу с большими данными.\",\n                \"Единый кастомный JSON-логгер через LoggerAdapter (время, уровень, имя функции, сообщение).\",\n                \"Документировать функции и ключевые участки кода.\",\n                \"Все изменения кода подписывай определенным файлом, в котором мы делаем изменения.\",\n                \"Не приводи допущений, где я сам должен что-то понять и довести дело до конца.\",\n                \"Объясни каждую строку кода.\",\n                \"Предлагай улучшения таким образом, чтобы не порушить существующую логику, которая уже работает.\",\n\n            ],\n            # Это «динамика»: что нужно сделать прямо сейчас\n            \"current_objectives\": [\n                # Примеры:\n                # \"Исправить фильтрацию по периоду в /users/money\",\n                # \"Усилить OCR: заменить pytesseract на ONNXRuntime PP-OCRv3\"\n            ],\n        },\n        \"structure_text\": structure_text,\n        \"files\": files_out,\n    }\n\n    out_path = Path(args.out)\n    out_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n\n    print(f\"[OK] Saved JSON → {out_path} ({len(files_out)} files)\")\n    if structure_text and not args.no_structure:\n        print(f\"[OK] Saved structure → structure.txt\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
      "path": "pyproject.toml",
      "language": "toml",
      "size_bytes": 794,
      "sha256": "b2adc49361283c4f7d963a43b4922f9cd9ff10d3092e11a98910e7823837b267",
      "content": "[tool.poetry]\npackage-mode = false\n\n[tool.poetry.dependencies]\npython = \"^3.12\"\nfastapi = \"^0.121.0\"\nuvicorn = {extras = [\"standard\"], version = \"^0.38.0\"}\npydantic = {extras = [\"email\"], version = \"^2.12.4\"}\npydantic-settings = \"^2.11.0\"\nsqlalchemy = {extras = [\"asyncio\"], version = \"^2.0.44\"}\nasyncpg = \"^0.30.0\"\nalembic = \"^1.17.1\"\norjson = \"^3.11.4\"\njinja2 = \"^3.1.6\"\npasslib = {extras = [\"bcrypt\"], version = \"^1.7.4\"}\npython-jose = {extras = [\"cryptography\"], version = \"^3.5.0\"}\nitsdangerous = \"^2.2.0\"\npython-multipart = \"^0.0.20\"\nbcrypt = \"4.0.1\"\npillow = \"^12.0.0\"\nlxml = \"^6.0.2\"\nopenpyxl = \"^3.1.5\"\nqdrant-client = \"^1.16.2\"\nsentence-transformers = \"^5.2.0\"\ntransformers = \"^4.57.3\"\naccelerate = \"^1.12.0\"\neinops = \"^0.8.1\"\n\n[tool.poetry.group.dev.dependencies]\nblack = \"^25.9.0\"\n\n"
    },
    {
      "path": "src/__init__.py",
      "language": "python",
      "size_bytes": 0,
      "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
      "content": ""
    },
    {
      "path": "src/admin.py",
      "language": "python",
      "size_bytes": 3556,
      "sha256": "95cf8ff21c87df5cce232a05dc5faf419c9c1856dd730e6d3de7b30c13d51c08",
      "content": "# /src/admin.py\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional, Type\n\nfrom sqlalchemy.orm import DeclarativeMeta\n\nfrom src.core.models.user import User\nfrom src.core.models.profile import Profile\nfrom src.core.models.permission import Permission\n\n\n@dataclass\nclass ModelAdmin:\n    \"\"\"\n    Описание модели для админки.\n    \"\"\"\n    model: Type[DeclarativeMeta]\n    slug: str                                     # url-часть, например \"users\"\n    list_display: List[str] = field(default_factory=list)   # колонки в списке\n    form_fields: List[str] = field(default_factory=list)    # редактируемые поля\n    readonly_fields: List[str] = field(default_factory=list)\n    field_labels: Dict[str, str] = field(default_factory=dict)\n    search_fields: List[str] = field(default_factory=list)  # поля для поиска\n    can_create: bool = False\n    can_delete: bool = False\n\n\nclass AdminSite:\n    def __init__(self) -> None:\n        self._registry: Dict[str, ModelAdmin] = {}\n\n    def register(self, model: Type[DeclarativeMeta], /, **kwargs: Any) -> None:\n        slug = kwargs.get(\"slug\") or getattr(model, \"__tablename__\", model.__name__.lower())\n        if slug in self._registry:\n            raise RuntimeError(f\"Slug '{slug}' already registered\")\n        ma = ModelAdmin(model=model, slug=slug, **{k: v for k, v in kwargs.items() if k != \"slug\"})\n        self._registry[slug] = ma\n\n    def get(self, slug: str) -> Optional[ModelAdmin]:\n        return self._registry.get(slug)\n\n    def all(self) -> List[ModelAdmin]:\n        # упорядочим по slug для стабильности\n        return [self._registry[k] for k in sorted(self._registry.keys())]\n\n\nadmin_site = AdminSite()\n\n# --- Регистрация моделей ---\n\nadmin_site.register(\n    User,\n    slug=\"users\",\n    list_display=[\"id\", \"email\", \"username\", \"is_active\"],\n    form_fields=[\"email\", \"username\", \"is_active\", \"activation_key\"],\n    readonly_fields=[\"id\"],\n    field_labels={\"email\": \"E-mail\", \"username\": \"Логин\", \"is_active\": \"Активен\"},\n    search_fields=[\"email\", \"username\"],\n    can_create=False,\n    can_delete=False,\n)\n\nadmin_site.register(\n    Profile,\n    slug=\"profiles\",\n    list_display=[\"id\", \"user_id\", \"nickname\", \"email\", \"verification\"],\n    form_fields=[\"nickname\", \"avatar\", \"first_name\", \"second_name\", \"phone\", \"email\", \"tg_id\", \"tg_nickname\", \"verification\", \"session\"],\n    readonly_fields=[\"id\", \"user_id\", \"avatar\"],  # avatar меняем из профиля пользователя, здесь readonly\n    field_labels={\"verification\": \"Подтвержден\"},\n    search_fields=[\"nickname\", \"email\", \"tg_nickname\", \"phone\"],\n    can_create=False,\n    can_delete=False,\n)\n\nadmin_site.register(\n    Permission,\n    slug=\"permissions\",\n    list_display=[\"id\", \"profile_id\", \"is_superadmin\", \"is_admin\", \"is_staff\", \"is_updater\", \"is_reader\", \"is_user\"],\n    form_fields=[\"is_superadmin\", \"is_admin\", \"is_staff\", \"is_updater\", \"is_reader\", \"is_user\"],\n    readonly_fields=[\"id\", \"profile_id\"],\n    field_labels={\n        \"is_superadmin\": \"Суперпользователь\",\n        \"is_admin\": \"Администратор\",\n        \"is_staff\": \"Сотрудник\",\n        \"is_updater\": \"Обновляющий\",\n        \"is_reader\": \"Читатель\",\n        \"is_user\": \"Пользователь\",\n    },\n    search_fields=[],\n    can_create=False,\n    can_delete=False,\n)\n"
    },
    {
      "path": "src/app_logging.py",
      "language": "python",
      "size_bytes": 2416,
      "sha256": "2dc0267542df2f0a16ad07239e2daa0de47f28b8ea2a3302866739d937adc79d",
      "content": "\"\"\"\n# path: src/app_logging.py\n\nЕдиный JSON-логгер для проекта.\n\nВАЖНО:\n- Файл НЕ должен называться logging.py, иначе он перекрывает стандартный модуль `logging`\n  и ломает Poetry/uvicorn (они импортируют stdlib logging на старте).\n- Здесь реализован LoggerAdapter, который добавляет в JSON:\n  time, level, func, message (+ optional extra).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport os\nimport sys\nfrom datetime import datetime, timezone\nfrom typing import Any, Dict, Optional\n\n\nclass JsonFormatter(logging.Formatter):\n    \"\"\"Форматтер, превращающий LogRecord в JSON строку.\"\"\"\n\n    def format(self, record: logging.LogRecord) -> str:\n        payload: Dict[str, Any] = {\n            \"time\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n            \"level\": record.levelname,\n            \"logger\": record.name,\n            \"func\": record.funcName,\n            \"message\": record.getMessage(),\n        }\n\n        extra: Optional[Dict[str, Any]] = getattr(record, \"extra\", None)\n        if isinstance(extra, dict) and extra:\n            payload.update(extra)\n\n        if record.exc_info:\n            payload[\"exc_info\"] = self.formatException(record.exc_info)\n\n        return json.dumps(payload, ensure_ascii=False)\n\n\nclass JsonLoggerAdapter(logging.LoggerAdapter):\n    \"\"\"LoggerAdapter, который безопасно прокидывает user extra в record.extra.\"\"\"\n\n    def process(self, msg: str, kwargs: Dict[str, Any]):\n        user_extra = kwargs.pop(\"extra\", None)\n        kwargs[\"extra\"] = {\"extra\": user_extra} if user_extra else {}\n        return msg, kwargs\n\n\ndef get_logger(name: str) -> JsonLoggerAdapter:\n    \"\"\"Создаёт/возвращает настроенный JSON-логгер (stdout, idempotent).\"\"\"\n    logger = logging.getLogger(name)\n    logger.propagate = False\n\n    if logger.handlers:\n        return JsonLoggerAdapter(logger, {})\n\n    level_str = os.getenv(\"LOG_LEVEL\", \"INFO\").upper()\n    level = getattr(logging, level_str, logging.INFO)\n    logger.setLevel(level)\n\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setLevel(level)\n    handler.setFormatter(JsonFormatter())\n\n    logger.addHandler(handler)\n    return JsonLoggerAdapter(logger, {})\n"
    },
    {
      "path": "src/core/__init__.py",
      "language": "python",
      "size_bytes": 0,
      "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
      "content": ""
    },
    {
      "path": "src/core/api/__init__.py",
      "language": "python",
      "size_bytes": 237,
      "sha256": "d06a12fc49cb9678acf2e5f6839e19628b1ceb47dca1050a9e19b813729cd4fd",
      "content": "# src/core/api/__init__.py\nfrom fastapi import APIRouter\n\nfrom .api_v1 import router as router_api_v1\nfrom src.core.config import settings\n\nrouter = APIRouter(\n    prefix=settings.api.prefix\n)\nrouter.include_router(\n    router_api_v1,\n)\n"
    },
    {
      "path": "src/core/api/api_v1/__init__.py",
      "language": "python",
      "size_bytes": 622,
      "sha256": "bfeb9cf2faf37d7343a9db5dc28ce2645a5199286ea2ab51dab8428eeb82fe23",
      "content": "# /src/core/api/api_v1/__init__.py\nfrom fastapi import APIRouter\n\nfrom src.core.config import settings\nfrom .users import router as users_router\nfrom .auth import router as auth_router\nfrom src.fsnb_matcher.api.api_v1 import router as fsnb_router\n\nrouter = APIRouter(prefix=settings.api.v1.prefix)\n\n# /api/<v1>/users/...\nrouter.include_router(\n    users_router,\n    prefix=settings.api.v1.users,\n)\n\n# /api/<v1>/auth/...\nrouter.include_router(\n    auth_router,\n    prefix=settings.api.v1.auth,   # <- префикс берём из конфига\n)\n\nrouter.include_router(fsnb_router,  prefix=\"\",       tags=[\"fsnb-match\"])"
    },
    {
      "path": "src/core/api/api_v1/auth.py",
      "language": "python",
      "size_bytes": 1483,
      "sha256": "749291dd48d6be6e586c9fbebc92629c5d4f8ae8a8f19a0cdb39537283f771f3",
      "content": "# /src/core/api/api_v1/auth.py\nfrom __future__ import annotations\n\nfrom typing import Annotated\n\nfrom fastapi import APIRouter, Depends, HTTPException, status\nfrom fastapi.security import OAuth2PasswordRequestForm\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom src.core.config import settings\nfrom src.core.models import db_helper\nfrom src.app_logging import get_logger\nfrom src.core.services.auth_service import AuthService\n\nrouter = APIRouter(prefix=settings.api.v1.auth, tags=[\"auth\"])\nlog = get_logger(\"api.auth\")\n\n\n@router.post(\"/token\")\nasync def auth_token(\n    form: Annotated[OAuth2PasswordRequestForm, Depends()],\n    session: Annotated[AsyncSession, Depends(db_helper.session_getter)],\n):\n    \"\"\"\n    Точка входа OAuth2 Password:\n    - Принимает form.username (email) и form.password (x-www-form-urlencoded)\n    - Возвращает {\"access_token\": \"...\", \"token_type\": \"bearer\"}\n    \"\"\"\n    service = AuthService()\n    try:\n        token = await service.authenticate(session, email=form.username.strip().lower(), password=form.password)\n        return {\"access_token\": token, \"token_type\": \"bearer\"}\n    except ValueError:\n        # 401 — неправильные креды\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid email or password\")\n    except Exception as e:\n        log.info({\"event\": \"auth_token_fail\", \"error\": str(e)})\n        raise HTTPException(status_code=500, detail=\"Auth failed\")\n"
    },
    {
      "path": "src/core/api/api_v1/users.py",
      "language": "python",
      "size_bytes": 1028,
      "sha256": "84de5055cb1cc4e6b8a61b7749ae75287b564b6b0d64287c0d36fb4d24713cc7",
      "content": "# src/core/api/api_v1/users.py\nfrom typing import Annotated\n\nfrom fastapi import APIRouter\nfrom fastapi.params import Depends\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom src.core.models import db_helper\nfrom src.core.schemas.user import UserRead, UserCreate\nfrom src.crud import user_repository as users_crud\n\nrouter = APIRouter(tags=[\"Users\"])\n\n\n@router.get(\"\", response_model=list[UserRead])\nasync def get_users(\n    session: Annotated[AsyncSession, Depends(db_helper.session_getter)],\n):\n    # функция появится в user_repository (см. ниже)\n    users = await users_crud.get_all_users(session=session)\n    return users\n\n\n@router.post(\"\", response_model=UserRead)\nasync def create_user(\n    session: Annotated[AsyncSession, Depends(db_helper.session_getter)],\n    user_create: UserCreate,\n):\n    # тонкая обёртка над репозиторием: создаём User+Profile+Permission\n    user = await users_crud.create_user(session=session, user_create=user_create)\n    return user\n"
    },
    {
      "path": "src/core/config.py",
      "language": "python",
      "size_bytes": 2843,
      "sha256": "8af6b143195eae5bfe1171a209d52ce0777e87a5de503a161ab275dae0a0d845",
      "content": "# src/core/config.py\nfrom pydantic import BaseModel\nfrom pydantic import PostgresDsn\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom pathlib import Path\nfrom typing import Literal\n\n\nclass RunConfig(BaseModel):\n    host: str = \"0.0.0.0\"\n    port: int = 8015\n\n\nclass ApiV1Prefix(BaseModel):\n    prefix: str = \"/v1\"\n    users: str = \"/users\"\n    auth: str = \"/auth\"\n\n\nclass ApiPrefix(BaseModel):\n    prefix: str = \"/api\"\n    v1: ApiV1Prefix = ApiV1Prefix()\n\n\nclass DatabaseConfig(BaseModel):\n    url: PostgresDsn\n    echo: bool = False\n    echo_pool: bool = False\n    pool_size: int = 50\n    max_overflow: int = 10\n\n    naming_convention: dict[str, str] = {\n        \"ix\": \"ix_%(column_0_label)s\",\n        \"uq\": \"uq_%(table_name)s_%(column_0_N_name)s\",\n        \"ck\": \"ck_%(table_name)s_%(constraint_name)s\",\n        \"fk\": \"fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s\",\n        \"pk\": \"pk_%(table_name)s\",\n    }\n\nclass AuthConfig(BaseModel):\n    secret_key: str = \"CHANGE_ME\"                # общий секрет (JWT/CSRF/сессии)\n    algorithm: str = \"HS256\"\n    access_token_minutes: int = 60\n    email_verify_secret: str = \"CHANGE_ME_EMAIL\" # отдельный секрет для ссылок\n    verify_token_ttl_hours: int = 48\n\n\nclass EmailConfig(BaseModel):\n    smtp_host: str = \"localhost\"\n    smtp_port: int = 25\n    smtp_user: str = \"\"\n    smtp_password: str = \"\"\n    use_tls: bool = False\n    use_ssl: bool = False\n    from_email: str = \"noreply@example.com\"\n\n\nclass SiteConfig(BaseModel):\n    # если нужно строить абсолютные ссылки вне Request (опционально)\n    base_url: str = \"http://127.0.0.1:8000\"\n\n\nclass QdrantConfig(BaseModel):\n    host: str = \"localhost\"          # в docker-сети будет \"qdrant\"\n    port: int = 6333\n    timeout_s: int = 300\n\nclass FsnbConfig(BaseModel):\n    fsnb_dir: str = \"FSNB-2022_28_08_25\"\n    weights_dir: str = \"weights\"\n    model_giga_dir: str = \"weights/Giga-Embeddings-instruct\"\n    similarity_threshold: float = 0.70\n    embed_batch_size: int = 128\n\n    # Тонкие настройки/инференс\n    gpu_slots: int = 1\n    giga_query_bs: int = 2\n    giga_index_bs: int = 8\n    hf_embed_device: Literal[\"auto\", \"cuda\", \"cpu\"] = \"auto\"\n    hf_embed_fp16: bool = True\n\n\nclass Settings(BaseSettings):\n    model_config = SettingsConfigDict(\n        env_file=(\".env.example\", \".env\"),\n        case_sensitive=False,\n        env_nested_delimiter=\"__\",\n        env_prefix=\"APP_CONFIG__\",\n    )\n    run: RunConfig = RunConfig()\n    api: ApiPrefix = ApiPrefix()\n    db: DatabaseConfig\n\n    auth: AuthConfig = AuthConfig()\n    email: EmailConfig = EmailConfig()\n    site: SiteConfig = SiteConfig()\n\n    qdrant: QdrantConfig = QdrantConfig()\n    fsnb: FsnbConfig = FsnbConfig()\n\nsettings = Settings()\n"
    },
    {
      "path": "src/core/dependencies.py",
      "language": "python",
      "size_bytes": 1230,
      "sha256": "9d2cf33dfdb0b2e9c9da7336588143593d5e2282eaf54dbd82a4f6122d207a07",
      "content": "# /src/core/dependencies.py\nfrom __future__ import annotations\n\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import JWTError\n\nfrom .security import decode_token\nfrom src.app_logging import get_logger\nfrom src.crud import item_repository\n\n# ВАЖНО: tokenUrl должен совпадать с реальным API-роутом получения токена\n# Если у тебя токен выдаёт, например, /api/api_v1/auth/token — укажи его.\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"/api/v1/auth/token\")\nlog = get_logger(\"deps\")\n\n\ndef get_current_subject(token: str = Depends(oauth2_scheme)) -> dict:\n    \"\"\"Возвращает payload токена или 401.\"\"\"\n    try:\n        payload = decode_token(token)\n        return payload\n    except JWTError as e:\n        log.info({\"event\": \"jwt_error\", \"error\": str(e)})\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid or expired token\",\n        )\n\n\ndef get_item_repository():\n    \"\"\"Dependency для работы с таблицей items (используется в fsnb_matcher).\"\"\"\n    return item_repository\n"
    },
    {
      "path": "src/core/email_tokens.py",
      "language": "python",
      "size_bytes": 612,
      "sha256": "b9b0b91724b5295996989bb458dae2ae2c77bb715e94466f912be98f35b589b7",
      "content": "# /src/core/email_tokens.py\nfrom __future__ import annotations\n\nfrom itsdangerous import URLSafeTimedSerializer, BadSignature, SignatureExpired\n\nfrom .config import settings\n\n\ndef _serializer() -> URLSafeTimedSerializer:\n    return URLSafeTimedSerializer(\n        secret_key=settings.auth.email_verify_secret,\n        salt=\"email-verify\",\n    )\n\n\ndef make_email_token(user_id: int) -> str:\n    return _serializer().dumps({\"uid\": user_id})\n\n\ndef read_email_token(token: str) -> dict:\n    max_age = settings.auth.verify_token_ttl_hours * 3600\n    data = _serializer().loads(token, max_age=max_age)\n    return data\n"
    },
    {
      "path": "src/core/mailing/__init__.py",
      "language": "python",
      "size_bytes": 0,
      "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
      "content": ""
    },
    {
      "path": "src/core/mailing/email.py",
      "language": "python",
      "size_bytes": 2011,
      "sha256": "500f201441fd959e8aef41fa76e0c0872bda9cfa50fc4d2873050c9f90389179",
      "content": "# /src/core/mailing/email.py\nfrom __future__ import annotations\n\nimport smtplib\nfrom email.mime.text import MIMEText\n\nfrom src.core.config import settings\nfrom src.app_logging import get_logger\n\nlog = get_logger(\"mail\")\n\n\ndef send_verification_email_sync(to_email: str, verify_link: str) -> bool:\n    \"\"\"\n    Простая синхронная отправка (SMTP). Для продакшена можно вынести в Celery.\n    \"\"\"\n    subject = \"Подтверждение регистрации\"\n    body = (\n        \"Для подтверждения e-mail пройдите по ссылке:\\n\\n\"\n        f\"{verify_link}\\n\\n\"\n        \"Если это были не вы — игнорируйте письмо.\"\n    )\n    msg = MIMEText(body, _charset=\"utf-8\")\n    msg[\"Subject\"] = subject\n    msg[\"From\"] = settings.email.from_email\n    msg[\"To\"] = to_email\n\n    try:\n        if settings.email.use_ssl:\n            with smtplib.SMTP_SSL(\n                settings.email.smtp_host,\n                settings.email.smtp_port,\n                timeout=5\n            ) as s:\n                if settings.email.smtp_user:\n                    s.login(settings.email.smtp_user, settings.email.smtp_password)\n                s.send_message(msg)\n        else:\n            with smtplib.SMTP(\n                settings.email.smtp_host,\n                settings.email.smtp_port,\n                timeout=5\n            ) as s:\n                # безопаснее явно послать EHLO перед STARTTLS\n                s.ehlo()\n                if settings.email.use_tls:\n                    s.starttls()\n                    s.ehlo()\n                if settings.email.smtp_user:\n                    s.login(settings.email.smtp_user, settings.email.smtp_password)\n                s.send_message(msg)\n\n        log.info({\"event\": \"email_sent\", \"to\": to_email})\n        return True\n    except Exception as e:\n        log.info({\"event\": \"email_fail\", \"to\": to_email, \"error\": str(e)})\n        return False\n\n"
    },
    {
      "path": "src/core/schemas/__init__.py",
      "language": "python",
      "size_bytes": 0,
      "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
      "content": ""
    },
    {
      "path": "src/core/schemas/permission.py",
      "language": "python",
      "size_bytes": 353,
      "sha256": "2b79fc2eb308477290e94f6c288f90ac5eb847eb92c85e639e5f0b908e1b39c5",
      "content": "# /src/core/schemas/permission.py\nfrom __future__ import annotations\n\nfrom pydantic import BaseModel, ConfigDict\n\n\nclass PermissionRead(BaseModel):\n    model_config = ConfigDict(from_attributes=True)\n    id: int\n    profile_id: int\n    is_superadmin: bool\n    is_admin: bool\n    is_staff: bool\n    is_updater: bool\n    is_reader: bool\n    is_user: bool\n"
    },
    {
      "path": "src/core/schemas/profile.py",
      "language": "python",
      "size_bytes": 690,
      "sha256": "e15dba6b03a7b9f1801945bc003173b2e3e4f137c534e60cab0687a7075425e9",
      "content": "# /src/core/schemas/profile.py\nfrom __future__ import annotations\n\nfrom typing import Optional, List\n\nfrom pydantic import BaseModel, ConfigDict, EmailStr\n\nfrom .permission import PermissionRead\n\n\nclass ProfileRead(BaseModel):\n    model_config = ConfigDict(from_attributes=True)\n    id: int\n    user_id: int\n    email: Optional[EmailStr] = None\n    nickname: Optional[str] = None\n    avatar: Optional[str] = None\n    first_name: Optional[str] = None\n    second_name: Optional[str] = None\n    phone: Optional[str] = None\n    tg_id: Optional[int] = None\n    tg_nickname: Optional[str] = None\n    verification: bool\n    session: Optional[str] = None\n    permissions: List[PermissionRead] = []\n"
    },
    {
      "path": "src/core/schemas/user.py",
      "language": "python",
      "size_bytes": 393,
      "sha256": "1e9356c4a909f30fad4fca032d922dad25df4998426a48d8f71bbd81416df726",
      "content": "# /src/core/schemas/user.py\nfrom __future__ import annotations\n\nfrom pydantic import BaseModel, ConfigDict, EmailStr\n\n\nclass UserBase(BaseModel):\n    username: str | None = None\n    foo: int = 0\n    bar: int = 0\n\n\nclass UserCreate(BaseModel):\n    email: EmailStr\n    password: str\n\n\nclass UserRead(UserBase):\n    model_config = ConfigDict(from_attributes=True)\n    id: int\n    email: EmailStr\n"
    },
    {
      "path": "src/core/security.py",
      "language": "python",
      "size_bytes": 1619,
      "sha256": "0ca6b03f7b90d7ba4fbdd40798d05e14cc48e7b3a09d51806564dbb0468ee4c8",
      "content": "# /src/core/security.py\nfrom __future__ import annotations\n\nfrom datetime import datetime, timedelta, timezone\nfrom typing import Any, Dict, Optional\n\nfrom jose import jwt, JWTError\nfrom passlib.hash import bcrypt_sha256\n\nfrom src.core.config import settings\n\n\ndef hash_password(password: str) -> str:\n    \"\"\"Хэш пароля через bcrypt_sha256 (снимает лимит 72 байта у bcrypt).\"\"\"\n    return bcrypt_sha256.hash(password)\n\n\ndef verify_password(plain_password: str, hashed_password: str) -> bool:\n    \"\"\"Проверка пароля.\"\"\"\n    if not hashed_password:\n        return False\n    return bcrypt_sha256.verify(plain_password, hashed_password)\n\n\ndef create_access_token(\n    *,\n    subject: int | str,\n    expires_minutes: Optional[int] = None,\n    extra: Optional[Dict[str, Any]] = None,\n) -> str:\n    \"\"\"Создаёт JWT (Bearer).\"\"\"\n    to_encode: Dict[str, Any] = {\"sub\": str(subject)}\n    if extra:\n        to_encode.update(extra)\n    expire = datetime.now(timezone.utc) + timedelta(\n        minutes=expires_minutes or settings.auth.access_token_minutes\n    )\n    to_encode[\"exp\"] = int(expire.timestamp())\n    return jwt.encode(to_encode, settings.auth.secret_key, algorithm=settings.auth.algorithm)\n\n\ndef decode_token(token: str) -> Dict[str, Any]:\n    \"\"\"\n    Декодирует и валидирует JWT. Бросает jose.JWTError при неверной подписи/просрочке.\n    \"\"\"\n    payload = jwt.decode(token, settings.auth.secret_key, algorithms=[settings.auth.algorithm])\n    # payload уже проверен по exp\n    return dict(payload)\n"
    },
    {
      "path": "src/core/services/__init__.py",
      "language": "python",
      "size_bytes": 0,
      "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
      "content": ""
    },
    {
      "path": "src/core/services/auth_service.py",
      "language": "python",
      "size_bytes": 4850,
      "sha256": "8a7a9db769bf06451be8480b0c669213440b10ee988ff30faa8949234855d095",
      "content": "# /src/core/services/auth_service.py\nfrom __future__ import annotations\n\nfrom datetime import datetime, timezone\n\nfrom src.core.config import settings\nfrom src.app_logging import get_logger\nfrom src.core.security import (\n    hash_password,\n    verify_password,\n    create_access_token,\n)\nfrom src.crud.user_repository import UserRepository\n\ntry:\n    # itsdangerous — короткие verify-ссылки\n    from itsdangerous import URLSafeTimedSerializer, BadSignature, SignatureExpired\nexcept Exception:  # pragma: no cover\n    URLSafeTimedSerializer = None  # type: ignore\n\nlog = get_logger(\"service.auth\")\n\n\nclass AuthService:\n    def __init__(self) -> None:\n        self.repo = UserRepository()\n\n    # --- verify-токен для письма ---\n    def _get_serializer(self) -> URLSafeTimedSerializer:\n        if URLSafeTimedSerializer is None:\n            raise RuntimeError(\"itsdangerous is not installed\")\n        # соль можно оставить пустой/константой, главное — один и тот же секрет\n        return URLSafeTimedSerializer(settings.auth.email_verify_secret)\n\n    def make_verify_token(self, *, uid: int, email: str) -> str:\n        s = self._get_serializer()\n        return s.dumps({\"uid\": uid, \"email\": email})\n\n    def read_verify_token(self, token: str) -> dict:\n        s = self._get_serializer()\n        max_age = settings.auth.verify_token_ttl_hours * 3600\n        try:\n            data = s.loads(token, max_age=max_age)\n            return dict(data)\n        except SignatureExpired as e:\n            raise ValueError(\"verify_link_expired\") from e\n        except BadSignature as e:\n            raise ValueError(\"verify_link_bad\") from e\n\n    # --- Регистрация ---\n    async def register_user(self, session, *, email: str, password: str) -> tuple[int, str]:\n        existing = await self.repo.get_by_email(session, email=email)\n        if existing:\n            raise ValueError(\"email_already_exists\")\n\n        user = await self.repo.create_user_with_profile_and_permission(\n            session,\n            email=email,\n            hashed_password=hash_password(password),\n        )\n\n        # Сохраняем verify-токен и метку\n        token = self.make_verify_token(uid=user.id, email=email)\n        user.activation_key = token\n        user.activation_sent_at = datetime.now(tz=timezone.utc)\n        await session.flush()\n\n        log.info({\"event\": \"register_success\", \"email\": email, \"user_id\": user.id})\n        return user.id, token\n\n    # --- Подтверждение e-mail ---\n    async def verify_email(self, session, token: str) -> int:\n        data = self.read_verify_token(token)\n        email = str(data.get(\"email\", \"\")).strip().lower()\n        uid = int(data.get(\"uid\", 0))\n\n        user = await self.repo.get_by_email(session, email=email)\n        if not user or user.id != uid:\n            raise ValueError(\"verify_link_bad_user\")\n\n        profile = await self.repo.get_profile_by_user_id(session, user_id=user.id)\n        if profile:\n            profile.verification = True\n\n        user.activation_key = None\n        await session.flush()\n\n        log.info({\"event\": \"verify_ok\", \"email\": email, \"uid\": uid})\n        return user.id\n\n    # --- Аутентификация (для /auth/login или API) ---\n    async def authenticate(self, session, *, email: str, password: str) -> str:\n        user = await self.repo.get_by_email(session, email=email.strip().lower())\n        if not user:\n            log.info({\"event\": \"auth_fail\", \"reason\": \"user_not_found\", \"email\": email})\n            raise ValueError(\"bad_credentials\")\n\n        if not verify_password(password, user.hashed_password):\n            log.info({\"event\": \"auth_fail\", \"reason\": \"wrong_password\", \"email\": email})\n            raise ValueError(\"bad_credentials\")\n\n        profile = await self.repo.get_profile_by_user_id(session, user_id=user.id)\n        email_verified = bool(profile and profile.verification)\n\n        if not email_verified:\n            log.info({\"event\": \"auth_warn_unverified\", \"email\": email})\n\n        # генерим JWT через общий helper\n        token = create_access_token(\n            subject=email,\n            extra={\n                \"uid\": user.id,\n                \"email_verified\": email_verified,\n            },\n        )\n        log.info({\"event\": \"auth_ok\", \"email\": email, \"uid\": user.id, \"email_verified\": email_verified})\n        return token\n\n    # --- вспомогательный метод для HTML-потока (авто-логин после регистрации) ---\n    def make_access_token(self, *, email: str, uid: int, email_verified: bool) -> str:\n        return create_access_token(\n            subject=email,\n            extra={\"uid\": uid, \"email_verified\": email_verified},\n        )\n"
    },
    {
      "path": "src/core/utils/__init__.py",
      "language": "python",
      "size_bytes": 133,
      "sha256": "2cd7735669ecb97987f51cdb710065d6815b13f29cf16492ed511f8afd0695ce",
      "content": "# /src/core/services/__init__.py\n__all__ = (\n    \"camel_case_to_snake_case\",\n)\n\nfrom .case_converter import camel_case_to_snake_case\n"
    },
    {
      "path": "src/core/utils/case_converter.py",
      "language": "python",
      "size_bytes": 956,
      "sha256": "e8afbbb46723ee1fbc0a96c581919afc05422fdc5b32f1b73deee5cccddf9cbd",
      "content": "# /src/core/services/case_service.py\n\"\"\"\nTaken from\nhttps://github.com/mahenzon/ri-sdk-python-wrapper/blob/master/ri_sdk_codegen/utils/case_converter.py\n\"\"\"\n\n\ndef camel_case_to_snake_case(input_str: str) -> str:\n    \"\"\"\n    >>> camel_case_to_snake_case(\"SomeSDK\")\n    'some_sdk'\n    >>> camel_case_to_snake_case(\"RServoDrive\")\n    'r_servo_drive'\n    >>> camel_case_to_snake_case(\"SDKDemo\")\n    'sdk_demo'\n    \"\"\"\n    chars = []\n    for c_idx, char in enumerate(input_str):\n        if c_idx and char.isupper():\n            nxt_idx = c_idx + 1\n            # idea of the flag is to separate abbreviations\n            # as new words, show them in lower case\n            flag = nxt_idx >= len(input_str) or input_str[nxt_idx].isupper()\n            prev_char = input_str[c_idx - 1]\n            if prev_char.isupper() and flag:\n                pass\n            else:\n                chars.append(\"_\")\n        chars.append(char.lower())\n    return \"\".join(chars)\n"
    },
    {
      "path": "src/core/views/__init__.py",
      "language": "python",
      "size_bytes": 370,
      "sha256": "b9db6a7af34291f68d2ef4bbd299ae2d6a3784c320a9d14f9160ab4c54485798",
      "content": "# /src/core/views/__init__.py\nfrom fastapi import APIRouter\nfrom .web import router as web_router\nfrom .auth import router as auth_router\nfrom .admin import router as admin_router\n\n# Единая точка подключения HTML-вьюх\nrouter = APIRouter()\nrouter.include_router(web_router)\nrouter.include_router(auth_router)\nrouter.include_router(admin_router)\n"
    },
    {
      "path": "src/core/views/admin.py",
      "language": "python",
      "size_bytes": 11710,
      "sha256": "3f51269d84b7b5c8e7393287212e947b1fbffc0ef74f6ad90db91c13ae7e8fdd",
      "content": "# /src/core/views/admin.py\nfrom __future__ import annotations\n\nimport secrets\nfrom pathlib import Path\nfrom typing import Annotated, Optional, Any, Dict\n\nfrom fastapi import APIRouter, Request, Depends, Form, status, HTTPException\nfrom fastapi.responses import RedirectResponse\nfrom fastapi.templating import Jinja2Templates\nfrom sqlalchemy import select, update, or_\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.inspection import inspect as sa_inspect\nfrom sqlalchemy.sql.schema import Column\n\nfrom src.app_logging import get_logger\nfrom src.core.models import db_helper\nfrom src.core.models.user import User\nfrom src.core.models.profile import Profile\nfrom src.core.models.permission import Permission\nfrom src.core.security import verify_password\nfrom src.admin import admin_site\n\nrouter = APIRouter()\nlog = get_logger(\"views.admin\")\n\nTEMPLATES_DIR = Path(__file__).resolve().parents[2] / \"templates\"\ntemplates = Jinja2Templates(directory=str(TEMPLATES_DIR))\n# ДАДИМ Jinja функцию attr(obj, name) = getattr(obj, name, None)\ntemplates.env.globals.update(attr=lambda o, n: getattr(o, n, None))\n\n\ndef _ensure_csrf(request: Request) -> str:\n    token = request.session.get(\"admin_csrf\")\n    if not token:\n        token = secrets.token_urlsafe(16)\n        request.session[\"admin_csrf\"] = token\n    return token\n\n\ndef _admin_identity(request: Request) -> Optional[int]:\n    return request.session.get(\"admin_user_id\")\n\n\nasync def _require_admin(request: Request, session: AsyncSession) -> Optional[User]:\n    admin_uid = _admin_identity(request)\n    if not admin_uid:\n        return None\n    u = await session.get(User, admin_uid)\n    if not u:\n        return None\n\n    prof = (await session.execute(select(Profile).where(Profile.user_id == u.id))).scalar_one_or_none()\n    if not prof:\n        return None\n    perm = (await session.execute(select(Permission).where(Permission.profile_id == prof.id))).scalar_one_or_none()\n    if not perm or not (perm.is_superadmin or perm.is_admin):\n        return None\n    return u\n\n\n# ------------- ЛОГИН/ЛОГАУТ -------------\n\n@router.get(\"/admin/login\", name=\"admin_login\")\nasync def admin_login_get(request: Request):\n    csrf = _ensure_csrf(request)\n    return templates.TemplateResponse(\"admin/login.html\", {\"request\": request, \"csrf\": csrf})\n\n\n@router.post(\"/admin/login\", name=\"admin_login_post\")\nasync def admin_login_post(\n    request: Request,\n    session: Annotated[AsyncSession, Depends(db_helper.session_getter)],\n    username: Annotated[str, Form(...)],\n    password: Annotated[str, Form(...)],\n    csrf_token: Annotated[str, Form(...)],\n):\n    if csrf_token != request.session.get(\"admin_csrf\"):\n        return templates.TemplateResponse(\n            \"admin/login.html\",\n            {\"request\": request, \"csrf\": _ensure_csrf(request), \"alert\": {\"kind\": \"error\", \"text\": \"CSRF error\"}},\n            status_code=400,\n        )\n    username = (username or \"\").strip()\n    q = await session.execute(select(User).where(User.username == username))\n    user = q.scalar_one_or_none()\n    if not user or not verify_password(password or \"\", user.hashed_password or \"\"):\n        return templates.TemplateResponse(\n            \"admin/login.html\",\n            {\"request\": request, \"csrf\": _ensure_csrf(request), \"alert\": {\"kind\": \"error\", \"text\": \"Invalid creds\"}},\n            status_code=400,\n        )\n\n    prof = (await session.execute(select(Profile).where(Profile.user_id == user.id))).scalar_one_or_none()\n    perm = None\n    if prof:\n        perm = (await session.execute(select(Permission).where(Permission.profile_id == prof.id))).scalar_one_or_none()\n\n    if not perm or not (perm.is_superadmin or perm.is_admin):\n        return templates.TemplateResponse(\n            \"admin/login.html\",\n            {\"request\": request, \"csrf\": _ensure_csrf(request), \"alert\": {\"kind\": \"error\", \"text\": \"No admin rights\"}},\n            status_code=403,\n        )\n\n    request.session[\"admin_user_id\"] = user.id\n    log.info({\"event\": \"admin_login_ok\", \"user_id\": user.id, \"username\": username})\n    return RedirectResponse(url=\"/admin\", status_code=status.HTTP_303_SEE_OTHER)\n\n\n@router.post(\"/admin/logout\", name=\"admin_logout\")\nasync def admin_logout_post(request: Request):\n    request.session.pop(\"admin_user_id\", None)\n    return RedirectResponse(url=\"/admin/login\", status_code=status.HTTP_303_SEE_OTHER)\n\n\n# ------------- ИНДЕКС -------------\n\n@router.get(\"/admin\", name=\"admin_index\")\nasync def admin_index(\n    request: Request,\n    session: Annotated[AsyncSession, Depends(db_helper.session_getter)],\n):\n    me = await _require_admin(request, session)\n    if not me:\n        return RedirectResponse(url=\"/admin/login\", status_code=status.HTTP_303_SEE_OTHER)\n\n    # модели из реестра\n    models = [{\"slug\": m.slug, \"model_name\": m.model.__name__} for m in admin_site.all()]\n    return templates.TemplateResponse(\n        \"admin/index.html\",\n        {\"request\": request, \"me\": me, \"models\": models},\n    )\n\n\n# ------------- ГЕНЕРИК: СПИСОК -------------\n\n@router.get(\"/admin/m/{slug}\", name=\"admin_model_list\")\nasync def admin_model_list(\n    request: Request,\n    slug: str,\n    session: Annotated[AsyncSession, Depends(db_helper.session_getter)],\n    q: str | None = None,\n):\n    me = await _require_admin(request, session)\n    if not me:\n        return RedirectResponse(url=\"/admin/login\", status_code=status.HTTP_303_SEE_OTHER)\n\n    ma = admin_site.get(slug)\n    if not ma:\n        raise HTTPException(status_code=404, detail=\"Model not registered\")\n\n    Model = ma.model\n    stmt = select(Model)\n\n    # Поиск\n    if q and ma.search_fields:\n        insp = sa_inspect(Model)\n        clauses = []\n        for f in ma.search_fields:\n            col = getattr(Model, f, None)\n            if col is not None:\n                clauses.append(getattr(Model, f).ilike(f\"%{q}%\"))\n        if clauses:\n            stmt = stmt.where(or_(*clauses))\n\n    # Сортировка по первичному ключу\n    insp = sa_inspect(Model)\n    pk_cols = insp.primary_key\n    if pk_cols:\n        stmt = stmt.order_by(pk_cols[0])\n\n    rows = (await session.execute(stmt)).scalars().all()\n\n    return templates.TemplateResponse(\n        \"admin/model_list.html\",\n        {\n            \"request\": request,\n            \"slug\": slug,\n            \"model_name\": Model.__name__,\n            \"list_display\": ma.list_display or [c.key for c in sa_inspect(Model).columns],\n            \"rows\": rows,\n            \"q\": q or \"\",\n        },\n    )\n\n\n# ------------- ГЕНЕРИК: ФОРМА РЕДАКТИРОВАНИЯ -------------\n\ndef _coerce_value(col: Column, raw: str | None) -> Any:\n    if raw is None:\n        return None\n    # пустые строки -> None\n    if raw.strip() == \"\":\n        return None\n\n    t = col.type.__class__.__name__.lower()\n    try:\n        if \"boolean\" in t:\n            # чекбоксы обрабатываем отдельно, сюда редко попадем\n            return raw.lower() in (\"1\", \"true\", \"on\", \"yes\")\n        if \"integer\" in t or \"bigint\" in t or \"smallint\" in t:\n            return int(raw)\n        if \"float\" in t or \"numeric\" in t or \"decimal\" in t:\n            return float(raw)\n        # даты/датавремя можно разобрать позже ISO-строкой — оставим строкой\n        return raw\n    except Exception:\n        return raw\n\n\n@router.get(\"/admin/m/{slug}/{obj_id}/edit\", name=\"admin_model_edit\")\nasync def admin_model_edit_get(\n    request: Request,\n    slug: str,\n    obj_id: int,\n    session: Annotated[AsyncSession, Depends(db_helper.session_getter)],\n):\n    me = await _require_admin(request, session)\n    if not me:\n        return RedirectResponse(url=\"/admin/login\", status_code=status.HTTP_303_SEE_OTHER)\n\n    ma = admin_site.get(slug)\n    if not ma:\n        raise HTTPException(status_code=404, detail=\"Model not registered\")\n\n    Model = ma.model\n    obj = await session.get(Model, obj_id)\n    if not obj:\n        return RedirectResponse(url=f\"/admin/m/{slug}\", status_code=status.HTTP_303_SEE_OTHER)\n\n    csrf = _ensure_csrf(request)\n\n    # для чекбокса is_superadmin: редактировать может только супер\n    me_prof = (await session.execute(select(Profile).where(Profile.user_id == me.id))).scalar_one_or_none()\n    me_perm = None\n    if me_prof:\n        me_perm = (await session.execute(select(Permission).where(Permission.profile_id == me_prof.id))).scalar_one_or_none()\n    can_edit_super_flag = bool(me_perm and me_perm.is_superadmin)\n\n    return templates.TemplateResponse(\n        \"admin/model_edit.html\",\n        {\n            \"request\": request,\n            \"slug\": slug,\n            \"model_name\": Model.__name__,\n            \"obj\": obj,\n            \"fields\": ma.form_fields,\n            \"readonly_fields\": ma.readonly_fields,\n            \"labels\": ma.field_labels,\n            \"csrf\": csrf,\n            \"can_edit_super_flag\": can_edit_super_flag,\n        },\n    )\n\n\n@router.post(\"/admin/m/{slug}/{obj_id}/edit\", name=\"admin_model_edit_post\")\nasync def admin_model_edit_post(\n    request: Request,\n    slug: str,\n    obj_id: int,\n    session: Annotated[AsyncSession, Depends(db_helper.session_getter)],\n    csrf_token: Annotated[str, Form(...)],\n):\n    me = await _require_admin(request, session)\n    if not me:\n        return RedirectResponse(url=\"/admin/login\", status_code=status.HTTP_303_SEE_OTHER)\n    if csrf_token != request.session.get(\"admin_csrf\"):\n        return RedirectResponse(url=f\"/admin/m/{slug}/{obj_id}/edit\", status_code=status.HTTP_303_SEE_OTHER)\n\n    ma = admin_site.get(slug)\n    if not ma:\n        raise HTTPException(status_code=404, detail=\"Model not registered\")\n\n    Model = ma.model\n    obj = await session.get(Model, obj_id)\n    if not obj:\n        return RedirectResponse(url=f\"/admin/m/{slug}\", status_code=status.HTTP_303_SEE_OTHER)\n\n    insp = sa_inspect(Model)\n    columns: Dict[str, Column] = {c.key: c for c in insp.columns}\n    vals: Dict[str, Any] = {}\n\n    # Права на редактирование супер-флага\n    me_prof = (await session.execute(select(Profile).where(Profile.user_id == me.id))).scalar_one_or_none()\n    me_perm = None\n    if me_prof:\n        me_perm = (await session.execute(select(Permission).where(Permission.profile_id == me_prof.id))).scalar_one_or_none()\n    actor_is_super = bool(me_perm and me_perm.is_superadmin)\n\n    # Чекбоксы: соберём все как presence->bool\n    # Остальные поля читаем через _coerce_value\n    for f in ma.form_fields:\n        if f not in columns:\n            continue\n        col = columns[f]\n        tname = col.type.__class__.__name__.lower()\n        if \"boolean\" in tname:\n            raw = (await request.form()).get(f)  # \"on\" или None\n            vals[f] = bool(raw == \"on\")\n        else:\n            raw = (await request.form()).get(f)\n            vals[f] = _coerce_value(col, raw)\n\n    # Если редактируем Permission — не давать менять is_superadmin, если актор не супер\n    if Model is Permission and not actor_is_super and \"is_superadmin\" in vals:\n        vals.pop(\"is_superadmin\", None)\n\n    # Никогда не пишем readonly\n    for ro in ma.readonly_fields:\n        vals.pop(ro, None)\n\n    if vals:\n        await session.execute(update(Model).where(insp.primary_key[0] == obj_id).values(**vals))\n        await session.commit()\n        log.info({\"event\": \"admin_model_update\", \"slug\": slug, \"obj_id\": obj_id, **vals})\n\n    return RedirectResponse(url=f\"/admin/m/{slug}/{obj_id}/edit\", status_code=status.HTTP_303_SEE_OTHER)\n"
    },
    {
      "path": "src/core/views/auth.py",
      "language": "python",
      "size_bytes": 12894,
      "sha256": "50e1145d89ad345b22e6a370e9c5a4530e061ce82594b19eb121aef59f978ad1",
      "content": "# /src/core/views/auth.py\nfrom __future__ import annotations\n\nimport secrets\nfrom pathlib import Path\nfrom typing import Annotated\n\nfrom fastapi import APIRouter, Request, Depends, Form, BackgroundTasks, status\nfrom fastapi.responses import RedirectResponse\nfrom fastapi.templating import Jinja2Templates\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.exc import IntegrityError\n\nfrom src.core.models import db_helper\nfrom src.app_logging import get_logger\nfrom src.core.mailing.email import send_verification_email_sync\nfrom src.core.services.auth_service import AuthService\nfrom src.crud.user_repository import UserRepository  # <-- добавили\n\nrouter = APIRouter()\nlog = get_logger(\"views.auth\")\n\nTEMPLATES_DIR = Path(__file__).resolve().parents[2] / \"templates\"\ntemplates = Jinja2Templates(directory=str(TEMPLATES_DIR))\n\n\ndef _ensure_csrf(request: Request) -> str:\n    token = request.session.get(\"csrf\")\n    if not token:\n        token = secrets.token_urlsafe(16)\n        request.session[\"csrf\"] = token\n    return token\n\n\ndef _new_captcha(request: Request) -> tuple[int, int, int]:\n    a = secrets.randbelow(9) + 1\n    b = secrets.randbelow(9) + 1\n    s = a + b\n    request.session[\"captcha_sum\"] = s\n    return a, b, s\n\n\n# ---------- LOGIN (GET) ----------\n@router.get(\"/auth/login\", name=\"login_html\")\nasync def login_html(request: Request):\n    csrf = _ensure_csrf(request)\n    a, b, _ = _new_captcha(request)\n    log.info({\"event\": \"open_page\", \"path\": \"/auth/login\", \"method\": \"GET\"})\n    return templates.TemplateResponse(\n        \"core/login.html\",\n        {\"request\": request, \"csrf\": csrf, \"a\": a, \"b\": b},\n    )\n\n\n# ---------- LOGIN (POST) ----------\n@router.post(\"/auth/login\", name=\"login_post_html\")\nasync def login_post_html(\n    request: Request,\n    session: Annotated[AsyncSession, Depends(db_helper.session_getter)],\n    email: Annotated[str, Form(...)],\n    password: Annotated[str, Form(...)],\n    csrf_token: Annotated[str, Form(...)],\n    captcha: Annotated[int, Form(...)],\n):\n    # CSRF\n    if csrf_token != request.session.get(\"csrf\"):\n        log.info({\"event\": \"login_fail\", \"reason\": \"csrf\"})\n        csrf = _ensure_csrf(request)\n        a, b, _ = _new_captcha(request)\n        return templates.TemplateResponse(\n            \"core/login.html\",\n            {\"request\": request, \"csrf\": csrf, \"a\": a, \"b\": b,\n             \"alert\": {\"kind\": \"error\", \"text\": \"CSRF error\"}},\n            status_code=400,\n        )\n\n    # Капча\n    try:\n        if int(captcha) != int(request.session.get(\"captcha_sum\", -1)):\n            raise ValueError\n    except Exception:\n        log.info({\"event\": \"login_fail\", \"reason\": \"bad_captcha\"})\n        csrf = _ensure_csrf(request)\n        a, b, _ = _new_captcha(request)\n        return templates.TemplateResponse(\n            \"core/login.html\",\n            {\"request\": request, \"csrf\": csrf, \"a\": a, \"b\": b,\n             \"alert\": {\"kind\": \"error\", \"text\": \"Капча неверна\"}},\n            status_code=400,\n        )\n\n    service = AuthService()\n    repo = UserRepository()\n    email_norm = email.strip().lower()\n\n    try:\n        # Проверяем логин/пароль (важно только отсутствие исключения)\n        await service.authenticate(session, email=email_norm, password=password)\n\n        # Берём модель пользователя\n        user = await repo.get_by_email(session, email=email_norm)\n        if not user:\n            raise ValueError(\"user_not_found_after_auth\")\n\n        # КРИТИЧНО: НЕ лезем в user.profile напрямую (lazyload)!\n        # Явно читаем профиль отдельным запросом:\n        profile = await repo.get_profile_by_user_id(session, user_id=user.id)\n        email_verified = bool(getattr(profile, \"verification\", False))\n\n    except ValueError:\n        log.info({\"event\": \"login_fail\", \"reason\": \"bad_credentials_or_not_found\", \"email\": email_norm})\n        csrf = _ensure_csrf(request)\n        a, b, _ = _new_captcha(request)\n        return templates.TemplateResponse(\n            \"core/login.html\",\n            {\"request\": request, \"csrf\": csrf, \"a\": a, \"b\": b,\n             \"alert\": {\"kind\": \"error\", \"text\": \"Неверный e-mail или пароль\"}},\n            status_code=400,\n        )\n\n    # Успех — кладём токен в сессию и редиректим на главную\n    access_token = service.make_access_token(\n        email=user.email,\n        uid=user.id,\n        email_verified=email_verified,\n    )\n    request.session[\"access_token\"] = access_token\n    request.session[\"user_email\"] = user.email\n    request.session[\"user_id\"] = user.id\n\n    log.info({\"event\": \"login_ok\", \"email\": user.email, \"email_verified\": email_verified})\n    return RedirectResponse(url=\"/\", status_code=status.HTTP_303_SEE_OTHER)\n\n\n# ---------- REGISTER (GET) ----------\n@router.get(\"/auth/register\", name=\"register_html\")\nasync def register_html(request: Request):\n    csrf = _ensure_csrf(request)\n    a, b, _ = _new_captcha(request)\n    log.info({\"event\": \"open_page\", \"path\": \"/auth/register\", \"method\": \"GET\"})\n    return templates.TemplateResponse(\n        \"core/register.html\",\n        {\"request\": request, \"csrf\": csrf, \"a\": a, \"b\": b},\n    )\n\n\n# ---------- REGISTER (POST) ----------\n@router.post(\"/auth/register\", name=\"register_post_html\")\nasync def register_post_html(\n    request: Request,\n    background: BackgroundTasks,\n    session: Annotated[AsyncSession, Depends(db_helper.session_getter)],\n    email: Annotated[str, Form(...)],\n    password: Annotated[str, Form(...)],\n    password2: Annotated[str, Form(...)],\n    csrf_token: Annotated[str, Form(...)],\n    captcha: Annotated[int, Form(...)],\n):\n    # CSRF\n    if csrf_token != request.session.get(\"csrf\"):\n        log.info({\"event\": \"register_fail\", \"email\": email, \"reason\": \"csrf\"})\n        csrf = _ensure_csrf(request)\n        a, b, _ = _new_captcha(request)\n        return templates.TemplateResponse(\n            \"core/register.html\",\n            {\"request\": request, \"csrf\": csrf, \"a\": a, \"b\": b,\n             \"alert\": {\"kind\": \"error\", \"text\": \"CSRF error\"}},\n            status_code=400,\n        )\n\n    # Капча\n    try:\n        if int(captcha) != int(request.session.get(\"captcha_sum\", -1)):\n            raise ValueError\n    except Exception:\n        log.info({\"event\": \"register_fail\", \"email\": email, \"reason\": \"bad_captcha\"})\n        csrf = _ensure_csrf(request)\n        a, b, _ = _new_captcha(request)\n        return templates.TemplateResponse(\n            \"core/register.html\",\n            {\"request\": request, \"csrf\": csrf, \"a\": a, \"b\": b,\n             \"alert\": {\"kind\": \"error\", \"text\": \"Капча неверна\"}},\n            status_code=400,\n        )\n\n    # Пароль\n    email_norm = email.strip().lower()\n    if len(password) < 8 or len(password) > 256 or password != password2:\n        log.info({\"event\": \"register_fail\", \"email\": email_norm, \"reason\": \"weak_or_mismatch_password\"})\n        csrf = _ensure_csrf(request)\n        a, b, _ = _new_captcha(request)\n        return templates.TemplateResponse(\n            \"core/register.html\",\n            {\"request\": request, \"csrf\": csrf, \"a\": a, \"b\": b,\n             \"alert\": {\"kind\": \"error\", \"text\": \"Пароль должен быть 8..256 символов и совпадать.\"}},\n            status_code=400,\n        )\n\n    service = AuthService()\n    try:\n        user_id, verify_token = await service.register_user(session, email=email_norm, password=password)\n        await session.commit()\n\n        # ссылка подтверждения\n        verify_link = str(request.url_for(\"verify_email\", token=verify_token))\n        log.info({\"event\": \"verify_link\", \"email\": email_norm, \"verify_link\": verify_link})\n\n        # письмо в фоне\n        background.add_task(send_verification_email_sync, email_norm, verify_link)\n\n        # авто-логин\n        access_token = service.make_access_token(email=email_norm, uid=user_id, email_verified=False)\n        request.session[\"access_token\"] = access_token\n        request.session[\"user_email\"] = email_norm\n        request.session[\"user_id\"] = user_id\n\n        log.info({\"event\": \"register_success\", \"email\": email_norm, \"user_id\": user_id})\n        return RedirectResponse(url=\"/\", status_code=status.HTTP_303_SEE_OTHER)\n\n    except ValueError as e:\n        # pre-check\n        if str(e) == \"email_already_exists\":\n            log.info({\"event\": \"register_fail\", \"email\": email_norm, \"reason\": \"precheck_exists\"})\n            csrf = _ensure_csrf(request)\n            a, b, _ = _new_captcha(request)\n            return templates.TemplateResponse(\n                \"core/register.html\",\n                {\"request\": request, \"csrf\": csrf, \"a\": a, \"b\": b,\n                 \"alert\": {\"kind\": \"error\", \"text\": \"Пользователь с таким e-mail уже существует\"}},\n                status_code=400,\n            )\n        await session.rollback()\n        log.info({\"event\": \"register_fail\", \"email\": email_norm, \"error\": str(e)})\n        csrf = _ensure_csrf(request)\n        a, b, _ = _new_captcha(request)\n        return templates.TemplateResponse(\n            \"core/register.html\",\n            {\"request\": request, \"csrf\": csrf, \"a\": a, \"b\": b,\n             \"alert\": {\"kind\": \"error\", \"text\": f\"Ошибка регистрации: {e}\"}},\n            status_code=400,\n        )\n    except IntegrityError as e:\n        await session.rollback()\n        cname = getattr(getattr(e, \"orig\", None), \"constraint_name\", None)\n        txt = str(getattr(e, \"orig\", e))\n        human = \"Нарушение целостности данных.\"\n        if \"not-null constraint\" in txt or \"NotNullViolation\" in txt:\n            human = \"Внутренняя ошибка схемы: одно из полей пустое. Обнови миграции (username теперь необязателен).\"\n        elif cname == \"uq_users_email\" or \"uq_users_email\" in txt:\n            human = \"Пользователь с таким e-mail уже зарегистрирован\"\n        elif cname == \"uq_users_username\" or \"uq_users_username\" in txt:\n            human = \"Такой username уже занят\"\n        log.info({\"event\": \"register_fail\", \"email\": email_norm, \"constraint\": cname, \"sql_error\": txt})\n        csrf = _ensure_csrf(request)\n        a, b, _ = _new_captcha(request)\n        return templates.TemplateResponse(\n            \"core/register.html\",\n            {\"request\": request, \"csrf\": csrf, \"a\": a, \"b\": b,\n             \"alert\": {\"kind\": \"error\", \"text\": human}},\n            status_code=400,\n        )\n    except Exception as e:\n        await session.rollback()\n        log.info({\"event\": \"register_fail\", \"email\": email_norm, \"error\": str(e)})\n        csrf = _ensure_csrf(request)\n        a, b, _ = _new_captcha(request)\n        return templates.TemplateResponse(\n            \"core/register.html\",\n            {\"request\": request, \"csrf\": csrf, \"a\": a, \"b\": b,\n             \"alert\": {\"kind\": \"error\", \"text\": \"Не удалось зарегистрировать пользователя\"}},\n            status_code=400,\n        )\n\n\n# ---------- VERIFY EMAIL ----------\n@router.get(\"/auth/verify/{token}\", name=\"verify_email\")\nasync def verify_email(\n    request: Request,\n    token: str,\n    session: Annotated[AsyncSession, Depends(db_helper.session_getter)],\n):\n    service = AuthService()\n    try:\n        uid = await service.verify_email(session, token)\n        await session.commit()\n\n        # если тот же пользователь в сессии — перезальём токен с флагом\n        if request.session.get(\"user_id\") == uid:\n            access_token = service.make_access_token(\n                email=request.session.get(\"user_email\", \"\"),\n                uid=uid,\n                email_verified=True,\n            )\n            request.session[\"access_token\"] = access_token\n\n        log.info({\"event\": \"verify_ok\", \"uid\": uid})\n        alert = {\"kind\": \"success\", \"text\": \"E-mail подтвержден. Спасибо!\"}\n    except Exception as e:\n        await session.rollback()\n        log.info({\"event\": \"verify_fail\", \"error\": str(e)})\n        alert = {\"kind\": \"error\", \"text\": \"Ссылка недействительна или устарела.\"}\n\n    return templates.TemplateResponse(\"core/index.html\", {\"request\": request, \"alert\": alert})\n\n\n# ---------- LOGOUT ----------\n@router.post(\"/auth/logout\", name=\"logout_html\")\n@router.get(\"/auth/logout\")\nasync def logout_html(request: Request):\n    request.session.pop(\"access_token\", None)\n    request.session.pop(\"user_email\", None)\n    request.session.pop(\"user_id\", None)\n    log.info({\"event\": \"logout_ok\"})\n    return RedirectResponse(url=\"/\", status_code=status.HTTP_303_SEE_OTHER)\n"
    },
    {
      "path": "src/core/views/web.py",
      "language": "python",
      "size_bytes": 9770,
      "sha256": "2190369b3dcdd56727968e5f185546473d34e305d63da7166b3bd28cad018754",
      "content": "# /src/core/views/web.py\nfrom __future__ import annotations\n\nimport os\nimport io\nfrom datetime import datetime\nfrom PIL import Image\nfrom pathlib import Path\nfrom typing import Annotated, Optional\n\nfrom fastapi import APIRouter, Request, Depends, UploadFile, File, Form, status\nfrom fastapi.responses import RedirectResponse\nfrom fastapi.templating import Jinja2Templates\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom src.app_logging import get_logger\nfrom src.core.models import db_helper\nfrom src.crud.user_repository import UserRepository, get_all_users\n\nrouter = APIRouter()\nlog = get_logger(\"web\")\n\nTEMPLATES_DIR = Path(__file__).resolve().parents[2] / \"templates\"\ntemplates = Jinja2Templates(directory=str(TEMPLATES_DIR))\n\nSTATIC_DIR = Path(__file__).resolve().parents[2] / \"static\"\nAVATAR_DIR = STATIC_DIR / \"uploads\" / \"avatars\"\nAVATAR_DIR.mkdir(parents=True, exist_ok=True)\n\n# --- Ограничения на аватар ---\nMAX_AVATAR_MB = 3\nMAX_AVATAR_BYTES = MAX_AVATAR_MB * 1024 * 1024\nALLOWED_IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".gif\", \".webp\"}\n\n\n@router.get(\"/\", name=\"home\")\nasync def index_html(request: Request):\n    \"\"\"\n    Главная страница.\n    Имя роута — 'home' (используется в templates/core/_header.html).\n    \"\"\"\n    log.info({\"event\": \"open_page\", \"path\": \"/\", \"method\": \"GET\"})\n    return templates.TemplateResponse(\"core/index.html\", {\"request\": request})\n\n\n@router.get(\"/users/\", name=\"users_list_html\")\nasync def users_list_html(\n    request: Request,\n    session: Annotated[AsyncSession, Depends(db_helper.session_getter)],\n):\n    \"\"\"HTML-страница со списком пользователей.\"\"\"\n    users = await get_all_users(session)\n    log.info({\"event\": \"open_page\", \"path\": \"/users/\", \"method\": \"GET\", \"count\": len(users)})\n    return templates.TemplateResponse(\"users/list.html\", {\"request\": request, \"users\": users})\n\n\ndef _require_logged_in(request: Request) -> Optional[str]:\n    \"\"\"Возвращает email пользователя из сессии, если он залогинен, иначе None.\"\"\"\n    token = request.session.get(\"access_token\")\n    email = request.session.get(\"user_email\")\n    if not token or not email:\n        return None\n    return email\n\n\n@router.get(\"/profile\", name=\"profile_html\")\nasync def profile_html(\n    request: Request,\n    session: Annotated[AsyncSession, Depends(db_helper.session_getter)],\n):\n    \"\"\"Профиль пользователя (форма редактирования).\"\"\"\n    email = _require_logged_in(request)\n    if not email:\n        return RedirectResponse(url=\"/auth/login\", status_code=status.HTTP_303_SEE_OTHER)\n\n    repo = UserRepository()\n    user = await repo.get_by_email(session, email=email)\n    if not user:\n        return RedirectResponse(url=\"/auth/login\", status_code=status.HTTP_303_SEE_OTHER)\n\n    profile = await repo.get_profile_by_user_id(session, user_id=user.id)\n    return templates.TemplateResponse(\"core/profile.html\", {\"request\": request, \"user\": user, \"profile\": profile})\n\n\n@router.post(\"/profile\", name=\"profile_post_html\")\nasync def profile_post_html(\n    request: Request,\n    session: Annotated[AsyncSession, Depends(db_helper.session_getter)],\n    nickname: Annotated[str | None, Form()] = None,\n    first_name: Annotated[str | None, Form()] = None,\n    second_name: Annotated[str | None, Form()] = None,\n    phone: Annotated[str | None, Form()] = None,\n    email_field: Annotated[str | None, Form()] = None,  # поле email профиля (не user.email)\n    tg_id: Annotated[str | None, Form()] = None,\n    tg_nickname: Annotated[str | None, Form()] = None,\n    session_str: Annotated[str | None, Form()] = None,\n    avatar: Annotated[UploadFile | None, File()] = None,\n):\n    \"\"\"Обработка формы профиля. Валидация аватара: тип=image/*, размер ≤ 3 МБ. Перезапись старого файла.\"\"\"\n    def _clean_str(v: str | None) -> str | None:\n        if v is None:\n            return None\n        s = v.strip()\n        return s if s else None\n\n    def _clean_tg_id(v: str | None) -> int | None:\n        if not v:\n            return None\n        s = v.strip()\n        if not s:\n            return None\n        digits = \"\".join(ch for ch in s if ch.isdigit())\n        return int(digits) if digits else None\n\n    email = _require_logged_in(request)\n    if not email:\n        return RedirectResponse(url=\"/auth/login\", status_code=status.HTTP_303_SEE_OTHER)\n\n    repo = UserRepository()\n    user = await repo.get_by_email(session, email=email)\n    if not user:\n        return RedirectResponse(url=\"/auth/login\", status_code=status.HTTP_303_SEE_OTHER)\n\n    profile = await repo.get_profile_by_user_id(session, user_id=user.id)\n    if not profile:\n        return RedirectResponse(url=\"/\", status_code=status.HTTP_303_SEE_OTHER)\n\n    updates: dict[str, object] = {\n        \"nickname\": _clean_str(nickname),\n        \"first_name\": _clean_str(first_name),\n        \"second_name\": _clean_str(second_name),\n        \"phone\": _clean_str(phone),\n        \"email\": _clean_str(email_field),\n        \"tg_id\": _clean_tg_id(tg_id),\n        \"tg_nickname\": _clean_str(tg_nickname),\n        \"session\": _clean_str(session_str),\n    }\n\n    # загрузка аватара\n    if avatar and avatar.filename:\n        # 1) Контент-тайп\n        allowed = {\"image/jpeg\", \"image/png\", \"image/gif\", \"image/webp\"}\n        if avatar.content_type not in allowed:\n            alert = {\"kind\": \"error\", \"text\": \"Только изображения (JPG/PNG/GIF/WebP).\"}\n            return templates.TemplateResponse(\"core/profile.html\",\n                                              {\"request\": request, \"user\": user, \"profile\": profile, \"alert\": alert})\n\n        # 2) Размер\n        content = await avatar.read()\n        MAX_BYTES = 3 * 1024 * 1024\n        if len(content) > MAX_BYTES:\n            alert = {\"kind\": \"error\", \"text\": \"Файл слишком большой. Максимум 3 МБ.\"}\n            return templates.TemplateResponse(\"core/profile.html\",\n                                              {\"request\": request, \"user\": user, \"profile\": profile, \"alert\": alert})\n\n        # 3) Геометрия (минимум 40x40)\n        try:\n            im = Image.open(io.BytesIO(content))\n            im.load()\n            if im.width < 40 or im.height < 40:\n                alert = {\"kind\": \"error\", \"text\": \"Минимальный размер изображения — 40×40 пикселей.\"}\n                return templates.TemplateResponse(\"core/profile.html\",\n                                                  {\"request\": request, \"user\": user, \"profile\": profile,\n                                                   \"alert\": alert})\n        except Exception:\n            alert = {\"kind\": \"error\", \"text\": \"Файл не распознан как изображение.\"}\n            return templates.TemplateResponse(\"core/profile.html\",\n                                              {\"request\": request, \"user\": user, \"profile\": profile, \"alert\": alert})\n\n        # 4) Готовим имя файла и удаляем старый, чтобы не копились\n        ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n        ext = os.path.splitext(avatar.filename)[1].lower()[:8] or \".jpg\"\n        # сохраняем под фиксированным именем на пользователя — это ЗАТРЁТ старый файл:\n        filename = f\"user_{user.id}{ext}\"\n        dst = AVATAR_DIR / filename\n\n        # если был другой файл — удалим его (любой ext)\n        if profile.avatar:\n            try:\n                (STATIC_DIR / profile.avatar).unlink(missing_ok=True)\n            except Exception:\n                pass\n\n        dst.write_bytes(content)\n        updates[\"avatar\"] = f\"uploads/avatars/{filename}\"\n        log.info({\"event\": \"avatar_saved\", \"user_id\": user.id, \"path\": updates[\"avatar\"]})\n\n    await repo.update_profile(session, profile_id=profile.id, **updates)\n    await session.commit()\n    log.info({\"event\": \"profile_updated\", \"user_id\": user.id, \"fields\": [k for k, v in updates.items() if v is not None]})\n\n    return RedirectResponse(url=\"/profile\", status_code=status.HTTP_303_SEE_OTHER)\n\n\n@router.post(\"/profile/avatar/delete\", name=\"profile_avatar_delete\")\nasync def profile_avatar_delete(\n    request: Request,\n    session: Annotated[AsyncSession, Depends(db_helper.session_getter)],\n):\n    \"\"\"Удаляет файл аватара и сбрасывает поле avatar в NULL.\"\"\"\n    email = _require_logged_in(request)\n    if not email:\n        return RedirectResponse(url=\"/auth/login\", status_code=status.HTTP_303_SEE_OTHER)\n\n    repo = UserRepository()\n    user = await repo.get_by_email(session, email=email)\n    if not user:\n        return RedirectResponse(url=\"/auth/login\", status_code=status.HTTP_303_SEE_OTHER)\n\n    profile = await repo.get_profile_by_user_id(session, user_id=user.id)\n    if not profile:\n        return RedirectResponse(url=\"/\", status_code=status.HTTP_303_SEE_OTHER)\n\n    if profile.avatar:\n        path = (STATIC_DIR / profile.avatar).resolve()\n        try:\n            if str(path).startswith(str(AVATAR_DIR.resolve())) and path.exists():\n                path.unlink()\n                log.info({\"event\": \"avatar_deleted\", \"user_id\": user.id, \"path\": str(path)})\n        except Exception as e:\n            log.info({\"event\": \"avatar_delete_failed\", \"user_id\": user.id, \"error\": str(e)})\n\n    await repo.update_profile(session, profile_id=profile.id, avatar=None)\n    await session.commit()\n\n    return RedirectResponse(url=\"/profile\", status_code=status.HTTP_303_SEE_OTHER)\n"
    },
    {
      "path": "src/crud/__init__.py",
      "language": "python",
      "size_bytes": 0,
      "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
      "content": ""
    },
    {
      "path": "src/crud/item_repository.py",
      "language": "python",
      "size_bytes": 7700,
      "sha256": "5e6e64b92934198444a9656bd3b21d368b14f3c1a0c33ee233f089eb7a0fc604",
      "content": "from __future__ import annotations\n\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n)\n\nfrom sqlalchemy import delete, select, text\nfrom sqlalchemy.dialects.postgresql import insert as pg_insert\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom src.fsnb_matcher.models.item import Item\n\n\nRowTuple = Tuple[str, str, Optional[str], str]  # (code, name, unit, type)\n\n\nclass ItemRepository:\n    \"\"\"\n    Репозиторий для таблицы items.\n\n    ВАЖНО:\n    - Старые функции (bulk_insert_items, fetch_*) интегрированы как методы класса\n      и дополнительно оставлены как алиасы-методы, чтобы не ломать существующие вызовы.\n    \"\"\"\n\n    # -----------------------------\n    # НОВЫЕ базовые методы (удобно для CLI/сервисов)\n    # -----------------------------\n    @staticmethod\n    async def truncate(session: AsyncSession) -> None:\n        \"\"\"Быстрый сброс таблицы + sequence.\"\"\"\n        await session.execute(text(\"TRUNCATE TABLE items RESTART IDENTITY;\"))\n        await session.commit()\n\n    @staticmethod\n    async def delete_all(session: AsyncSession) -> int:\n        \"\"\"Мягкая очистка (DELETE), если TRUNCATE не подходит.\"\"\"\n        res = await session.execute(delete(Item))\n        await session.commit()\n        return int(res.rowcount or 0)\n\n    @staticmethod\n    async def count(session: AsyncSession) -> int:\n        res = await session.execute(text(\"SELECT COUNT(*) FROM items;\"))\n        return int(res.scalar_one())\n\n    @staticmethod\n    async def iter_for_index(\n        session: AsyncSession,\n        *,\n        yield_per: int = 2000,\n    ) -> AsyncIterator[Tuple[int, str, Optional[str], Optional[str], str]]:\n        \"\"\"\n        Потоковая выгрузка для индексации:\n        (id, name, code, unit, type)\n\n        execution_options(yield_per=...) работает корректно вместе с session.stream(...)\n        \"\"\"\n        stmt = (\n            select(Item.id, Item.name, Item.code, Item.unit, Item.type)\n            .order_by(Item.id)\n            .execution_options(yield_per=yield_per)\n        )\n\n        stream = await session.stream(stmt)\n        async for row in stream:\n            yield (\n                int(row[0]),\n                str(row[1]),\n                row[2] if row[2] is not None else None,\n                row[3] if row[3] is not None else None,\n                str(row[4]),\n            )\n\n    # -----------------------------\n    # НОВОЕ bulk API (для сервисов)\n    # -----------------------------\n    @staticmethod\n    async def bulk_upsert_dicts(\n        session: AsyncSession,\n        rows: Sequence[Dict[str, Any]],\n    ) -> int:\n        \"\"\"\n        rows: [{\"code\": \"...\", \"name\": \"...\", \"unit\": \"...\", \"type\": \"work\"}, ...]\n        ON CONFLICT(code) DO NOTHING.\n\n        Возвращает число обработанных строк как приближение (как и раньше),\n        потому что rowcount на DO NOTHING бывает нестабилен.\n        \"\"\"\n        if not rows:\n            return 0\n\n        stmt = (\n            pg_insert(Item)\n            .values(list(rows))\n            .on_conflict_do_nothing(index_elements=[Item.code])\n        )\n        await session.execute(stmt)\n        await session.commit()\n        return len(rows)\n\n    # -----------------------------\n    # ✅ ИНТЕГРАЦИЯ СТАРОГО КОДА (bulk_insert_items + _flush)\n    # -----------------------------\n    @classmethod\n    async def bulk_insert_items(\n        cls,\n        session: AsyncSession,\n        rows: Iterable[RowTuple],\n        chunk_size: int = 1000,\n    ) -> int:\n        \"\"\"\n        СТАРОЕ ПОВЕДЕНИЕ (интегрировано):\n\n        Вставка (code, name, unit, type) с ON CONFLICT (code) DO NOTHING.\n        Возвращает кол-во добавленных строк (приближенно) — как раньше.\n\n        ВАЖНО:\n        - Мы не пытаемся \"точно\" посчитать вставки по rowcount, потому что\n          на on_conflict_do_nothing это реально может быть None/нестабильно.\n        \"\"\"\n        buf: List[RowTuple] = []\n        inserted = 0\n\n        for row in rows:\n            buf.append(row)\n            if len(buf) >= chunk_size:\n                inserted += await cls._flush(session, buf)\n                buf.clear()\n\n        if buf:\n            inserted += await cls._flush(session, buf)\n\n        return inserted\n\n    @staticmethod\n    async def _flush(session: AsyncSession, rows: List[RowTuple]) -> int:\n        \"\"\"\n        СТАРЫЙ _flush (интегрирован):\n        - commit делаем здесь, чтобы chunk-и фиксировались сразу.\n        \"\"\"\n        stmt = (\n            pg_insert(Item)\n            .values([{\"code\": c, \"name\": n, \"unit\": u, \"type\": t} for (c, n, u, t) in rows])\n            .on_conflict_do_nothing(index_elements=[Item.code])\n        )\n        await session.execute(stmt)\n        await session.commit()\n        # rowcount может быть None -> возвращаем len(rows) как приближение (как было)\n        return len(rows)\n\n    # -----------------------------\n    # ✅ ИНТЕГРАЦИЯ СТАРЫХ fetch_* ФУНКЦИЙ\n    # -----------------------------\n    @staticmethod\n    async def fetch_all_item_ids_and_names(session: AsyncSession) -> List[Tuple[int, str]]:\n        res = await session.execute(select(Item.id, Item.name).order_by(Item.id))\n        return [(int(i), str(n)) for (i, n) in res.all()]\n\n    @staticmethod\n    async def fetch_item_name_unit_by_id(\n        session: AsyncSession,\n        item_id: int,\n    ) -> Optional[Tuple[str, Optional[str]]]:\n        res = await session.execute(select(Item.name, Item.unit).where(Item.id == int(item_id)))\n        row = res.first()\n        return (str(row[0]), row[1]) if row else None\n\n    @staticmethod\n    async def fetch_item_name_unit_code_by_id(\n        session: AsyncSession,\n        item_id: int,\n    ) -> Optional[Tuple[str, Optional[str], Optional[str]]]:\n        res = await session.execute(select(Item.name, Item.unit, Item.code).where(Item.id == int(item_id)))\n        row = res.first()\n        return (str(row[0]), row[1], row[2]) if row else None\n\n    @staticmethod\n    async def fetch_item_codes(\n        session: AsyncSession,\n        item_ids: Sequence[int],\n    ) -> Dict[int, Optional[str]]:\n        if not item_ids:\n            return {}\n\n        ids = sorted({int(i) for i in item_ids if i is not None})\n        if not ids:\n            return {}\n\n        res = await session.execute(select(Item.id, Item.code).where(Item.id.in_(ids)))\n        return {int(i): (c if c is not None else None) for (i, c) in res.all()}\n\n    # -----------------------------\n    # ✅ СОВМЕСТИМОСТЬ: алиасы (если где-то ожидали функциональный стиль)\n    # -----------------------------\n    # Если в коде раньше было: from src.crud.item_repository import bulk_insert_items\n    # и ты хочешь не переписывать импорты — можно в конце файла оставить функции-обёртки.\n    # Но ты просил \"не сохранять старые функции\" — поэтому алиасы оставляю только методами класса.\n"
    },
    {
      "path": "src/crud/permission_repository.py",
      "language": "python",
      "size_bytes": 653,
      "sha256": "f70385927f69123444c0517bdac6eb809be5ed46fe87718d6b7b48143ac3268a",
      "content": "# /src/crud/permission_repository.py\nfrom __future__ import annotations\n\nfrom typing import Protocol, Sequence\n\nfrom sqlalchemy import select\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom src.core.models.permission import Permission\n\n\nclass IPermissionRepository(Protocol):\n    async def list_for_profile(self, session: AsyncSession, profile_id: int) -> Sequence[Permission]: ...\n\n\nclass PermissionRepository(IPermissionRepository):\n    async def list_for_profile(self, session: AsyncSession, profile_id: int):\n        res = await session.execute(select(Permission).where(Permission.profile_id == profile_id))\n        return list(res.scalars())\n"
    },
    {
      "path": "src/crud/profile_repository.py",
      "language": "python",
      "size_bytes": 1467,
      "sha256": "86c0368cb372d6e60e3d827735984b3575687d33c90c8abc5f21c9a4cde1d09e",
      "content": "# /src/crud/profile_repository.py\nfrom __future__ import annotations\n\nfrom typing import Protocol, Optional, Sequence\n\nfrom sqlalchemy import select\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom src.core.models.profile import Profile\nfrom src.core.models.permission import Permission\n\n\nclass IProfileRepository(Protocol):\n    async def get_by_id(self, session: AsyncSession, profile_id: int) -> Optional[Profile]: ...\n    async def get_by_user_id(self, session: AsyncSession, user_id: int) -> Optional[Profile]: ...\n    async def create_with_defaults(self, session: AsyncSession, *, user_id: int, email: str) -> Profile: ...\n\n\nclass ProfileRepository(IProfileRepository):\n    async def get_by_id(self, session: AsyncSession, profile_id: int) -> Optional[Profile]:\n        res = await session.execute(select(Profile).where(Profile.id == profile_id))\n        return res.scalar_one_or_none()\n\n    async def get_by_user_id(self, session: AsyncSession, user_id: int) -> Optional[Profile]:\n        res = await session.execute(select(Profile).where(Profile.user_id == user_id))\n        return res.scalar_one_or_none()\n\n    async def create_with_defaults(self, session: AsyncSession, *, user_id: int, email: str) -> Profile:\n        profile = Profile(user_id=user_id, email=email, verification=False)\n        profile.permissions = [Permission(is_user=True)]  # базовый флаг\n        session.add(profile)\n        await session.flush()\n        return profile\n"
    },
    {
      "path": "src/crud/user_repository.py",
      "language": "python",
      "size_bytes": 6007,
      "sha256": "133a3a5ac02891501b251f873c76e6324414becdf3a44f28d3044e272ad23071",
      "content": "# /src/crud/user_repository.py\nfrom __future__ import annotations\n\nfrom typing import Optional, Sequence\n\nfrom sqlalchemy import select, update\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.orm import selectinload\n\nfrom src.app_logging import get_logger\nfrom src.core.models import User, Profile, Permission\nfrom src.core.schemas.user import UserCreate\nfrom src.core.security import hash_password\n\nlog = get_logger(\"repo.user\")\n\n\nclass UserRepository:\n    \"\"\"\n    DI-friendly репозиторий: операции с User и связанными сущностями.\n    \"\"\"\n\n    # --- Users ---\n\n    async def get_by_email(self, session: AsyncSession, *, email: str) -> Optional[User]:\n        log.info({\"event\": \"get_by_email\", \"email\": email})\n        stmt = select(User).where(User.email == email)\n        return (await session.execute(stmt)).scalar_one_or_none()\n\n    async def get_by_email_with_related(self, session: AsyncSession, *, email: str) -> Optional[User]:\n        \"\"\"\n        Возвращает пользователя вместе с profile -> permissions (жадная загрузка),\n        чтобы не ловить MissingGreenlet в шаблонах/админке.\n        \"\"\"\n        log.info({\"event\": \"get_by_email\", \"email\": email})\n        stmt = (\n            select(User)\n            .where(User.email == email)\n            .options(\n                selectinload(User.profile).selectinload(Profile.permissions)\n            )\n        )\n        return (await session.execute(stmt)).scalar_one_or_none()\n\n    async def create_user_with_profile_and_permission(\n        self,\n        session: AsyncSession,\n        *,\n        email: str,\n        hashed_password: str,\n    ) -> User:\n        \"\"\"\n        Атомарно создаёт:\n          - User\n          - Profile (1:1)\n          - Permission (базовая роль is_user=True)\n        Коммит — снаружи.\n        \"\"\"\n        log.info({\"event\": \"create_user_start\", \"email\": email})\n\n        user = User(email=email, hashed_password=hashed_password, is_active=True)\n        session.add(user)\n        await session.flush()  # появится user.id\n\n        log.info({\"event\": \"create_profile\", \"user_id\": user.id})\n        profile = Profile(user_id=user.id, email=email, verification=False)\n        session.add(profile)\n        await session.flush()  # появится profile.id\n\n        log.info({\"event\": \"create_permission\", \"profile_id\": profile.id})\n        perm = Permission(\n            profile_id=profile.id,\n            is_superadmin=False,\n            is_admin=False,\n            is_staff=False,\n            is_updater=False,\n            is_reader=False,\n            is_user=True,\n        )\n        session.add(perm)\n        await session.flush()\n\n        log.info({\"event\": \"create_user_done\", \"user_id\": user.id})\n        return user\n\n    # --- Profiles ---\n\n    async def get_profile_by_user_id(self, session: AsyncSession, *, user_id: int) -> Optional[Profile]:\n        stmt = select(Profile).where(Profile.user_id == user_id)\n        return (await session.execute(stmt)).scalar_one_or_none()\n\n    async def update_profile(self, session: AsyncSession, *, profile_id: int, **fields) -> None:\n        \"\"\"\n        Обновляет произвольные поля профиля по id.\n        Пример fields: {\"nickname\": \"...\", \"session\": \"...\", \"avatar\": \"...\"}.\n        Пустые строки предварительно нормализуй в вызывающем коде ('' -> None для tg_id и т.п.).\n        \"\"\"\n        if not fields:\n            return\n        await session.execute(\n            update(Profile)\n            .where(Profile.id == profile_id)\n            .values(**fields)\n        )\n\n    # --- Permissions ---\n\n    async def get_permission_by_profile_id(\n        self, session: AsyncSession, *, profile_id: int\n    ) -> Optional[Permission]:\n        stmt = select(Permission).where(Permission.profile_id == profile_id)\n        return (await session.execute(stmt)).scalar_one_or_none()\n\n    async def create_permission(\n        self, session: AsyncSession, *, profile_id: int, **flags\n    ) -> Permission:\n        perm = Permission(profile_id=profile_id, **flags)\n        session.add(perm)\n        await session.flush()\n        log.info({\"event\": \"permission_create\", \"profile_id\": profile_id, \"flags\": list(flags.keys())})\n        return perm\n\n    async def update_permission(\n        self, session: AsyncSession, *, permission_id: int, **flags\n    ) -> None:\n        if not flags:\n            return\n        await session.execute(\n            update(Permission).where(Permission.id == permission_id).values(**flags)\n        )\n        log.info({\"event\": \"permission_update\", \"permission_id\": permission_id, \"flags\": list(flags.keys())})\n\n\n# --- Функции-обёртки для удобного использования из views / API ---\n\nasync def get_all_users(session: AsyncSession) -> Sequence[User]:\n    \"\"\"\n    Вернёт всех пользователей в порядке убывания id.\n    Параметр назван `session`, чтобы совпадать с вызовами вида get_all_users(session=session).\n    \"\"\"\n    log.info({\"event\": \"list_users\"})\n    res = await session.execute(select(User).order_by(User.id.desc()))\n    return list(res.scalars())\n\n\nasync def create_user(session: AsyncSession, user_create: UserCreate) -> User:\n    \"\"\"\n    Создать пользователя по схеме UserCreate (email+password).\n    \"\"\"\n    repo = UserRepository()\n    email = user_create.email.strip().lower()\n\n    if await repo.get_by_email(session, email=email):\n        log.info({\"event\": \"create_user_exists\", \"email\": email})\n        raise ValueError(\"email_already_exists\")\n\n    user = await repo.create_user_with_profile_and_permission(\n        session,\n        email=email,\n        hashed_password=hash_password(user_create.password),\n    )\n    return user\n"
    },
    {
      "path": "src/fsnb_matcher/__init__.py",
      "language": "python",
      "size_bytes": 66,
      "sha256": "fbf70d1845c74aad04dcd42384f88fc186bdff93366a8808278c994a759c1098",
      "content": "# src/fsnb_matcher/__init__.py\nfrom __future__ import annotations\n"
    },
    {
      "path": "src/fsnb_matcher/api/__init__.py",
      "language": "python",
      "size_bytes": 0,
      "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
      "content": ""
    },
    {
      "path": "src/fsnb_matcher/api/api_v1/__init__.py",
      "language": "python",
      "size_bytes": 244,
      "sha256": "18c52f714edf47530fbc0e1ce6d2fd7491901401f2824633a23cd639991f78e1",
      "content": "# src/fsnb_matcher/api/api_v1/__init__.py\nfrom __future__ import annotations\nfrom fastapi import APIRouter\nfrom .match import router as match_router\n\nrouter = APIRouter()\nrouter.include_router(match_router, prefix=\"/fsnb\", tags=[\"fsnb-match\"])\n"
    },
    {
      "path": "src/fsnb_matcher/api/api_v1/match.py",
      "language": "python",
      "size_bytes": 1485,
      "sha256": "5dd28bc320fe9977860f8d9bb0c98abb72152b3bf1cdeca547c279e5dfec0a0c",
      "content": "# src/fsnb_matcher/api/api_v1/match.py\nfrom __future__ import annotations\nimport json\nfrom fastapi import APIRouter, Depends, File, HTTPException, Request, UploadFile\nfrom fastapi.responses import Response\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom src.core.models import db_helper\nfrom src.core.config import settings\nfrom src.fsnb_matcher.services.matcher_service import build_match_xlsx\n\nrouter = APIRouter()\n\ndef _ensure_auth(request: Request) -> None:\n    # тот же флаг, что в header: access_token + user_email\n    if not (request.session.get(\"access_token\") and request.session.get(\"user_email\")):\n        raise HTTPException(status_code=401, detail=\"Auth required\")\n\n@router.post(\"/match\", name=\"fsnb_match_process\")\nasync def fsnb_match_process(\n    request: Request,\n    session: AsyncSession = Depends(db_helper.session_getter),\n    file: UploadFile = File(...),\n):\n    _ensure_auth(request)\n\n    raw = await file.read()\n    try:\n        payload = json.loads(raw.decode(\"utf-8\"))\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f\"Invalid JSON: {e}\")\n\n    try:\n        xlsx = await build_match_xlsx(payload, session)\n    except ValueError as e:\n        raise HTTPException(status_code=400, detail=str(e))\n\n    return Response(\n        content=xlsx,\n        media_type=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n        headers={\"Content-Disposition\": 'attachment; filename=\"smeta.xlsx\"'},\n    )\n"
    },
    {
      "path": "src/fsnb_matcher/embeddings/__init__.py",
      "language": "python",
      "size_bytes": 100,
      "sha256": "3185ea0231460ed133cae7056e1aacce8acbb294180c935f09bc9072555c7416",
      "content": "# file: src/fsnb_matcher/embeddings/__init__.py\nfrom . import model_giga as GIGA\n\n__all__ = [\"GIGA\"]"
    },
    {
      "path": "src/fsnb_matcher/embeddings/model_giga.py",
      "language": "python",
      "size_bytes": 3517,
      "sha256": "c2b3e82e3271d8be84cdaecc6f69987363a41ba15f6d9aa03ca7e3976998b66a",
      "content": "from __future__ import annotations\n\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom typing import List\nimport threading\n\nimport torch\nfrom sentence_transformers import SentenceTransformer\n\nfrom src.core.config import settings\n\nINSTRUCT_QUERY = \"Instruct: Given a database query, retrieve relevant FSNB entries\\nQuery: \"\n\n\ndef _fsnb_dir(path_str: str) -> Path:\n    # paths в конфиге у тебя строки — приводим к Path относительно /app\n    p = Path(path_str)\n    if p.is_absolute():\n        return p\n    return (Path(\"/app\") / p).resolve()\n\n\n@lru_cache()\ndef _gpu_sem() -> threading.Semaphore:\n    slots = int(getattr(settings.fsnb, \"gpu_slots\", 1) or 1)\n    return threading.Semaphore(max(1, slots))\n\n\ndef _device() -> str:\n    dev = getattr(settings.fsnb, \"hf_embed_device\", \"auto\")\n    if dev == \"auto\":\n        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    return dev\n\n\ndef _use_fp16() -> bool:\n    return bool(getattr(settings.fsnb, \"hf_embed_fp16\", True))\n\n\ndef _dtype():\n    if _use_fp16() and _device().startswith(\"cuda\"):\n        return torch.float16\n    return torch.float32\n\n\n@lru_cache()\ndef get() -> SentenceTransformer:\n    model_dir = _fsnb_dir(settings.fsnb.model_giga_dir)\n    model_path = str(model_dir)\n\n    # ВАЖНО: trust_remote_code нужен для Giga\n    model = SentenceTransformer(\n        model_path,\n        device=_device(),\n        trust_remote_code=True,\n        model_kwargs={\"torch_dtype\": _dtype()},\n    )\n    model.eval()\n    return model\n\n\ndef dim() -> int:\n    return int(get().get_sentence_embedding_dimension())\n\n\ndef _encode_impl(texts: List[str], batch_size: int) -> List[List[float]]:\n    dev = _device()\n    if dev.startswith(\"cuda\"):\n        with torch.inference_mode(), torch.amp.autocast(\"cuda\", dtype=_dtype()):\n            embs = get().encode(\n                texts,\n                batch_size=batch_size,\n                convert_to_numpy=True,\n                normalize_embeddings=False,\n                show_progress_bar=False,\n            )\n            torch.cuda.synchronize()\n    else:\n        with torch.inference_mode():\n            embs = get().encode(\n                texts,\n                batch_size=batch_size,\n                convert_to_numpy=True,\n                normalize_embeddings=False,\n                show_progress_bar=False,\n            )\n    return embs.tolist()\n\n\ndef encode(texts: List[str], *, is_query: bool, batch_size: int | None = None) -> List[List[float]]:\n    if batch_size is None:\n        if is_query:\n            batch_size = int(getattr(settings.fsnb, \"giga_query_bs\", 2))\n        else:\n            batch_size = int(getattr(settings.fsnb, \"giga_index_bs\", getattr(settings.fsnb, \"embed_batch_size\", 128)))\n\n    if is_query:\n        texts = [INSTRUCT_QUERY + (t or \"\") for t in texts]\n\n    sem = _gpu_sem()\n    sem.acquire()\n    try:\n        return _encode_impl(texts, batch_size=batch_size)\n    finally:\n        sem.release()\n\n\ndef unload() -> None:\n    try:\n        get.cache_clear()\n    except Exception:\n        pass\n    if torch.cuda.is_available() and _device().startswith(\"cuda\"):\n        torch.cuda.empty_cache()\n\ndef embed_texts(texts: list[str], *, is_query: bool = False, batch_size: int | None = None) -> list[list[float]]:\n    \"\"\"\n    Backward-compatible alias.\n    Старый код ожидает embed_texts(), а новый модуль использует encode().\n    \"\"\"\n    return encode(texts, is_query=is_query, batch_size=batch_size)\n"
    },
    {
      "path": "src/fsnb_matcher/schemas/__init__.py",
      "language": "python",
      "size_bytes": 0,
      "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
      "content": ""
    },
    {
      "path": "src/fsnb_matcher/schemas/item.py",
      "language": "python",
      "size_bytes": 211,
      "sha256": "0a9eb93929216e2b792e6203a6f83d0540adcfd655dabbcd0bde9230b3974b66",
      "content": "# src/fsnb_matcher/schemas/item.py\nfrom __future__ import annotations\nfrom pydantic import BaseModel\n\nclass ItemMeta(BaseModel):\n    id: int\n    code: str | None\n    name: str\n    unit: str | None\n    type: str\n"
    },
    {
      "path": "src/fsnb_matcher/services/__init__.py",
      "language": "python",
      "size_bytes": 0,
      "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
      "content": ""
    },
    {
      "path": "src/fsnb_matcher/services/fsnb_xml_parser.py",
      "language": "python",
      "size_bytes": 3928,
      "sha256": "e45be72104213c3571eb9a5b13ac58d0699ee1b16fce7f6a186d3948cbbdb649",
      "content": "from __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Iterator, Optional, Tuple\n\nfrom lxml import etree\n\nRowTuple = Tuple[str, str, Optional[str], str]  # (code, name, unit, type)\n\n\ndef iter_items_from_fsnb_xml(fsnb_dir: str | Path) -> Iterator[RowTuple]:\n    \"\"\"\n    Потоковый парсер FSNB-2022 XML: отдаёт плоскую витрину items:\n      - work:  Work(Code, EndName, MeasureUnit) + BeginName из NameGroup\n      - resource: ResourceCatalog -> Resource(Code, Name, MeasureUnit)\n\n    fsnb_dir: директория с *.xml\n    \"\"\"\n    fsnb_dir = Path(fsnb_dir)\n    if not fsnb_dir.exists():\n        raise FileNotFoundError(f\"FSNB dir not found: {fsnb_dir}\")\n\n    xml_files = sorted([p for p in fsnb_dir.iterdir() if p.is_file() and p.suffix.lower() == \".xml\"])\n    if not xml_files:\n        raise FileNotFoundError(f\"No .xml files in: {fsnb_dir}\")\n\n    for xml_path in xml_files:\n        # Бывает много служебных файлов — просто пропускаем те, что не похожи\n        # на base / ResourceCatalog.\n        try:\n            # Считываем только корневой тег\n            for _, root in etree.iterparse(str(xml_path), events=(\"start\",), recover=True, huge_tree=True):\n                root_tag = root.tag\n                break\n            else:\n                continue\n        except Exception:\n            continue\n\n        if root_tag == \"base\":\n            yield from _iter_items_from_base(xml_path)\n        elif root_tag == \"ResourceCatalog\":\n            yield from _iter_items_from_resource_catalog(xml_path)\n        else:\n            continue\n\n\ndef _iter_items_from_base(xml_path: Path) -> Iterator[RowTuple]:\n    \"\"\"\n    base -> ... NameGroup(BeginName) -> Work(Code, EndName, MeasureUnit)\n    \"\"\"\n    begin_name_stack: list[Optional[str]] = []\n    in_name_group = 0\n\n    context = etree.iterparse(\n        str(xml_path),\n        events=(\"start\", \"end\"),\n        recover=True,\n        huge_tree=True,\n    )\n\n    for event, el in context:\n        tag = el.tag\n\n        if event == \"start\" and tag == \"NameGroup\":\n            in_name_group += 1\n            begin = (el.get(\"BeginName\") or \"\").strip() or None\n            begin_name_stack.append(begin)\n\n        elif event == \"end\" and tag == \"Work\":\n            code = (el.get(\"Code\") or \"\").strip()\n            end_name = (el.get(\"EndName\") or \"\").strip()\n            unit = (el.get(\"MeasureUnit\") or \"\").strip() or None\n\n            if code and end_name:\n                begin = begin_name_stack[-1] if begin_name_stack else None\n                full_name = f\"{begin} {end_name}\".strip() if begin else end_name\n                yield (code, full_name, unit, \"work\")\n\n            # освобождаем память\n            el.clear()\n            while el.getprevious() is not None:\n                del el.getparent()[0]\n\n        elif event == \"end\" and tag == \"NameGroup\":\n            if in_name_group > 0:\n                in_name_group -= 1\n            if begin_name_stack:\n                begin_name_stack.pop()\n            el.clear()\n            while el.getprevious() is not None:\n                del el.getparent()[0]\n\n\ndef _iter_items_from_resource_catalog(xml_path: Path) -> Iterator[RowTuple]:\n    \"\"\"\n    ResourceCatalog -> ... Section -> Resource(Code, Name, MeasureUnit)\n    \"\"\"\n    context = etree.iterparse(\n        str(xml_path),\n        events=(\"end\",),\n        recover=True,\n        huge_tree=True,\n    )\n\n    for _, el in context:\n        if el.tag != \"Resource\":\n            continue\n\n        code = (el.get(\"Code\") or \"\").strip()\n        name = (el.get(\"Name\") or \"\").strip()\n        unit = (el.get(\"MeasureUnit\") or \"\").strip() or None\n\n        if code and name:\n            yield (code, name, unit, \"resource\")\n\n        el.clear()\n        while el.getprevious() is not None:\n            del el.getparent()[0]\n"
    },
    {
      "path": "src/fsnb_matcher/services/index_qdrant.py",
      "language": "python",
      "size_bytes": 2505,
      "sha256": "b63972f4784d11d01d475c90d25e395c5bdadddb5f4c44e48925a39c03448d1e",
      "content": "from __future__ import annotations\n\nfrom typing import List, Tuple\n\nfrom qdrant_client.models import Distance, VectorParams, PointStruct\n\nfrom src.app_logging import get_logger\nfrom src.core.config import settings\nfrom src.core.models.db_helper import db_helper\nfrom src.crud.item_repository import ItemRepository\nfrom src.fsnb_matcher.services.qdr import get_qdrant_client\nfrom src.fsnb_matcher.embeddings import model_giga  # <-- поправь импорт под свой путь\n\nlogger = get_logger(__name__)\n\nCOLLECTION_GIGA = \"fsnb_giga\"  # можешь вынести в settings позже\n\n\nasync def fetch_all_item_ids_and_names() -> List[Tuple[int, str]]:\n    async for session in db_helper.session_getter():\n        return await ItemRepository.fetch_all_item_ids_and_names(session)\n    return []\n\n\nasync def init_all_collections() -> int:\n    \"\"\"\n    Создаёт коллекцию в Qdrant и заливает туда эмбеддинги (Giga) для всех items из Postgres.\n    Возвращает количество залитых точек.\n    \"\"\"\n    items = await fetch_all_item_ids_and_names()\n    total = len(items)\n    logger.info({\"event\": \"pg_items_loaded\", \"count\": total})\n\n    if total == 0:\n        return 0\n\n    client = get_qdrant_client()\n\n    dim = int(model_giga.dim())\n    batch = int(settings.fsnb.giga_index_bs)  # микро-батч для Giga, напр. 8\n\n    # пересоздаём коллекцию\n    client.recreate_collection(\n        collection_name=COLLECTION_GIGA,\n        vectors_config=VectorParams(size=dim, distance=Distance.COSINE),\n    )\n    logger.info({\"event\": \"qdrant_collection_recreated\", \"collection\": COLLECTION_GIGA, \"dim\": dim})\n\n    done = 0\n    for i in range(0, total, batch):\n        chunk = items[i : i + batch]\n        ids = [item_id for (item_id, _) in chunk]\n        texts = [name for (_, name) in chunk]\n\n        vectors = model_giga.encode(texts, is_query=False, batch_size=len(texts))\n\n        points = [\n            PointStruct(id=int(pid), vector=vec, payload={\"name\": texts[idx]})\n            for idx, (pid, vec) in enumerate(zip(ids, vectors))\n        ]\n\n        client.upsert(collection_name=COLLECTION_GIGA, points=points, wait=False)\n        done += len(points)\n\n        if done % 1000 < batch:\n            logger.info({\"event\": \"qdrant_upsert_progress\", \"done\": done, \"total\": total})\n\n    logger.info({\"event\": \"qdrant_upsert_done\", \"collection\": COLLECTION_GIGA, \"count\": done})\n    return done\n"
    },
    {
      "path": "src/fsnb_matcher/services/ingest.py",
      "language": "python",
      "size_bytes": 691,
      "sha256": "e68827e53cddf35840c09f4e8291fa0a96fc9d7e2ac904cdb0e3418af8dfd0f7",
      "content": "# path: src/fsnb_matcher/services/ingest.py\nfrom __future__ import annotations\n\nfrom pathlib import Path\n\nfrom src.core.config import settings\nfrom src.core.models.db_helper import db_helper\nfrom src.crud.item_repository import ItemRepository\nfrom src.fsnb_matcher.services.fsnb_xml_parser import iter_items_from_fsnb_xml\n\n\nasync def ingest_to_postgres() -> int:\n    fsnb_dir = Path(getattr(settings, \"fsnb_dir\", \"/app/FSNB-2022_28_08_25\"))\n    inserted_total = 0\n\n    async for session in db_helper.session_getter():\n        rows = iter_items_from_fsnb_xml(fsnb_dir)\n        inserted_total = await ItemRepository.bulk_insert_items(session, rows, chunk_size=1000)\n\n    return inserted_total\n"
    },
    {
      "path": "src/fsnb_matcher/services/ingest_items.py",
      "language": "python",
      "size_bytes": 667,
      "sha256": "24fba285c4874f16fb3d97f9a8b73548677f61559d80ba03d7d9f201a711a1af",
      "content": "# path: src/fsnb_matcher/services/ingest_items.py\n\"\"\"\nCLI-утилита: парсит все XML-файлы ФСНБ и вставляет их в таблицу items.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom src.app_logging import get_logger\nfrom src.fsnb_matcher.services.ingest import ingest_to_postgres\n\nlogger = get_logger(__name__)\n\n\nasync def main() -> None:\n    logger.info(\"🚀 [ingest_items] Начинаем загрузку ФСНБ XML...\")\n    count = await ingest_to_postgres()\n    logger.info(f\"✅ [ingest_items] Завершено. Добавлено {count} строк.\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"
    },
    {
      "path": "src/fsnb_matcher/services/matcher_service.py",
      "language": "python",
      "size_bytes": 6554,
      "sha256": "ed5d5c8a3924ef46a1c7ea4bc9b0e99f2abae548f7ba582eda0c48ee748d3ae2",
      "content": "# path: src/fsnb_matcher/services/matcher_service.py\n\"\"\"\nСервис сопоставления элементов из JSON с базой ФСНБ.\n\nСовместимость:\n- Роутер сейчас импортирует build_match_xlsx(), поэтому эта функция обязана существовать.\n- Новая логика сопоставления реализована в match_items() и используется внутри build_match_xlsx().\n\nПроизводительность:\n- Embeddings и Qdrant client синхронные → выносим в asyncio.to_thread, чтобы не блокировать event loop.\n- Работа с Postgres через AsyncSession (db_helper.session_factory()).\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport io\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom openpyxl import Workbook\n\nfrom src.app_logging import get_logger\nfrom src.core.config import settings\nfrom src.core.models.db_helper import db_helper\nfrom src.crud.item_repository import ItemRepository\nfrom src.fsnb_matcher.embeddings.model_giga import embed_texts\nfrom src.fsnb_matcher.services.qdr import get_qdrant_client\n\nlogger = get_logger(__name__)\n\n\ndef _safe_int(value: Any) -> Optional[int]:\n    \"\"\"Пытается привести value к int, иначе возвращает None.\"\"\"\n    try:\n        return int(value)\n    except (TypeError, ValueError):\n        return None\n\n\nasync def _embed_captions(captions: List[str]) -> List[List[float]]:\n    \"\"\"Эмбеддинги в отдельном потоке (не блокируем event loop).\"\"\"\n    return await asyncio.to_thread(embed_texts, captions)\n\n\nasync def _qdrant_search(\n    *,\n    collection_name: str,\n    vectors: List[List[float]],\n    top_k: int,\n) -> List[List[Any]]:\n    \"\"\"\n    Поиск в Qdrant для каждого вектора.\n    Возвращает список результатов на каждый caption.\n    \"\"\"\n    client = get_qdrant_client()\n\n    def _do_search_one(vec: List[float]) -> List[Any]:\n        return client.search(\n            collection_name=collection_name,\n            query_vector=vec,\n            limit=top_k,\n        )\n\n    tasks = [asyncio.to_thread(_do_search_one, vec) for vec in vectors]\n    return await asyncio.gather(*tasks)\n\n\nasync def _fetch_meta_by_item_id(\n    item_id: int,\n) -> Optional[Tuple[str, Optional[str], Optional[str]]]:\n    \"\"\"Достаёт (name, unit, code) из Postgres по item_id.\"\"\"\n    async with db_helper.session_factory() as session:\n        return await ItemRepository.fetch_item_name_unit_code_by_id(session, item_id)\n\n\nasync def match_items(\n    json_items: List[Dict[str, Any]],\n    top_k: int = 3,\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Сопоставляет элементы JSON с коллекцией Qdrant.\n\n    Вход:\n        [{\"Caption\": \"...\", \"Units\": \"...\", \"Quantity\": \"...\", ...}, ...]\n    Выход:\n        добавляет поля:\n        - \"FSNB Name\"\n        - \"FSNB code\"\n        - \"FSNB Units\"\n        - \"conf\"\n    \"\"\"\n    if not json_items:\n        return []\n\n    collection_name = getattr(getattr(settings, \"fsnb\", None), \"qdrant_collection\", None)\n    if not isinstance(collection_name, str) or not collection_name.strip():\n        collection_name = \"fsnb_giga\"\n\n    captions = [str(i.get(\"Caption\", \"\") or \"\") for i in json_items]\n\n    logger.info(\n        \"Starting match\",\n        extra={\"items\": len(json_items), \"top_k\": top_k, \"collection\": collection_name},\n    )\n\n    vectors = await _embed_captions(captions)\n    searches = await _qdrant_search(collection_name=collection_name, vectors=vectors, top_k=top_k)\n\n    results: List[Dict[str, Any]] = []\n\n    for idx, found in enumerate(searches):\n        if not found:\n            results.append(\n                {\n                    **json_items[idx],\n                    \"FSNB Name\": None,\n                    \"FSNB code\": None,\n                    \"FSNB Units\": None,\n                    \"conf\": 0.0,\n                }\n            )\n            continue\n\n        best = found[0]\n        score = float(getattr(best, \"score\", 0.0))\n        raw_id = getattr(best, \"id\", None)\n        item_id = _safe_int(raw_id)\n\n        if item_id is None:\n            results.append(\n                {\n                    **json_items[idx],\n                    \"FSNB Name\": None,\n                    \"FSNB code\": None,\n                    \"FSNB Units\": None,\n                    \"conf\": score,\n                }\n            )\n            continue\n\n        meta = await _fetch_meta_by_item_id(item_id)\n        if not meta:\n            results.append(\n                {\n                    **json_items[idx],\n                    \"FSNB Name\": None,\n                    \"FSNB code\": None,\n                    \"FSNB Units\": None,\n                    \"conf\": score,\n                }\n            )\n            continue\n\n        name, unit, code = meta\n        results.append(\n            {\n                **json_items[idx],\n                \"FSNB Name\": name,\n                \"FSNB code\": code,\n                \"FSNB Units\": unit,\n                \"conf\": score,\n            }\n        )\n\n    logger.info(\"Match completed\", extra={\"rows\": len(results)})\n    return results\n\n\nasync def build_match_xlsx(\n    payload: Dict[str, Any],\n    *,\n    top_k: int = 3,\n) -> bytes:\n    \"\"\"\n    Совместимый API для старого роутера:\n    - принимает payload вида {\"items\":[{...}, {...}]}\n    - строит xlsx и возвращает bytes.\n\n    Это позволяет не менять src/fsnb_matcher/api/api_v1/match.py прямо сейчас.\n    \"\"\"\n    items = payload.get(\"items\", [])\n    if not isinstance(items, list):\n        items = []\n\n    matched = await match_items(items, top_k=top_k)\n\n    wb = Workbook()\n    ws = wb.active\n    ws.title = \"GIGA\"\n\n    # Заголовки (как ты описывал)\n    headers = [\n        \"Caption\",\n        \"FSNB Name\",\n        \"FSNB code\",\n        \"Units\",\n        \"FSNB Units\",\n        \"Quantity\",\n        \"conf\",\n    ]\n    ws.append(headers)\n\n    for row in matched:\n        ws.append(\n            [\n                row.get(\"Caption\", \"\"),\n                row.get(\"FSNB Name\"),\n                row.get(\"FSNB code\"),\n                row.get(\"Units\"),\n                row.get(\"FSNB Units\"),\n                row.get(\"Quantity\"),\n                row.get(\"conf\", 0.0),\n            ]\n        )\n\n    buf = io.BytesIO()\n    wb.save(buf)\n    return buf.getvalue()\n"
    },
    {
      "path": "src/fsnb_matcher/services/parser.py",
      "language": "python",
      "size_bytes": 2107,
      "sha256": "f9d4476843e712479c662549d603d42fcd47fae7c873674224bf9577ec0a3862",
      "content": "# src/fsnb_matcher/services/parser.py\nfrom __future__ import annotations\nfrom pathlib import Path\nfrom typing import Iterable\nfrom lxml import etree\n\ndef _list_xml(fsnb_dir: Path) -> list[Path]:\n    files: list[Path] = []\n    for p in fsnb_dir.iterdir():\n        if p.is_file() and p.suffix.lower() == \".xml\":\n            files.append(p)\n    return sorted(files)\n\ndef iter_items(fsnb_dir: Path) -> Iterable[tuple[str, str, str | None, str]]:\n    files = _list_xml(fsnb_dir)\n    print(f\"[INFO] FSNB dir: {fsnb_dir}\")\n    print(f\"[INFO] XML files found: {len(files)}\")\n    for p in files:\n        print(f\"  - {p.name}\")\n\n    for xml_path in files:\n        name_lc = xml_path.name.lower()\n        try:\n            root = etree.parse(str(xml_path)).getroot()\n        except Exception as e:\n            print(f\"[WARN] parse failed: {xml_path.name}: {e}\")\n            continue\n\n        if \"гэсн\" in name_lc:\n            count = 0\n            for el in root.xpath(\".//NameGroup\"):\n                begin = (el.get(\"BeginName\") or \"\").strip()\n                for w in el.xpath(\"./Work\"):\n                    code = (w.get(\"Code\") or \"\").strip()\n                    end = (w.get(\"EndName\") or \"\").strip()\n                    unit = (w.get(\"MeasureUnit\") or \"\").strip() or None\n                    if not code or not end:\n                        continue\n                    full = f\"{begin} {end}\".strip() if begin else end\n                    count += 1\n                    yield (code, full, unit, \"work\")\n            print(f\"[INFO] Parsed {count} works from {xml_path.name}\")\n\n        if \"фсбц\" in name_lc:\n            count = 0\n            for r in root.xpath(\".//Resource[@Code]\"):\n                code = (r.get(\"Code\") or \"\").strip()\n                title = (r.get(\"Name\") or r.get(\"EndName\") or \"\").strip()\n                unit = (r.get(\"MeasureUnit\") or \"\").strip() or None\n                if not code or not title:\n                    continue\n                count += 1\n                yield (code, title, unit, \"resource\")\n            print(f\"[INFO] Parsed {count} resources from {xml_path.name}\")\n"
    },
    {
      "path": "src/fsnb_matcher/services/qdr.py",
      "language": "python",
      "size_bytes": 880,
      "sha256": "72d9d115baebd83614c2790756d3c0f7471ebbbe2ca6809ced0808de0ec3460b",
      "content": "# path: src/fsnb_matcher/services/qdr.py\n\"\"\"\nУтилиты для подключения и работы с Qdrant.\nИспользуется индексатором и matcher_service.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom qdrant_client import QdrantClient\nfrom src.app_logging import get_logger\nfrom src.core.config import settings\n\nlogger = get_logger(__name__)\n\n\ndef get_qdrant_client() -> QdrantClient:\n    \"\"\"\n    Возвращает экземпляр клиента Qdrant, используя настройки из .env.\n    \"\"\"\n    logger.info(f\"🔗 Подключение к Qdrant: {settings.qdrant.host}:{settings.qdrant.port}\")\n    client = QdrantClient(\n        host=settings.qdrant.host,\n        port=settings.qdrant.port,\n        prefer_grpc=False,\n        timeout=settings.qdrant.timeout_s,\n        check_compatibility=False,\n    )\n    return client\n"
    },
    {
      "path": "src/fsnb_matcher/utils/__init__.py",
      "language": "python",
      "size_bytes": 0,
      "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
      "content": ""
    },
    {
      "path": "src/fsnb_matcher/views/__init__.py",
      "language": "python",
      "size_bytes": 0,
      "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
      "content": ""
    },
    {
      "path": "src/main.py",
      "language": "python",
      "size_bytes": 1746,
      "sha256": "b978afde8971f24feb642027b8ad25345ec641fd56870bf54f79548e42e802d8",
      "content": "# /src/main.py\nfrom __future__ import annotations\n\nfrom contextlib import asynccontextmanager\nfrom pathlib import Path\n\nimport uvicorn\nfrom fastapi import FastAPI\nfrom fastapi.responses import ORJSONResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom starlette.middleware.sessions import SessionMiddleware\n\n# ВАЖНО: относительные импорты внутри пакета base_app\nfrom .core.config import settings\nfrom .core.models import db_helper\nfrom src.core.api import router as api_router\nfrom src.core.views import router as views_router  # HTML-вьюхи (/, /users/)\n\nPROJECT_ROOT = Path(__file__).resolve().parents[1]\nSTATIC_DIR = PROJECT_ROOT / \"static\"\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # startup\n    yield\n    # shutdown\n    await db_helper.dispose()\n\n\ndef create_app() -> FastAPI:\n    app = FastAPI(\n        default_response_class=ORJSONResponse,\n        lifespan=lifespan,\n    )\n    app.add_middleware(\n        SessionMiddleware,\n        secret_key=settings.auth.secret_key,\n        session_cookie=\"fsnb_session\",\n        same_site=\"lax\",\n    )  # <— для session/CSRF\n\n    # /static -> ./static (в корне проекта)\n    app.mount(\"/static\", StaticFiles(directory=str(STATIC_DIR)), name=\"static\")\n\n    # HTML-views и API\n    app.include_router(views_router)\n    app.include_router(api_router, prefix=settings.api.prefix)\n    return app\n\n\n# Экспортируемый объект приложения\nmain_app = create_app()\n\n\nif __name__ == \"__main__\":\n    # Запуск: poetry run uvicorn base_app.main:main_app --reload\n    uvicorn.run(\n        \"src.main:main_app\",\n        host=settings.run.host,\n        port=settings.run.port,\n        reload=True,\n    )\n"
    },
    {
      "path": "src/manage.py",
      "language": "python",
      "size_bytes": 1326,
      "sha256": "855c4b5d8d25b2e30f8e5c66167c6cf5d8f75fd3874ef1d131efae199fc68fc1",
      "content": "# /src/manage.py\nfrom __future__ import annotations\n\nimport argparse\nimport asyncio\nfrom getpass import getpass\n\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom src.app_logging import get_logger\nfrom src.core.models import db_helper\nfrom src.scripts.superuser import create_superuser\n\nlog = get_logger(\"manage\")\n\n\nasync def _cmd_create_superuser() -> None:\n    print(\"Create superuser\")\n    username = input(\"Username: \").strip()\n    password = getpass(\"Password: \")\n    email = input(\"E-mail (optional): \").strip()\n\n    async with db_helper.session_factory() as session:  # type: AsyncSession\n        uid = await create_superuser(session, username=username, password=password, email=email or None)\n        await session.commit()\n        log.info({\"event\": \"create_superuser_ok\", \"user_id\": uid, \"username\": username})\n        print(f\"✔ Superuser created: id={uid}, username={username}\")\n\n\ndef main(argv: list[str] | None = None) -> None:\n    parser = argparse.ArgumentParser(prog=\"base_app.manage\", description=\"Management commands\")\n    parser.add_argument(\"--create_superuser\", action=\"store_true\", help=\"Create a superuser\")\n    args = parser.parse_args(argv)\n\n    if args.create_superuser:\n        asyncio.run(_cmd_create_superuser())\n    else:\n        parser.print_help()\n\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
      "path": "src/scripts/create_fsnb_pg.py",
      "language": "python",
      "size_bytes": 825,
      "sha256": "b9be68223dcb8e5fca4c0f0b208541aad881d23e0e9e68013014222303352760",
      "content": "# path: src/scripts/create_fsnb_pg.py\n\"\"\"\nИмпорт XML ФСНБ в PostgreSQL.\nИспользуется при первом запуске:\n  docker compose exec app bash -lc \"python -m src.scripts.create_fsnb_pg\"\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom src.app_logging import get_logger\nfrom src.fsnb_matcher.services.ingest import ingest_to_postgres\n\nlogger = get_logger(__name__)\n\n\nasync def main() -> None:\n    \"\"\"Асинхронная точка входа — импортирует ФСНБ в базу.\"\"\"\n    logger.info(\"⏳ [create_fsnb_pg] Начало импорта XML → Postgres ...\")\n    inserted = await ingest_to_postgres()\n    logger.info(f\"✅ Импорт завершён: добавлено {inserted} записей.\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"
    },
    {
      "path": "src/scripts/init_vector_db.py",
      "language": "python",
      "size_bytes": 461,
      "sha256": "1e3386331948fe5258e90eceea34ab48f645d349664da67e021fe68f1d202ad1",
      "content": "import asyncio\nfrom src.fsnb_matcher.services.index_qdrant import init_all_collections\nfrom src.app_logging import get_logger\n\nlog = get_logger(__name__)\n\nasync def main():\n    log.info(\"⏳ [init_vector_db] Индексация Qdrant начата...\")\n    count = await init_all_collections()\n    log.info(f\"✅ [init_vector_db] Индексация завершена. Всего записей: {count}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"
    },
    {
      "path": "src/scripts/superuser.py",
      "language": "python",
      "size_bytes": 2434,
      "sha256": "f37b093c467370ecc7fbac768f275f05392123b175831a5289cbafb82803df57",
      "content": "# /src/scripts/superuser.py\nfrom __future__ import annotations\n\nfrom typing import Optional\n\nfrom sqlalchemy import select\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom src.app_logging import get_logger\nfrom src.core.models.user import User\nfrom src.core.models.profile import Profile\nfrom src.core.models.permission import Permission\nfrom src.core.security import hash_password\n\nlog = get_logger(\"scripts.superuser\")\n\n\nasync def create_superuser(\n    session: AsyncSession,\n    *,\n    username: str,\n    password: str,\n    email: Optional[str] = None,\n) -> int:\n    \"\"\"\n    Создаёт суперпользователя:\n    - User.username = username, User.email = email or f\"{username}@localhost\"\n    - пароль хэшируется\n    - e-mail считается подтверждённым (activation_key=None)\n    - создаётся Profile (verification=True, email заполняем как у user.email)\n    - создаётся Permission со всеми True\n    Возвращает user.id\n    \"\"\"\n    if not username:\n        raise ValueError(\"username is required\")\n\n    # e-mail обязателен в модели, поэтому подставим дефолт\n    email_final = (email or f\"{username}@localhost\").strip().lower()\n    password_hash = hash_password(password or \"\")\n\n    # Проверка уникальности username / email\n    existing_u = await session.execute(select(User).where((User.username == username) | (User.email == email_final)))\n    if existing_u.scalar_one_or_none():\n        raise ValueError(\"User with same username or email already exists\")\n\n    user = User(\n        username=username,\n        email=email_final,\n        hashed_password=password_hash,\n        is_active=True,\n        activation_key=None,  # нет ключа = подтверждён\n    )\n    session.add(user)\n    await session.flush()  # получим user.id\n\n    profile = Profile(\n        user_id=user.id,\n        verification=True,\n        email=email_final,\n    )\n    session.add(profile)\n    await session.flush()  # получим profile.id\n\n    perm = Permission(\n        profile_id=profile.id,\n        is_superadmin=True,\n        is_admin=True,\n        is_staff=True,\n        is_updater=True,\n        is_reader=True,\n        is_user=True,\n    )\n    session.add(perm)\n\n    log.info({\"event\": \"create_superuser\", \"user_id\": user.id, \"username\": username})\n    return user.id\n"
    },
    {
      "path": "src/templates/admin/index.html",
      "language": "html",
      "size_bytes": 390,
      "sha256": "020f516c5301ab7c7b023385c8e7fa0d9d11cbe4de294f20a0ce5912f45bc139",
      "content": "{% extends \"core/base.html\" %}\n{% block content %}\n<h1>Админка</h1>\n\n<div class=\"card\">\n  <h3>Зарегистрированные модели</h3>\n  <ul>\n    {% for m in models %}\n      <li>\n        <a href=\"{{ request.url_for('admin_model_list', slug=m.slug) }}\">\n          {{ m.model_name }} ({{ m.slug }})\n        </a>\n      </li>\n    {% endfor %}\n  </ul>\n</div>\n{% endblock %}\n"
    },
    {
      "path": "src/templates/admin/login.html",
      "language": "html",
      "size_bytes": 776,
      "sha256": "4966f70e2940e31ef2f07165b0415e1ec8bf42e3faf9954f8f100a351603320a",
      "content": "<!-- /templates/admin/login.html -->\n{% extends \"core/base.html\" %}\n{% block title %}Admin — Login{% endblock %}\n\n{% block content %}\n  <h2>Admin Login</h2>\n  {% if alert %}\n    <div class=\"alert {{ 'alert-success' if alert.kind == 'success' else 'alert-error' }}\">{{ alert.text }}</div>\n  {% endif %}\n\n  <form method=\"post\" action=\"{{ request.url_for('admin_login_post') }}\">\n    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf }}\">\n    <div class=\"form-row\">\n      <label>Username</label>\n      <input name=\"username\" type=\"text\" required>\n    </div>\n    <div class=\"form-row\">\n      <label>Password</label>\n      <input name=\"password\" type=\"password\" required>\n    </div>\n    <button class=\"btn btn-primary\" type=\"submit\">Sign in</button>\n  </form>\n{% endblock %}\n"
    },
    {
      "path": "src/templates/admin/model_edit.html",
      "language": "html",
      "size_bytes": 1503,
      "sha256": "ee18c52b2d6440f354c9dc22cc00bc0275b320cfb55bb007ed5ee8d28e4590e9",
      "content": "{% extends \"core/base.html\" %}\n{% block content %}\n<h1>Редактирование: {{ model_name }} #{{ obj.id }}</h1>\n\n<form method=\"post\" action=\"{{ request.url_for('admin_model_edit_post', slug=slug, obj_id=obj.id) }}\" class=\"form\" style=\"max-width:720px;\">\n  <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf }}\"/>\n\n  {% for name in fields %}\n    {% set label = labels.get(name, name) %}\n    {% set val = attr(obj, name) %}\n    <div class=\"form-row\">\n      <label>{{ label }}</label>\n\n      {% if name in readonly_fields %}\n        <div class=\"readonly\">{{ val }}</div>\n      {% else %}\n        {% if name.startswith(\"is_\") and (val is boolean or val in [True, False, 0, 1, '0', '1']) %}\n          {% set checked = (val == True) %}\n          <input type=\"checkbox\" name=\"{{ name }}\" {% if checked %}checked{% endif %}\n                 {% if name == 'is_superadmin' and not can_edit_super_flag %}disabled{% endif %}>\n          {% if name == 'is_superadmin' and not can_edit_super_flag %}\n            <small class=\"hint\">Изменять может только суперпользователь</small>\n          {% endif %}\n        {% else %}\n          <input type=\"text\" name=\"{{ name }}\" value=\"{{ val if val is not none else '' }}\">\n        {% endif %}\n      {% endif %}\n    </div>\n  {% endfor %}\n\n  <button type=\"submit\" class=\"btn btn-primary\">Сохранить</button>\n  <a href=\"{{ request.url_for('admin_model_list', slug=slug) }}\" class=\"btn\">Назад</a>\n</form>\n{% endblock %}\n"
    },
    {
      "path": "src/templates/admin/model_list.html",
      "language": "html",
      "size_bytes": 1128,
      "sha256": "74baeea178c2b00c70e5fb9b3676bfec173d19927cbdc22824d6d94cd8bccb3e",
      "content": "{% extends \"core/base.html\" %}\n{% block content %}\n<h1>Модель: {{ model_name }}</h1>\n\n<form method=\"get\" action=\"{{ request.url_for('admin_model_list', slug=slug) }}\" class=\"form-row\" style=\"max-width:480px; gap:8px;\">\n  <input type=\"text\" name=\"q\" value=\"{{ q }}\" placeholder=\"Поиск...\" />\n  <button class=\"btn btn-primary\" type=\"submit\">Найти</button>\n</form>\n\n<div class=\"table-wrap\" style=\"overflow:auto; margin-top:12px;\">\n  <table class=\"table\">\n    <thead>\n      <tr>\n        {% for f in list_display %}\n          <th>{{ f }}</th>\n        {% endfor %}\n        <th></th>\n      </tr>\n    </thead>\n    <tbody>\n      {% for row in rows %}\n        <tr>\n          {% for f in list_display %}\n            <td>{{ attr(row, f) }}</td>\n          {% endfor %}\n          <td>\n            <a class=\"btn btn-xs\" href=\"{{ request.url_for('admin_model_edit', slug=slug, obj_id=attr(row, 'id')) }}\">Edit</a>\n          </td>\n        </tr>\n      {% endfor %}\n      {% if not rows %}\n        <tr><td colspan=\"{{ list_display|length + 1 }}\">Пусто</td></tr>\n      {% endif %}\n    </tbody>\n  </table>\n</div>\n{% endblock %}\n"
    },
    {
      "path": "src/templates/admin/perm_edit.html",
      "language": "html",
      "size_bytes": 2011,
      "sha256": "26208056ea0fe0dd7133c013043fcf146fce927cd7235ebcc82d1f1b9f523f0b",
      "content": "<!-- /templates/admin/perm_edit.html -->\n{% extends \"core/base.html\" %}\n{% block title %}Admin — Edit Permissions{% endblock %}\n\n{% block content %}\n  <h2>Permissions (profile_id={{ perm.profile_id }})</h2>\n\n  {% if forbidden_reason %}\n    <div class=\"alert alert-error\">{{ forbidden_reason }}</div>\n  {% endif %}\n\n  <form method=\"post\" action=\"{{ request.url_for('admin_perm_edit_post', profile_id=perm.profile_id) }}\">\n    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf }}\">\n\n    {% set ro = 'disabled' if read_only else '' %}\n    {% set dis_super = '' if can_edit_super_flag and not read_only else 'disabled' %}\n\n    <div class=\"form-row\">\n      <label>is_superadmin</label>\n      <input name=\"is_superadmin\" type=\"checkbox\" {% if perm.is_superadmin %}checked{% endif %} {{ dis_super }}>\n      {% if not can_edit_super_flag %}\n        <small class=\"hint\">Только суперпользователь может менять этот флаг</small>\n      {% endif %}\n    </div>\n\n    <div class=\"form-row\"><label>is_admin</label>\n      <input name=\"is_admin\" type=\"checkbox\" {% if perm.is_admin %}checked{% endif %} {{ ro }}>\n    </div>\n    <div class=\"form-row\"><label>is_staff</label>\n      <input name=\"is_staff\" type=\"checkbox\" {% if perm.is_staff %}checked{% endif %} {{ ro }}>\n    </div>\n    <div class=\"form-row\"><label>is_updater</label>\n      <input name=\"is_updater\" type=\"checkbox\" {% if perm.is_updater %}checked{% endif %} {{ ro }}>\n    </div>\n    <div class=\"form-row\"><label>is_reader</label>\n      <input name=\"is_reader\" type=\"checkbox\" {% if perm.is_reader %}checked{% endif %} {{ ro }}>\n    </div>\n    <div class=\"form-row\"><label>is_user</label>\n      <input name=\"is_user\" type=\"checkbox\" {% if perm.is_user %}checked{% endif %} {{ ro }}>\n    </div>\n\n    {% if not read_only %}\n      <button class=\"btn btn-primary\" type=\"submit\">Save</button>\n    {% else %}\n      <button class=\"btn btn-primary\" type=\"submit\" disabled>Save</button>\n    {% endif %}\n  </form>\n{% endblock %}\n"
    },
    {
      "path": "src/templates/admin/profile_edit.html",
      "language": "html",
      "size_bytes": 1215,
      "sha256": "7a0c6dc162ca741ccaa426e773aaab3bc84bb639b6ab1a8d8a6d9368766915be",
      "content": "<!-- /templates/admin/profile_edit.html -->\n{% extends \"core/base.html\" %}\n{% block title %}Admin — Edit Profile{% endblock %}\n\n{% block content %}\n  <h2>Profile #{{ profile.id }} (user_id={{ profile.user_id }})</h2>\n  <form method=\"post\" action=\"{{ request.url_for('admin_profile_edit_post', profile_id=profile.id) }}\">\n    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf }}\">\n\n    <div class=\"form-row\"><label>Nickname</label><input name=\"nickname\" value=\"{{ profile.nickname or '' }}\"></div>\n    <div class=\"form-row\"><label>First name</label><input name=\"first_name\" value=\"{{ profile.first_name or '' }}\"></div>\n    <div class=\"form-row\"><label>Second name</label><input name=\"second_name\" value=\"{{ profile.second_name or '' }}\"></div>\n    <div class=\"form-row\"><label>Phone</label><input name=\"phone\" value=\"{{ profile.phone or '' }}\"></div>\n    <div class=\"form-row\"><label>E-mail</label><input name=\"email\" type=\"email\" value=\"{{ profile.email or '' }}\"></div>\n    <div class=\"form-row\"><label>Verified</label><input name=\"verification\" type=\"checkbox\" {% if profile.verification %}checked{% endif %}></div>\n\n    <button class=\"btn btn-primary\" type=\"submit\">Save</button>\n  </form>\n{% endblock %}\n"
    },
    {
      "path": "src/templates/admin/user_edit.html",
      "language": "html",
      "size_bytes": 3715,
      "sha256": "4f430c58f407aee0893c887d5c16280cade6dd15616ffe0b1d9f4bcb2267af8b",
      "content": "{% extends \"core/base.html\" %}\n{% block title %}Admin — Edit User{% endblock %}\n\n{% block content %}\n  <h2>Редактирование пользователя #{{ user.id }}</h2>\n\n  <form method=\"post\" action=\"{{ request.url_for('admin_user_edit_post', user_id=user.id) }}\" class=\"card\">\n    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf }}\">\n\n    <h3>Учётная запись</h3>\n    <div class=\"form-row\">\n      <label>Username</label>\n      <input type=\"text\" name=\"username\" value=\"{{ user.username or '' }}\">\n    </div>\n    <div class=\"form-row\">\n      <label>Активен</label>\n      <input type=\"checkbox\" name=\"is_active\" {% if user.is_active %}checked{% endif %}>\n    </div>\n\n    <h3>Профиль</h3>\n    <div class=\"form-row\">\n      <label>Nickname</label>\n      <input type=\"text\" name=\"nickname\" value=\"{{ profile.nickname or '' }}\">\n    </div>\n    <div class=\"form-row\">\n      <label>Имя</label>\n      <input type=\"text\" name=\"first_name\" value=\"{{ profile.first_name or '' }}\">\n    </div>\n    <div class=\"form-row\">\n      <label>Фамилия</label>\n      <input type=\"text\" name=\"second_name\" value=\"{{ profile.second_name or '' }}\">\n    </div>\n    <div class=\"form-row\">\n      <label>Телефон</label>\n      <input type=\"text\" name=\"phone\" value=\"{{ profile.phone or '' }}\">\n    </div>\n    <div class=\"form-row\">\n      <label>E-mail (в профиле)</label>\n      <input type=\"text\" name=\"email\" value=\"{{ profile.email or '' }}\">\n    </div>\n    <div class=\"form-row\">\n      <label>Telegram ID</label>\n      <input type=\"text\" name=\"tg_id\" value=\"{{ profile.tg_id or '' }}\">\n    </div>\n    <div class=\"form-row\">\n      <label>Telegram nickname</label>\n      <input type=\"text\" name=\"tg_nickname\" value=\"{{ profile.tg_nickname or '' }}\">\n    </div>\n    <div class=\"form-row\">\n      <label>Подтверждён</label>\n      <input type=\"checkbox\" name=\"verification\" {% if profile.verification %}checked{% endif %}>\n    </div>\n\n    <h3>Права</h3>\n    {% if perm_note %}\n      <div class=\"alert alert-error\">{{ perm_note }}</div>\n    {% endif %}\n    <div class=\"grid-2\">\n      <label>\n        <input type=\"checkbox\" name=\"is_superadmin\"\n               {% if perm.is_superadmin %}checked{% endif %}\n               {% if not can_edit_super_flag or not perm_editable %}disabled{% endif %}>\n        is_superadmin\n      </label>\n\n      <label>\n        <input type=\"checkbox\" name=\"is_admin_flag\"\n               {% if perm.is_admin %}checked{% endif %}\n               {% if not perm_editable %}disabled{% endif %}>\n        is_admin\n      </label>\n\n      <label>\n        <input type=\"checkbox\" name=\"is_staff\"\n               {% if perm.is_staff %}checked{% endif %}\n               {% if not perm_editable %}disabled{% endif %}>\n        is_staff\n      </label>\n\n      <label>\n        <input type=\"checkbox\" name=\"is_updater\"\n               {% if perm.is_updater %}checked{% endif %}\n               {% if not perm_editable %}disabled{% endif %}>\n        is_updater\n      </label>\n\n      <label>\n        <input type=\"checkbox\" name=\"is_reader\"\n               {% if perm.is_reader %}checked{% endif %}\n               {% if not perm_editable %}disabled{% endif %}>\n        is_reader\n      </label>\n\n      <label>\n        <input type=\"checkbox\" name=\"is_user_flag\"\n               {% if perm.is_user %}checked{% endif %}\n               {% if not perm_editable %}disabled{% endif %}>\n        is_user\n      </label>\n    </div>\n\n    <div style=\"margin-top:16px\">\n      <button type=\"submit\" class=\"btn btn-primary\">Сохранить</button>\n      <a class=\"btn\" href=\"{{ request.url_for('admin_users') }}\">Назад к списку</a>\n    </div>\n  </form>\n{% endblock %}\n"
    },
    {
      "path": "src/templates/admin/users.html",
      "language": "html",
      "size_bytes": 787,
      "sha256": "70a68867ab685ed12b1d9098db92031010cb1747b41502a9a97e23a5b09239f6",
      "content": "{% extends \"core/base.html\" %}\n{% block title %}Admin — Users{% endblock %}\n{% block content %}\n  <h2>Users</h2>\n  <table class=\"table\">\n    <thead>\n      <tr>\n        <th>ID</th><th>E-mail</th><th>Username</th><th>Active</th><th>Verified</th><th></th>\n      </tr>\n    </thead>\n    <tbody>\n      {% for it in items %}\n      {% set u = it.user %}\n      <tr>\n        <td>{{ u.id }}</td>\n        <td>{{ u.email }}</td>\n        <td>{{ u.username or \"—\" }}</td>\n        <td>{{ \"✓\" if u.is_active else \"—\" }}</td>\n        <td>{{ \"✓\" if it.verified else \"—\" }}</td>\n        <td>\n          <a class=\"btn btn-primary btn-xs\" href=\"{{ request.url_for('admin_user_edit_get', user_id=u.id) }}\">Edit</a>\n        </td>\n      </tr>\n      {% endfor %}\n    </tbody>\n  </table>\n{% endblock %}\n"
    },
    {
      "path": "src/templates/core/_header.html",
      "language": "html",
      "size_bytes": 1697,
      "sha256": "e7bac9953045e0fcd965969488310647ec68ee9ee39713922a18d3e948dcc3d8",
      "content": "<!-- templates/core/_header.html -->\n<header class=\"header\">\n  <div class=\"container header-inner\">\n    <a href=\"{{ request.url_for('home') }}\" class=\"brand\">fsnb-fastapi</a>\n\n    <nav class=\"nav\">\n      {% set is_auth = request.session.get('access_token') and request.session.get('user_email') %}\n      {% set admin_uid = request.session.get('admin_user_id') %}\n      <ul class=\"nav-list\">\n        {# 1) Если вошли в админку — показываем элементы админа #}\n        {% if admin_uid %}\n          <li class=\"nav-item\">\n            <a href=\"{{ request.url_for('admin_index') }}\">Админка</a>\n          </li>\n          <li class=\"nav-item\">\n            <form method=\"post\" action=\"{{ request.url_for('admin_logout') }}\">\n              <button type=\"submit\" class=\"nav-logout\">Выйти (admin)</button>\n            </form>\n          </li>\n\n        {# 2) Иначе — обычная пользовательская навигация #}\n        {% elif is_auth %}\n          <li class=\"nav-item\">\n            <a href=\"{{ request.url_for('profile_html') }}\">Профиль</a>\n          </li>\n          <li class=\"nav-item\">\n            <form method=\"post\" action=\"{{ request.url_for('logout_html') }}\">\n              <button type=\"submit\" class=\"nav-logout\">Выйти</button>\n            </form>\n          </li>\n        {% else %}\n          <li class=\"nav-item\">\n            <a href=\"{{ request.url_for('login_html') }}\">Вход</a>\n          </li>\n          <li class=\"nav-item\">\n            <a href=\"{{ request.url_for('register_html') }}\">Регистрация</a>\n          </li>\n        {% endif %}\n      </ul>\n    </nav>\n  </div>\n</header>\n"
    },
    {
      "path": "src/templates/core/base.html",
      "language": "html",
      "size_bytes": 780,
      "sha256": "6290bd6397d50e4839740e3986745701133f5fc0b4654018f90309b4d17d43e7",
      "content": "<!-- /templates/core/base.html -->\n<!doctype html>\n<html lang=\"ru\">\n  <head>\n    <meta charset=\"utf-8\" />\n    <title>{% block title %}fsnb-fastapi{% endblock %}</title>\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n    <link rel=\"stylesheet\" href=\"{{ request.url_for('static', path='css/style.css') }}\" />\n    {% block head_extra %}\n      <script src=\"{{ request.url_for('static', path='js/app.js') }}\" defer></script>\n    {% endblock %}\n  </head>\n  <body>\n    {% include \"core/_header.html\" %}\n    <main class=\"container\">\n      {% if alert %}\n        <div class=\"alert {{ 'alert-success' if alert.kind == 'success' else 'alert-error' }}\">{{ alert.text }}</div>\n      {% endif %}\n      {% block content %}{% endblock %}\n    </main>\n  </body>\n</html>\n"
    },
    {
      "path": "src/templates/core/index.html",
      "language": "html",
      "size_bytes": 1274,
      "sha256": "fb02205385b58cfe12c542db68d561b06cf0c6c4bda8bcf6e2dc0ce6cdc67d69",
      "content": "{% extends \"core/base.html\" %}\n\n{% block title %}Главная — fsnb-fastapi{% endblock %}\n\n{% block head_extra %}\n  {{ super() }}\n  <script src=\"{{ request.url_for('static', path='js/fsnb_matcher.js') }}\" defer></script>\n{% endblock %}\n\n{% block content %}\n  <h1>Главная</h1>\n\n  {% if verify_link %}\n    <p style=\"margin:.5rem 0 1rem\">\n      <small>Письмо не пришло? Можно подтвердить прямо сейчас:\n        <a href=\"{{ verify_link }}\">подтвердить e-mail</a>.\n      </small>\n    </p>\n  {% endif %}\n\n  <p>Перейти к <a href=\"{{ request.url_for('users_list_html') }}\">списку пользователей</a>.</p>\n\n  {% set is_auth = request.session.get('access_token') and request.session.get('user_email') %}\n  {% if is_auth %}\n    {% include \"fsnb_matcher/widget.html\" %}\n  {% else %}\n    <div class=\"card\">\n      <p>\n        Для использования функции сопоставления спецификации с ФСНБ-2022 (ВОР) необходимо\n        <a href=\"{{ request.url_for('login_html') }}\">войти</a> или\n        <a href=\"{{ request.url_for('register_html') }}\">зарегистрироваться</a>.\n      </p>\n    </div>\n  {% endif %}\n{% endblock %}\n"
    },
    {
      "path": "src/templates/core/login.html",
      "language": "html",
      "size_bytes": 909,
      "sha256": "88f788055848c725ea0047b7eeb459ccd88f49ea7d345e47230e51107e655929",
      "content": "{% extends \"core/base.html\" %}\n{% block title %}Вход — fsnb-fastapi{% endblock %}\n\n{% block content %}\n  <h2>Вход</h2>\n\n  {% if alert %}\n    <div class=\"alert {{ alert.kind }}\">{{ alert.text }}</div>\n  {% endif %}\n\n  <form method=\"post\" action=\"{{ request.url_for('login_post_html') }}\">\n    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf }}\" />\n\n    <div class=\"form-row\">\n      <label>E-mail</label>\n      <input type=\"email\" name=\"email\" required placeholder=\"email@domain.tld\" />\n    </div>\n\n    <div class=\"form-row\">\n      <label>Пароль</label>\n      <input type=\"password\" name=\"password\" required placeholder=\"********\" />\n    </div>\n\n    <div class=\"form-row\">\n      <label>Капча: {{ a }} + {{ b }} = ?</label>\n      <input type=\"number\" name=\"captcha\" required />\n    </div>\n\n    <button class=\"btn btn-primary\" type=\"submit\">Войти</button>\n  </form>\n{% endblock %}\n"
    },
    {
      "path": "src/templates/core/profile.html",
      "language": "html",
      "size_bytes": 3384,
      "sha256": "9957e6dcf0cdf83b06121f9c6623ce269b428b9759f15d2b410c8702d43ceb53",
      "content": "{% extends \"core/base.html\" %}\n\n{% block head_extra %}\n  {{ super() }}\n  <script src=\"{{ url_for('static', path='js/avatar-preview.js') }}\" defer></script>\n{% endblock %}\n{% block content %}\n<h1 class=\"title-row\">\n  <span>Профиль</span>\n\n  <div class=\"title-right\">\n    <img\n      id=\"avatar-mini\"\n      class=\"avatar-mini\"\n      src=\"{% if profile.avatar %}{{ url_for('static', path=profile.avatar) }}{% else %}{{ url_for('static', path='img/photo_cap.jpg') }}{% endif %}\"\n      alt=\"avatar\"\n      width=\"40\"\n      height=\"40\"\n    />\n    {% if profile.avatar %}\n      <form method=\"post\" action=\"{{ request.url_for('profile_avatar_delete') }}\" class=\"avatar-actions\">\n        <button type=\"submit\" class=\"btn btn-danger btn-xs\">Удалить аватар</button>\n      </form>\n    {% endif %}\n  </div>\n</h1>\n\n{% if alert %}\n  <div class=\"alert alert-{{ alert.kind }}\">{{ alert.text }}</div>\n{% endif %}\n\n<div class=\"card\">\n  <p><strong>User:</strong> {{ user.email }} | id={{ user.id }}</p>\n  <p><strong>Верификация e-mail:</strong>\n    {% if user.activation_key %}<span class=\"warn\">не подтверждён</span>\n    {% else %}<span class=\"ok\">подтверждён</span>\n    {% endif %}\n  </p>\n</div>\n\n<form method=\"post\" action=\"{{ request.url_for('profile_post_html') }}\" enctype=\"multipart/form-data\">\n  <div class=\"form-row\">\n    <label>Nickname</label>\n    <input type=\"text\" name=\"nickname\" value=\"{{ profile.nickname or '' }}\">\n  </div>\n\n  <div class=\"form-row\">\n    <label>Avatar</label>\n    <div class=\"avatar-uploader\">\n      <div class=\"avatar-preview\">\n        <img\n          id=\"avatar-preview-img\"\n          src=\"{% if profile.avatar %}{{ url_for('static', path=profile.avatar) }}{% else %}{{ url_for('static', path='img/photo_cap.jpg') }}{% endif %}\"\n          alt=\"avatar preview\"\n        />\n      </div>\n      <div class=\"avatar-controls\">\n        <input id=\"avatar-input\" type=\"file\" name=\"avatar\" accept=\"image/*\" hidden>\n        <label for=\"avatar-input\" class=\"btn btn-primary\">Выбрать файл</label>\n        <small class=\"hint\">JPG/PNG/GIF/WebP, минимум 40×40, до 3&nbsp;МБ.</small>\n        <span id=\"avatar-filename\" class=\"filename\"></span>\n      </div>\n    </div>\n  </div>\n\n  <div class=\"form-row\">\n    <label>Имя</label>\n    <input type=\"text\" name=\"first_name\" value=\"{{ profile.first_name or '' }}\">\n  </div>\n  <div class=\"form-row\">\n    <label>Фамилия</label>\n    <input type=\"text\" name=\"second_name\" value=\"{{ profile.second_name or '' }}\">\n  </div>\n  <div class=\"form-row\">\n    <label>Телефон</label>\n    <input type=\"text\" name=\"phone\" value=\"{{ profile.phone or '' }}\">\n  </div>\n  <div class=\"form-row\">\n    <label>E-mail (в профиле)</label>\n    <input type=\"email\" name=\"email_field\" value=\"{{ profile.email or '' }}\">\n  </div>\n  <div class=\"form-row\">\n    <label>Telegram ID</label>\n    <input type=\"text\" name=\"tg_id\" value=\"{{ profile.tg_id or '' }}\">\n  </div>\n  <div class=\"form-row\">\n    <label>Telegram nickname</label>\n    <input type=\"text\" name=\"tg_nickname\" value=\"{{ profile.tg_nickname or '' }}\">\n  </div>\n  <div class=\"form-row\">\n    <label>Session (строка)</label>\n    <input type=\"text\" name=\"session_str\" value=\"{{ profile.session or '' }}\">\n  </div>\n\n  <button type=\"submit\" class=\"btn btn-primary\">Сохранить</button>\n</form>\n{% endblock %}\n"
    },
    {
      "path": "src/templates/core/register.html",
      "language": "html",
      "size_bytes": 1131,
      "sha256": "b758aa21a9c458b18d7337fc0c893d063658571c501c7a8374742bcea81f6938",
      "content": "<!-- /templates/core/register.html -->\n{% extends \"core/base.html\" %}\n{% block title %}Регистрация — fsnb-fastapi{% endblock %}\n\n{% block content %}\n  <h2>Регистрация</h2>\n  <form method=\"post\" action=\"{{ request.url_for('register_post_html') }}\">\n    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf }}\" />\n    <div class=\"form-row\">\n      <label>E-mail</label>\n      <input name=\"email\" type=\"email\" required placeholder=\"email@domain.tld\" />\n    </div>\n    <div class=\"form-row\">\n      <label>Пароль</label>\n      <input name=\"password\" type=\"password\" minlength=\"8\" maxlength=\"256\" required placeholder=\"********\" />\n    </div>\n    <div class=\"form-row\">\n      <label>Повтор пароля</label>\n      <input name=\"password2\" type=\"password\" minlength=\"8\" maxlength=\"256\" required placeholder=\"********\" />\n    </div>\n    <div class=\"form-row\">\n      <label>Капча: {{ a }} + {{ b }} = ?</label>\n      <input name=\"captcha\" type=\"number\" required />\n    </div>\n    <button class=\"btn btn-primary\" type=\"submit\">Зарегистрироваться</button>\n  </form>\n{% endblock %}\n"
    },
    {
      "path": "src/templates/fsnb_matcher/widget.html",
      "language": "html",
      "size_bytes": 668,
      "sha256": "ef27ed5e5115436ea3ac25c33eae60aba3faf2be55b74fec4a228b455935512a",
      "content": "{# templates/fsnb_matcher/widget.html #}\n<section class=\"card\">\n  <h2>ФСНБ-2022 — сопоставление позиций (Giga)</h2>\n  <p>Загрузите JSON вида: <code>{\"items\":[{\"Caption\":\"\",\"Units\":\"\",\"Quantity\":\"\"}]}</code></p>\n\n  <form id=\"fsnb-form\">\n    <input id=\"fsnb-file\" type=\"file\" name=\"file\" accept=\".json\" required hidden>\n    <label for=\"fsnb-file\" class=\"btn btn-primary\">Выбрать JSON</label>\n    <button type=\"submit\" class=\"btn btn-primary\">Сопоставить и скачать Excel</button>\n  </form>\n\n  <div id=\"fsnb-overlay\" class=\"overlay\" style=\"display:none\">\n    <div class=\"spinner\"></div>\n  </div>\n</section>\n"
    },
    {
      "path": "src/templates/users/list.html",
      "language": "html",
      "size_bytes": 605,
      "sha256": "ca0428c58300fee5854c16bed37ec1cb97cc276e81cd95e79c3fd3668d693c0b",
      "content": "<!-- /templates/users/list.html -->\n{% extends \"core/base.html\" %}\n{% block title %}Пользователи — fsnb-fastapi{% endblock %}\n\n{% block content %}\n  <h2>Пользователи</h2>\n  <table class=\"table\">\n    <thead>\n      <tr><th>ID</th><th>Email</th><th>Username</th><th>Active</th></tr>\n    </thead>\n    <tbody>\n      {% for u in users %}\n        <tr>\n          <td>{{ u.id }}</td>\n          <td>{{ u.email }}</td>\n          <td>{{ u.username or '' }}</td>\n          <td>{{ '✓' if u.is_active else '—' }}</td>\n        </tr>\n      {% endfor %}\n    </tbody>\n  </table>\n{% endblock %}\n"
    },
    {
      "path": "static/css/style.css",
      "language": "css",
      "size_bytes": 5890,
      "sha256": "6cacd68092e9657e8196c87a6ebd525dff41d1f564e2caad412822e185536c29",
      "content": "/* /static/css/style.css */\n\n/* 1) Variables */\n:root {\n  --gap: 16px;\n  --radius: 10px;\n  --ok: #19a974;\n  --ok-bg: #e6fbf2;\n  --err: #d33;\n  --err-bg: #fdecec;\n}\n\n/* 2) Base */\n*,\n*::before,\n*::after {\n  box-sizing: border-box;\n}\nbody {\n  margin: 0;\n  font-family: system-ui, -apple-system, \"Segoe UI\", Roboto, sans-serif;\n  color: #111;\n  background: #fafafa;\n}\na {\n  color: #0366d6;\n  text-decoration: none;\n}\na:hover {\n  text-decoration: underline;\n}\n\n/* 3) Layout */\n.container {\n  max-width: 1000px;\n  margin: 0 auto;\n  padding: var(--gap);\n}\n\n/* Header / Navbar */\n.header {\n  background: #fff;\n  border-bottom: 1px solid #eee;\n}\n.header-inner {\n  display: flex;\n  align-items: center;\n  justify-content: space-between;\n  gap: var(--gap);\n}\n.brand {\n  font-weight: 700;\n}\n.nav-list {\n  display: flex;\n  align-items: center;\n  gap: 12px;\n  list-style: none;\n  margin: 0;\n  padding: 0;\n}\n.nav-item {\n  display: flex;\n  align-items: center;\n}\n.nav-logout {\n  background: none;\n  border: 0;\n  padding: 0;\n  cursor: pointer;\n  text-decoration: underline;\n  font: inherit;\n}\n\n/* Back-compat для старой разметки шапки */\n.navbar {\n  display: flex;\n  align-items: center;\n  gap: var(--gap);\n}\n.navbar .brand {\n  font-weight: 700;\n}\n.spacer {\n  flex: 1;\n}\n\n/* 4) Tables */\n.table {\n  width: 100%;\n  border-collapse: collapse;\n}\n.table th,\n.table td {\n  border: 1px solid #ddd;\n  padding: 8px;\n}\n.table th {\n  background: #f3f3f3;\n  text-align: left;\n}\n\n/* 5) Forms */\n.form-row {\n  display: flex;\n  align-items: center;\n  gap: 12px;\n  margin-bottom: 12px;\n}\n.form-row > label {\n  flex: 0 0 180px; /* фиксированная ширина подписи */\n}\n.form-row > input,\n.form-row > select,\n.form-row > textarea {\n  flex: 1 1 auto;\n  min-width: 0;\n}\ninput[type=\"text\"],\ninput[type=\"email\"],\ninput[type=\"password\"],\ninput[type=\"number\"],\nselect,\ntextarea {\n  padding: 10px;\n  border: 1px solid #ccc;\n  border-radius: var(--radius);\n  background: #fff;\n}\n\n/* 6) Buttons */\n.btn {\n  padding: 8px 14px;\n  border-radius: var(--radius);\n  border: 1px solid #ccc;\n  background: #fff;\n  cursor: pointer;\n}\n.btn:hover {\n  filter: brightness(0.98);\n}\n.btn-primary {\n  border-color: #0366d6;\n  background: #0366d6;\n  color: #fff;\n}\n\n/* 7) Alerts */\n.alert {\n  margin-bottom: var(--gap);\n  padding: 12px 14px;\n  border-radius: var(--radius);\n  border: 2px solid transparent;\n}\n.alert.success {\n  border-color: var(--ok);\n  background: var(--ok-bg);\n}\n.alert.error {\n  border-color: var(--err);\n  background: var(--err-bg);\n}\n/* Карточка под шапкой профиля */\n.card{\n  background:#fff;\n  border:1px solid #eee;\n  border-radius:var(--radius);\n  padding:16px;\n  margin-bottom:16px;\n}\n\n/* Общая обёртка формы профиля (ширина как у регистрации) */\n.profile-form{\n  max-width:640px;\n}\n\n/* Сетка строк формы уже задана .form-row (align, gap, mb) — используем её.\n   Чтобы поле \"прилипало\" к правому краю строки, инпут растягиваем на всю ширину. */\n.form-row > input,\n.form-row > select,\n.form-row > textarea{\n  width:100%;\n}\n\n/* Правое выравнивание текста внутри инпутов */\n.input-stick-right{\n  text-align:right;\n}\n\n/* Ячейка для аватара: превью + input:file по правому краю */\n.avatar-cell{\n  display:flex;\n  flex-direction:column;\n  align-items:flex-end; /* прижать к правому краю */\n  gap:8px;\n}\n\n/* Превью аватара компактнее и с округлением */\n.avatar-preview img{\n  max-height:120px;\n  border-radius:8px;\n  display:block;\n}\n\n/* Ряд с кнопкой — кнопка у правого края */\n.form-actions{\n  display:flex;\n  justify-content:flex-end;\n  margin-top:8px;\n}\n/* Заголовок профиля с мини-аватаром справа */\n.profile-head{\n  display:flex;\n  align-items:center;\n  justify-content:space-between;\n  gap:12px;\n  margin-bottom:12px;\n}\n\n.profile-head-avatar{\n  width:40px;\n  height:40px;\n  border-radius:50%;\n  object-fit:cover;\n  border:1px solid #e5e5e5;\n  background:#fff;\n}\n.btn-danger { border-color: #d33; background: #d33; color: #fff; }\n/* Профиль: заголовок и мини-аватар справа */\n.title-row {\n  display: flex;\n  align-items: center;\n  justify-content: space-between;\n  gap: var(--gap);\n  margin-bottom: var(--gap);\n}\n\n/* Правая колонка в заголовке (мини-аватар + кнопка удаления) */\n.title-right {\n  display: flex;\n  flex-direction: column;\n  align-items: flex-end;\n  gap: 6px;\n}\n\n.avatar-mini {\n  border-radius: 50%;\n  object-fit: cover;\n}\n\n/* Кнопка маленького размера (для удаления аватара под мини-аватаром) */\n.btn-xs {\n  padding: 4px 8px;\n  font-size: 12px;\n}\n\n/* Красная кнопка */\n.btn-danger {\n  border: 1px solid #d33;\n  background: #d33;\n  color: #fff;\n}\n\n/* Зона загрузки аватара в форме */\n.avatar-uploader {\n  display: flex;\n  align-items: center;\n  gap: 12px;\n  flex-wrap: wrap;\n}\n\n.avatar-preview img {\n  max-height: 120px;\n  width: auto;\n  border-radius: 8px;\n  object-fit: cover;\n  display: block;\n}\n\n.avatar-controls {\n  display: flex;\n  align-items: center;\n  flex-wrap: wrap;\n  gap: 8px;\n}\n\n.avatar-controls .hint {\n  opacity: 0.8;\n  font-size: 12px;\n}\n\n.filename {\n  font-size: 12px;\n  opacity: 0.9;\n}\n\n.table-wrap { overflow-x: auto; }\n.admin-models { display: grid; grid-template-columns: repeat(auto-fill, minmax(220px, 1fr)); gap: 12px; }\n.admin-model { display: block; padding: 12px; border-radius: var(--radius); background: #fff; border: 1px solid #eee; }\n.admin-model__name { font-weight: 700; margin-bottom: 6px; }\n"
    },
    {
      "path": "static/js/app.js",
      "language": "javascript",
      "size_bytes": 122,
      "sha256": "8766499306abae5f9b5a07a4a2fc3a75e84b8b4699bb989c1fc7a6120843dd26",
      "content": "// /static/js/app.js\nwindow.addEventListener(\"DOMContentLoaded\", () => {\n  console.debug(\"[fsnb-fastapi] dom-ready\");\n});\n"
    },
    {
      "path": "static/js/avatar-preview.js",
      "language": "javascript",
      "size_bytes": 1754,
      "sha256": "15a87545332ca802c4b0542da0e59da7fa77707397feb4cb574b92b4c11a7a91",
      "content": "// /static/js/avatar-preview.js\n(function () {\n  const fi = document.getElementById('avatar-input');\n  const out = document.getElementById('avatar-filename');\n  const previewLarge = document.getElementById('avatar-preview-img');\n  const previewMini = document.getElementById('avatar-mini');\n\n  if (!fi) return;\n\n  const MAX_BYTES = 3 * 1024 * 1024;  // 3MB\n  const MIN_W = 40, MIN_H = 40;\n\n  function showAlert(msg) {\n    alert(msg);\n  }\n\n  fi.addEventListener('change', () => {\n    const file = fi.files && fi.files[0] ? fi.files[0] : null;\n    if (out) out.textContent = file ? file.name : '';\n    if (!file) return;\n\n    if (file.size > MAX_BYTES) {\n      showAlert('Файл слишком большой. Максимум 3 МБ.');\n      fi.value = '';\n      if (out) out.textContent = '';\n      return;\n    }\n\n    const url = URL.createObjectURL(file);\n    const img = new Image();\n    img.onload = () => {\n      if (img.width < MIN_W || img.height < MIN_H) {\n        showAlert('Минимальный размер изображения — 40×40 пикселей.');\n        URL.revokeObjectURL(url);\n        fi.value = '';\n        if (out) out.textContent = '';\n        return;\n      }\n      // Мгновенный предпросмотр (без сохранения на сервере)\n      if (previewLarge) previewLarge.src = url;\n      if (previewMini) previewMini.src = url;\n      // URL будет отревокнут после перезагрузки страницы\n    };\n    img.onerror = () => {\n      showAlert('Не удалось прочитать файл как изображение.');\n      fi.value = '';\n      if (out) out.textContent = '';\n      URL.revokeObjectURL(url);\n    };\n    img.src = url;\n  });\n})();\n"
    },
    {
      "path": "static/js/fsnb_matcher.js",
      "language": "javascript",
      "size_bytes": 1293,
      "sha256": "2373a78fd75f76fd9d2a99c0dd4e883f907c3294147ac53b150768a84c6eb8cf",
      "content": "// static/js/fsnb_matcher.js\n(function () {\n  const form = document.getElementById('fsnb-form');\n  if (!form) return;\n\n  const overlay = document.getElementById('fsnb-overlay');\n\n  form.addEventListener('submit', async (e) => {\n    e.preventDefault();\n    const fileInput = document.getElementById('fsnb-file');\n    if (!fileInput.files || fileInput.files.length === 0) {\n      alert('Выберите JSON файл');\n      return;\n    }\n\n    const fd = new FormData();\n    fd.append('file', fileInput.files[0]);\n\n    overlay.style.display = 'flex';\n    try {\n      const res = await fetch('/api/v1/fsnb/match', { method: 'POST', body: fd });\n      if (res.status === 401) {\n        alert('Нужно авторизоваться для использования сервиса.');\n        return;\n      }\n      if (!res.ok) {\n        const msg = await res.text();\n        throw new Error(msg || 'Ошибка сервера');\n      }\n      const blob = await res.blob();\n      const url = URL.createObjectURL(blob);\n      const a = document.createElement('a');\n      a.href = url; a.download = 'smeta.xlsx'; a.click();\n      URL.revokeObjectURL(url);\n    } catch (err) {\n      alert(String(err));\n    } finally {\n      overlay.style.display = 'none';\n      form.reset();\n    }\n  });\n})();\n"
    },
    {
      "path": "weights/Giga-Embeddings-instruct/1_Pooling/config.json",
      "language": "json",
      "size_bytes": 313,
      "sha256": "2bc529695125f68de57d1fd347e3d2920b993bc635c5f4f09a45e130c102a989",
      "content": "{\n    \"word_embedding_dimension\": 2048,\n    \"pooling_mode_cls_token\": false,\n    \"pooling_mode_mean_tokens\": true,\n    \"pooling_mode_max_tokens\": false,\n    \"pooling_mode_mean_sqrt_len_tokens\": false,\n    \"pooling_mode_weightedmean_tokens\": false,\n    \"pooling_mode_lasttoken\": false,\n    \"include_prompt\": true\n}"
    },
    {
      "path": "weights/Giga-Embeddings-instruct/README.md",
      "language": "markdown",
      "size_bytes": 9864,
      "sha256": "f35fbf27dfdfc7aa96f12a6a89adf224603421c794aee1a7792c72e5a1b010f9",
      "content": "---\nlicense: mit\nlanguage:\n- ru\n- en\npipeline_tag: feature-extraction\ntags:\n- MTEB\n- transformers\nlibrary_name: sentence-transformers\n---\n## Giga-Embeddings-instruct\n- Base Decoder-only LLM: GigaChat-3b\n- Pooling Type: Latent-Attention\n- Embedding Dimension: 2048\n\nДля получения более подробной информации о технических деталях, пожалуйста, обратитесь к нашей [статье](https://aclanthology.org/2025.bsnlp-1.3/).\n\n## Использование\n\nНиже приведен пример кодирования запросов и текстов.\n\n### Requirements\n\n```bash\npip install -q transformers==4.51.0 sentence-transformers==5.1.1 flash-attn langchain_community langchain_huggingface langchain_gigachat\n```\n\n### Transformers\n\n```python\nimport torch\nimport torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef get_detailed_instruct(task_description: str, query: str) -> str:\n    return f'Instruct: {task_description}\\nQuery: {query}'\n\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\n\nqueries = [\n    get_detailed_instruct(task, 'What is the capital of Russia?'),\n    get_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n    \"The capital of Russia is Moscow.\",\n    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\ntokenizer = AutoTokenizer.from_pretrained(\n    'ai-sage/Giga-Embeddings-instruct',\n    trust_remote_code=True\n)\nmodel = AutoModel.from_pretrained(\n    'ai-sage/Giga-Embeddings-instruct', \n    attn_implementation=\"flash_attention_2\", \n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True\n)\nmodel.eval()\nmodel.cuda()\n\nmax_length = 4096\n\n# Tokenize the input texts\nbatch_dict = tokenizer(\n    input_texts,\n    padding=True,\n    truncation=True,\n    max_length=max_length,\n    return_tensors=\"pt\",\n)\nbatch_dict.to(model.device)\nembeddings = model(**batch_dict, return_embeddings=True)\n\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.58203125, 0.0712890625], [0.06884765625, 0.62109375]]\n```\n\n### Sentence Transformers\n\n```python\nimport torch\n\nfrom sentence_transformers import SentenceTransformer\n\n# Load the model\n# We recommend enabling flash_attention_2 for better acceleration and memory saving\nmodel = SentenceTransformer(\n    \"ai-sage/Giga-Embeddings-instruct\",\n    model_kwargs={\n        \"attn_implementation\": \"flash_attention_2\", \n        \"torch_dtype\": torch.bfloat16, \n        \"trust_remote_code\": \"True\"\n    },\n    config_kwargs={\n        \"trust_remote_code\": \"True\"\n    }\n)\nmodel.max_seq_length = 4096\n\n# The queries and documents to embed\nqueries = [\n    'What is the capital of Russia?',\n    'Explain gravity'\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n    \"The capital of Russia is Moscow.\",\n    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\n\n# Encode the queries and documents. Note that queries benefit from using a prompt\nquery_embeddings = model.encode(queries, prompt='Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: ')\ndocument_embeddings = model.encode(documents)\n\n# Compute the (cosine) similarity between the query and document embeddings\nsimilarity = model.similarity(query_embeddings, document_embeddings)\nprint(similarity)\n# tensor([[0.5846, 0.0702],\n#         [0.0691, 0.6207]])\n```\n\n### LangChain\n\n```python\nimport torch\n\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\n# Load model\nembeddings = HuggingFaceEmbeddings(\n    model_name='ai-sage/Giga-Embeddings-instruct',\n    encode_kwargs={},\n    model_kwargs={\n        'device': 'cuda',\n        'trust_remote_code': True,\n        'model_kwargs': {'torch_dtype': torch.bfloat16},\n        'prompts': {'query': 'Instruct: Given a question, retrieve passages that answer the question\\nQuery: '}\n    }\n)\n\n# Tokenizer\nembeddings._client.tokenizer.tokenize(\"Hello world! I am GigaChat\")\n\n# Query embeddings\nquery_embeddings = embeddings.embed_query(\"Hello world!\")\nprint(f\"Your embeddings: {query_embeddings[0:20]}...\")\nprint(f\"Vector size: {len(query_embeddings)}\")\n\n# Document embeddings\ndocuments = [\"foo bar\", \"bar foo\"]\ndocuments_embeddings = embeddings.embed_documents(documents)\nprint(f\"Vector size: {len(documents_embeddings)} x {len(documents_embeddings[0])}\")\n```\n\n## Инструктивность\n\n**Использование инструкций для улучшения качества эмбеддингов**  \n\nДля достижения более точных результатов при работе с эмбеддингами, особенно в задачах поиска и извлечения информации (retrieval), рекомендуется добавлять инструкцию на естественном языке перед текстовым запросом (query). Это помогает модели лучше понять контекст и цель запроса, что положительно сказывается на качестве результатов. Важно отметить, что инструкцию нужно добавлять только перед запросом, а не перед документом.  \n\nДля **симметричных задач**, таких как классификация (classification) или семантическое сравнение текстов (semantic text similarity), инструкцию необходимо добавлять перед каждым запросом. Это связано с тем, что такие задачи требуют одинакового контекста для всех входных данных, чтобы модель могла корректно сравнивать или классифицировать их.  \n\n**Примеры инструкций для симметричных задач:**  \n- `\"Retrieve semantically similar text\"`  \n- `\"Given a text, retrieve semantically similar text\"`  \n- `\"Дано предложение, необходимо найти его парафраз\"`  \n- `\"Классифицируй отзыв на товар как положительный, отрицательный или нейтральный\"`  \n- `\"Классифицируй чувствительную тему по запросу\"`  \n\nДля **retrieval-задач** (например, поиск ответа в тексте) можно использовать инструкцию:  \n`'Дан вопрос, необходимо найти абзац текста с ответом'`.  \n\nТакой подход особенно эффективен для задач поиска и извлечения информации, таких как поиск релевантных документов или извлечение ответов из текста.\n\n**Примеры инструкций для retrieval-задач:**   \n- `'Дан вопрос, необходимо найти абзац текста с ответом'`\n- `'Given the question, find a paragraph with the answer'`     \n\nИнструкции необходимо оборачивать в шаблон: `f'Instruct: {task_description}\\nQuery: {query}'`. Использование инструкций позволяет значительно улучшить качество поиска и релевантность результатов, что подтверждается тестами на бенчмарках, таких как RuBQ, MIRACL. Для симметричных задач добавление инструкции перед каждым запросом обеспечивает согласованность и повышает точность модели.\n\n## Поддерживаемые языки\n\nЭта модель инициализирована pretrain моделью GigaChat и дополнительно обучена на смеси английских и русских данных.\n\n## FAQ\n\n1. Нужно ли добавлять инструкции к запросу?\n\nДа, именно так модель обучалась, иначе вы увидите снижение качества. Определение задачи должно быть инструкцией в одном предложении, которая описывает задачу. Это способ настройки текстовых эмбеддингов для разных сценариев с помощью инструкций на естественном языке.\n\nС другой стороны, добавлять инструкции на сторону документа не требуется.\n\n2. Почему мои воспроизведённые результаты немного отличаются от указанных в карточке модели?\n\nРазные версии библиотек transformers и pytorch могут вызывать незначительные, но ненулевые различия в результатах.\n\n\n## Ограничения\n\nИспользование этой модели для входных данных, содержащих более 4096 токенов, невозможно."
    },
    {
      "path": "weights/Giga-Embeddings-instruct/config.json",
      "language": "json",
      "size_bytes": 5895,
      "sha256": "3d29a7884a422634115f3597bf111a898c068b53e2152b214f8ec02f2c0edf53",
      "content": "{\n  \"_non_freeze_layers_idxs\": null,\n  \"activation_checkpoint_layers_num\": null,\n  \"add_eos\": true,\n  \"add_pad_token\": true,\n  \"apply_torch_compile_to_projections\": true,\n  \"architectures\": [\n    \"GigarEmbedModel\"\n  ],\n  \"auto_map\": {\n    \"AutoConfig\": \"configuration_gigarembed.GigarEmbedConfig\",\n    \"AutoModel\": \"modeling_gigarembed.GigarEmbedModel\"\n  },\n  \"hidden_size\": 2048,\n  \"is_mask_instruction\": true,\n  \"latent_attention_config\": {\n    \"_attn_implementation_autoset\": false,\n    \"add_cross_attention\": false,\n    \"architectures\": null,\n    \"bad_words_ids\": null,\n    \"begin_suppress_tokens\": null,\n    \"bos_token_id\": null,\n    \"chunk_size_feed_forward\": 0,\n    \"cross_attention_hidden_size\": null,\n    \"cross_dim_head\": 2048,\n    \"decoder_start_token_id\": null,\n    \"diversity_penalty\": 0.0,\n    \"do_sample\": false,\n    \"early_stopping\": false,\n    \"encoder_no_repeat_ngram_size\": 0,\n    \"eos_token_id\": null,\n    \"exponential_decay_length_penalty\": null,\n    \"finetuning_task\": null,\n    \"forced_bos_token_id\": null,\n    \"forced_eos_token_id\": null,\n    \"hidden_dim\": 2048,\n    \"id2label\": {\n      \"0\": \"LABEL_0\",\n      \"1\": \"LABEL_1\"\n    },\n    \"is_decoder\": false,\n    \"is_encoder_decoder\": false,\n    \"label2id\": {\n      \"LABEL_0\": 0,\n      \"LABEL_1\": 1\n    },\n    \"latent_dim\": 2048,\n    \"length_penalty\": 1.0,\n    \"max_length\": 20,\n    \"min_length\": 0,\n    \"model_type\": \"latent_attention\",\n    \"mult\": 4,\n    \"no_repeat_ngram_size\": 0,\n    \"num_beam_groups\": 1,\n    \"num_beams\": 1,\n    \"num_cross_heads\": 8,\n    \"num_latents_value\": 512,\n    \"num_return_sequences\": 1,\n    \"output_attentions\": false,\n    \"output_hidden_states\": false,\n    \"output_scores\": false,\n    \"pad_token_id\": null,\n    \"prefix\": null,\n    \"problem_type\": null,\n    \"pruned_heads\": {},\n    \"remove_invalid_values\": false,\n    \"repetition_penalty\": 1.0,\n    \"return_dict\": true,\n    \"return_dict_in_generate\": false,\n    \"sep_token_id\": null,\n    \"suppress_tokens\": null,\n    \"task_specific_params\": null,\n    \"temperature\": 1.0,\n    \"tf_legacy_loss\": false,\n    \"tie_encoder_decoder\": false,\n    \"tie_word_embeddings\": true,\n    \"tokenizer_class\": null,\n    \"top_k\": 50,\n    \"top_p\": 1.0,\n    \"torch_dtype\": null,\n    \"torchscript\": false,\n    \"typical_p\": 1.0,\n    \"use_bfloat16\": false\n  },\n  \"mask_type\": \"b\",\n  \"model_type\": \"gigarembed\",\n  \"padding_side\": \"right\",\n  \"text_config\": {\n    \"_attn_implementation_autoset\": false,\n    \"add_cross_attention\": false,\n    \"apply_qk_norm\": true,\n    \"architectures\": null,\n    \"attention_bias\": false,\n    \"attention_dropout\": 0.0,\n    \"attention_hidden_size\": null,\n    \"attention_type\": \"LlamaLatentAttention\",\n    \"bad_words_ids\": null,\n    \"begin_suppress_tokens\": null,\n    \"bos_token_id\": 1,\n    \"chunk_size_feed_forward\": 0,\n    \"cross_attention_hidden_size\": null,\n    \"decoder_start_token_id\": null,\n    \"delete_logits\": true,\n    \"deterministic_attention\": false,\n    \"diversity_penalty\": 0.0,\n    \"do_sample\": false,\n    \"early_stopping\": false,\n    \"enable_async_tp\": false,\n    \"encoder_no_repeat_ngram_size\": 0,\n    \"eos_token_id\": 2,\n    \"exponential_decay_length_penalty\": null,\n    \"finetuning_task\": null,\n    \"forced_bos_token_id\": null,\n    \"forced_eos_token_id\": null,\n    \"freeze_non_embed\": false,\n    \"fused_mlp\": true,\n    \"fused_mlp_checkpoint_lvl\": 3,\n    \"head_dim\": 64,\n    \"hidden_act\": \"silu\",\n    \"hidden_size\": 2048,\n    \"id2label\": {\n      \"0\": \"LABEL_0\",\n      \"1\": \"LABEL_1\"\n    },\n    \"ignore_index\": -100,\n    \"init_device\": \"meta\",\n    \"initializer_range\": 0.02,\n    \"intermediate_size\": 11008,\n    \"is_decoder\": false,\n    \"is_encoder_decoder\": false,\n    \"kv_lora_rank\": 1024,\n    \"label2id\": {\n      \"LABEL_0\": 0,\n      \"LABEL_1\": 1\n    },\n    \"length_penalty\": 1.0,\n    \"lora_alpha\": null,\n    \"lora_r\": null,\n    \"loss_inplace_backward\": false,\n    \"max_length\": 20,\n    \"max_position_embeddings\": 4096,\n    \"max_window_layers\": 36,\n    \"min_length\": 0,\n    \"mla_config\": {\n      \"kv_lora_rank\": 1024,\n      \"q_lora_rank\": 0,\n      \"qk_nope_head_dim\": 64,\n      \"qk_rope_head_dim\": 64,\n      \"v_head_dim\": 128\n    },\n    \"mlp_bias\": false,\n    \"model_type\": \"gigar\",\n    \"mtp_loss_weight\": 0.1,\n    \"mtp_predictor_num\": 1,\n    \"no_repeat_ngram_size\": 0,\n    \"norm_type\": \"LlamaRMSNorm\",\n    \"num_attention_heads\": 16,\n    \"num_beam_groups\": 1,\n    \"num_beams\": 1,\n    \"num_hidden_layers\": 36,\n    \"num_key_value_heads\": 16,\n    \"num_return_sequences\": 1,\n    \"output_attentions\": false,\n    \"output_hidden_states\": false,\n    \"output_scores\": false,\n    \"pad_token_id\": 2,\n    \"parallel_embedding_type\": \"EmbeddingParallelEmbedding\",\n    \"prefix\": null,\n    \"pretraining_tp\": 1,\n    \"problem_type\": null,\n    \"pruned_heads\": {},\n    \"q_lora_rank\": 0,\n    \"qk_nope_head_dim\": 64,\n    \"qk_rope_head_dim\": 64,\n    \"remove_invalid_values\": false,\n    \"repetition_penalty\": 1.0,\n    \"return_dict\": true,\n    \"return_dict_in_generate\": false,\n    \"rms_norm_eps\": 1e-06,\n    \"rope_scaling\": null,\n    \"rope_theta\": 100000.0,\n    \"sep_token_id\": null,\n    \"skip_init_tp_modules\": true,\n    \"sliding_window\": null,\n    \"sp_split_type\": \"equal\",\n    \"suppress_tokens\": null,\n    \"task_specific_params\": null,\n    \"temperature\": 1.0,\n    \"tf_legacy_loss\": false,\n    \"tie_encoder_decoder\": false,\n    \"tie_word_embeddings\": false,\n    \"tokenizer_class\": null,\n    \"top_k\": 50,\n    \"top_p\": 1.0,\n    \"torch_dtype\": null,\n    \"torchscript\": false,\n    \"tp_group\": null,\n    \"tp_size\": 1,\n    \"typical_p\": 1.0,\n    \"unk_token_id\": 0,\n    \"use_bfloat16\": false,\n    \"use_cache\": false,\n    \"use_cache_force\": false,\n    \"use_custom_rotary_kernel\": false,\n    \"use_liger\": false,\n    \"use_mrope\": false,\n    \"use_mtp\": true,\n    \"use_sliding_window\": false,\n    \"v_head_dim\": 128,\n    \"varlen_input\": true,\n    \"vocab_size\": 128256,\n    \"z_loss_eps\": 5e-05\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.51.0\"\n}\n"
    },
    {
      "path": "weights/Giga-Embeddings-instruct/config_sentence_transformers.json",
      "language": "json",
      "size_bytes": 283,
      "sha256": "9986c6b1724526135bb80978ae31b52aa650ed1f85149c2ccedc2453f05bce79",
      "content": "{\n  \"model_type\": \"SentenceTransformer\",\n  \"__version__\": {\n    \"sentence_transformers\": \"5.1.1\",\n    \"transformers\": \"4.51.0\",\n    \"pytorch\": \"2.5.1+cu124\"\n  },\n  \"prompts\": {\n    \"query\": \"\",\n    \"document\": \"\"\n  },\n  \"default_prompt_name\": null,\n  \"similarity_fn_name\": \"cosine\"\n}"
    },
    {
      "path": "weights/Giga-Embeddings-instruct/configuration_gigarembed.py",
      "language": "python",
      "size_bytes": 14426,
      "sha256": "0b34786d1c8ee25c2df02d427657a89b2bfad4c4cf8f4858f2007bb223668962",
      "content": "import warnings\n\nfrom typing import Literal\nfrom transformers import AutoConfig\nfrom transformers.models.auto import CONFIG_MAPPING\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.modeling_rope_utils import rope_config_validation\n\nGIGAREMBED_TYPE = \"gigarembed\"\nLATENT_ATTENTION_TYPE = \"latent_attention\"\n\n\nclass GigarConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`GigarModel`]. It is used to instantiate an Gigar\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the Gigar-7B.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 32000):\n            Vocabulary size of the Gigar model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`GigarModel`]\n        hidden_size (`int`, *optional*, defaults to 4096):\n            Dimension of the hidden representations.\n        intermediate_size (`int`, *optional*, defaults to 11008):\n            Dimension of the MLP representations.\n        num_hidden_layers (`int`, *optional*, defaults to 32):\n            Number of hidden layers in the Transformer decoder.\n        num_attention_heads (`int`, *optional*, defaults to 32):\n            Number of attention heads for each attention layer in the Transformer decoder.\n        num_key_value_heads (`int`, *optional*):\n            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n            by meanpooling all the original heads within that group. For more details checkout [this\n            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n            `num_attention_heads`.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n            The non-linear activation function (function or string) in the decoder.\n        max_position_embeddings (`int`, *optional*, defaults to 2048):\n            The maximum sequence length that this model might ever be used with. Gigar 1 supports up to 2048 tokens,\n            Gigar 2 up to 4096, CodeLlama up to 16384.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n            The epsilon used by the rms normalization layers.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        pad_token_id (`int`, *optional*):\n            Padding token id.\n        bos_token_id (`int`, *optional*, defaults to 1):\n            Beginning of stream token id.\n        eos_token_id (`int`, *optional*, defaults to 2):\n            End of stream token id.\n        pretraining_tp (`int`, *optional*, defaults to 1):\n            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to\n            understand more about it. This value is necessary to ensure exact reproducibility of the pretraining\n            results. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/76232).\n        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n            Whether to tie weight embeddings\n        rope_theta (`float`, *optional*, defaults to 10000.0):\n            The base period of the RoPE embeddings.\n        rope_scaling (`Dict`, *optional*):\n            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n            accordingly.\n            Expected contents:\n                `rope_type` (`str`):\n                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n                    'gigar3'], with 'default' being the original RoPE implementation.\n                `factor` (`float`, *optional*):\n                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n                    original maximum pre-trained length.\n                `original_max_position_embeddings` (`int`, *optional*):\n                    Used with 'dynamic', 'longrope' and 'gigar3'. The original max position embeddings used during\n                    pretraining.\n                `attention_factor` (`float`, *optional*):\n                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n                    `factor` field to infer the suggested value.\n                `beta_fast` (`float`, *optional*):\n                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n                    ramp function. If unspecified, it defaults to 32.\n                `beta_slow` (`float`, *optional*):\n                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n                    ramp function. If unspecified, it defaults to 1.\n                `short_factor` (`List[float]`, *optional*):\n                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n                    size divided by the number of attention heads divided by 2\n                `long_factor` (`List[float]`, *optional*):\n                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n                    size divided by the number of attention heads divided by 2\n                `low_freq_factor` (`float`, *optional*):\n                    Only used with 'gigar3'. Scaling factor applied to low frequency components of the RoPE\n                `high_freq_factor` (`float`, *optional*):\n                    Only used with 'gigar3'. Scaling factor applied to high frequency components of the RoPE\n        attention_bias (`bool`, *optional*, defaults to `False`):\n            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n        attention_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for the attention probabilities.\n        mlp_bias (`bool`, *optional*, defaults to `False`):\n            Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n        head_dim (`int`, *optional*):\n            The attention head dimension. If None, it will default to hidden_size // num_attention_heads\n\n    ```python\n    >>> from transformers import GigarModel, GigarConfig\n\n    >>> # Initializing a Gigar gigar-7b style configuration\n    >>> configuration = GigarConfig()\n\n    >>> # Initializing a model from the gigar-7b style configuration\n    >>> model = GigarModel(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n\n    model_type = \"gigar\"\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n    # Default tensor parallel plan for base model `GigarModel`\n    base_model_tp_plan = {\n        \"layers.*.self_attn.q_proj\": \"colwise\",\n        \"layers.*.self_attn.k_proj\": \"colwise\",\n        \"layers.*.self_attn.v_proj\": \"colwise\",\n        \"layers.*.self_attn.o_proj\": \"rowwise\",\n        \"layers.*.mlp.gate_proj\": \"colwise\",\n        \"layers.*.mlp.up_proj\": \"colwise\",\n        \"layers.*.mlp.down_proj\": \"rowwise\",\n    }\n\n    def __init__(\n        self,\n        vocab_size=32000,\n        hidden_size=4096,\n        intermediate_size=11008,\n        num_hidden_layers=32,\n        num_attention_heads=32,\n        num_key_value_heads=None,\n        hidden_act=\"silu\",\n        max_position_embeddings=2048,\n        initializer_range=0.02,\n        rms_norm_eps=1e-6,\n        use_cache=True,\n        pad_token_id=None,\n        bos_token_id=1,\n        eos_token_id=2,\n        pretraining_tp=1,\n        tie_word_embeddings=False,\n        rope_theta=10000.0,\n        rope_scaling=None,\n        attention_bias=False,\n        attention_dropout=0.0,\n        mlp_bias=False,\n        head_dim=None,\n        apply_qk_norm=False,\n        mla_config=None,\n        **kwargs,\n    ):\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.pretraining_tp = pretraining_tp\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.rope_scaling = rope_scaling\n        self.attention_bias = attention_bias\n        self.attention_dropout = attention_dropout\n        self.mlp_bias = mlp_bias\n        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n        # Validate the correctness of rotary position embeddings parameters\n        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n        rope_config_validation(self)\n\n        self.apply_qk_norm = apply_qk_norm\n        self.mla_config = mla_config\n\n        self._validate_mla_config()\n\n    def _validate_mla_config(self):\n        if self.mla_config is None:\n            warnings.warn(\"MLA config is None!\")\n            return\n\n        EXPECTED_KEYS = [\n            \"qk_nope_head_dim\",\n            \"qk_rope_head_dim\",\n            \"v_head_dim\",\n            \"kv_lora_rank\",\n            \"q_lora_rank\",\n        ]\n        if not all((key in self.mla_config for key in EXPECTED_KEYS)):\n            raise ValueError(\n                f\"MLA config is expected to have the following keys {EXPECTED_KEYS} but got {self.mla_config.keys()}.\"\n            )\n\n        if self.mla_config[\"qk_nope_head_dim\"] + self.mla_config[\"qk_rope_head_dim\"] != self.mla_config[\"v_head_dim\"]:\n            err_msg = (\n                f\"QK and V head dims do not match! Got {self.mla_config['qk_nope_head_dim']} + {self.mla_config['qk_rope_head_dim']} \"\n                f\"= {self.mla_config['qk_rope_head_dim'] + self.mla_config['qk_nope_head_dim']} and {self.mla_config['v_head_dim']}.\"\n            )\n            raise ValueError(err_msg)\n\n\nclass GigarEmbedConfig(PretrainedConfig):\n    model_type = \"gigarembed\"\n    is_composition = False\n\n    def __init__(\n        self,\n        latent_attention_config=None,\n        text_config=None,\n        padding_side: Literal[\"right\", \"left\"]=\"right\",\n        add_pad_token: bool=True,\n        is_mask_instruction: bool = True,\n        add_eos: bool=True,\n        mask_type: str=\"b\",\n        **kwargs,\n    ):\n        if isinstance(latent_attention_config, dict):\n            latent_attention_config[\"model_type\"] = (\n                latent_attention_config[\"model_type\"] if \"model_type\" in latent_attention_config else LATENT_ATTENTION_TYPE\n            )\n            latent_attention_config = CONFIG_MAPPING[latent_attention_config[\"model_type\"]](**latent_attention_config)\n\n        self.latent_attention_config = latent_attention_config\n\n        if isinstance(text_config, dict):\n            text_config = GigarConfig(**text_config)\n        elif text_config is None:\n            text_config = None\n\n        self.text_config = text_config\n        self.padding_side = padding_side\n        self.is_mask_instruction = is_mask_instruction\n        self.add_pad_token = add_pad_token\n        self.add_eos = add_eos\n        self.mask_type = mask_type\n        if \"hidden_size\" in kwargs:\n            self.hidden_size = kwargs[\"hidden_size\"]\n\n        super().__init__(**kwargs)\n\n\nclass LatentAttentionConfig(PretrainedConfig):\n    model_type = LATENT_ATTENTION_TYPE\n    is_composition = False\n    _name_or_path = \"latent_attention\"\n\n    def __init__(\n        self,\n        num_latents_value: int,\n        num_cross_heads: int,\n        hidden_dim: int,\n        latent_dim: int,\n        cross_dim_head: int,\n        mult: int,\n        **kwargs,\n    ):\n        self.num_latents_value = num_latents_value\n        self.num_cross_heads = num_cross_heads\n        self.hidden_dim = hidden_dim\n        self.latent_dim = latent_dim\n        self.cross_dim_head = cross_dim_head\n        self.mult = mult\n\n        super().__init__(**kwargs)\n\n\nAutoConfig.register(GIGAREMBED_TYPE, GigarEmbedConfig)\nAutoConfig.register(LATENT_ATTENTION_TYPE, LatentAttentionConfig)\n\nGigarEmbedConfig.register_for_auto_class()\nLatentAttentionConfig.register_for_auto_class()\n"
    },
    {
      "path": "weights/Giga-Embeddings-instruct/model.safetensors.index.json",
      "language": "json",
      "size_bytes": 42876,
      "sha256": "0ea03eb0cad1fbb03a3b964ca0793ed6fa224396f63d5b400b1d80c0c3d430ee",
      "content": "{\n  \"metadata\": {\n    \"total_size\": 13798498304\n  },\n  \"weight_map\": {\n    \"latent_attention_model.cross_attend_blocks.0.to_kv.weight\": \"model-00001-of-00003.safetensors\",\n    \"latent_attention_model.cross_attend_blocks.0.to_out.weight\": \"model-00001-of-00003.safetensors\",\n    \"latent_attention_model.cross_attend_blocks.0.to_q.weight\": \"model-00001-of-00003.safetensors\",\n    \"latent_attention_model.cross_attend_blocks.1.down_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"latent_attention_model.cross_attend_blocks.1.gate_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"latent_attention_model.cross_attend_blocks.1.up_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"latent_attention_model.latents\": \"model-00001-of-00003.safetensors\",\n    \"model.embed_tokens.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.0.input_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.0.mlp.down_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.0.mlp.gate_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.0.mlp.up_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.0.post_attention_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.0.self_attn.dkv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.0.self_attn.kr_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.0.self_attn.kv_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.0.self_attn.o_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.0.self_attn.q_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.0.self_attn.qk_k_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.0.self_attn.qk_q_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.0.self_attn.uk_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.0.self_attn.uv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.1.input_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.1.mlp.down_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.1.mlp.gate_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.1.mlp.up_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.1.post_attention_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.1.self_attn.dkv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.1.self_attn.kr_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.1.self_attn.kv_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.1.self_attn.o_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.1.self_attn.q_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.1.self_attn.qk_k_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.1.self_attn.qk_q_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.1.self_attn.uk_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.1.self_attn.uv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.10.input_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.10.mlp.down_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.10.mlp.gate_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.10.mlp.up_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.10.post_attention_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.10.self_attn.dkv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.10.self_attn.kr_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.10.self_attn.kv_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.10.self_attn.o_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.10.self_attn.q_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.10.self_attn.qk_k_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.10.self_attn.qk_q_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.10.self_attn.uk_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.10.self_attn.uv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.11.input_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.11.mlp.down_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.11.mlp.gate_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.11.mlp.up_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.11.post_attention_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.11.self_attn.dkv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.11.self_attn.kr_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.11.self_attn.kv_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.11.self_attn.o_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.11.self_attn.q_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.11.self_attn.qk_k_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.11.self_attn.qk_q_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.11.self_attn.uk_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.11.self_attn.uv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.12.input_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.12.mlp.down_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.12.mlp.gate_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.12.mlp.up_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.12.post_attention_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.12.self_attn.dkv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.12.self_attn.kr_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.12.self_attn.kv_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.12.self_attn.o_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.12.self_attn.q_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.12.self_attn.qk_k_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.12.self_attn.qk_q_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.12.self_attn.uk_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.12.self_attn.uv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.13.input_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.13.mlp.down_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.13.mlp.gate_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.13.mlp.up_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.13.post_attention_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.13.self_attn.dkv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.13.self_attn.kr_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.13.self_attn.kv_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.13.self_attn.o_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.13.self_attn.q_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.13.self_attn.qk_k_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.13.self_attn.qk_q_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.13.self_attn.uk_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.13.self_attn.uv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.14.input_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.14.mlp.down_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.14.mlp.gate_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.14.mlp.up_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.14.post_attention_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.14.self_attn.dkv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.14.self_attn.kr_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.14.self_attn.kv_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.14.self_attn.o_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.14.self_attn.q_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.14.self_attn.qk_k_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.14.self_attn.qk_q_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.14.self_attn.uk_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.14.self_attn.uv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.15.input_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.15.mlp.down_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.15.mlp.gate_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.15.mlp.up_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.15.post_attention_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.15.self_attn.dkv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.15.self_attn.kr_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.15.self_attn.kv_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.15.self_attn.o_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.15.self_attn.q_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.15.self_attn.qk_k_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.15.self_attn.qk_q_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.15.self_attn.uk_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.15.self_attn.uv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.16.input_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.16.mlp.down_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.16.mlp.gate_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.16.mlp.up_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.16.post_attention_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.16.self_attn.dkv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.16.self_attn.kr_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.16.self_attn.kv_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.16.self_attn.o_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.16.self_attn.q_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.16.self_attn.qk_k_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.16.self_attn.qk_q_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.16.self_attn.uk_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.16.self_attn.uv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.17.input_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.17.mlp.down_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.17.mlp.gate_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.17.mlp.up_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.17.post_attention_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.17.self_attn.dkv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.17.self_attn.kr_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.17.self_attn.kv_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.17.self_attn.o_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.17.self_attn.q_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.17.self_attn.qk_k_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.17.self_attn.qk_q_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.17.self_attn.uk_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.17.self_attn.uv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.18.input_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.18.mlp.down_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.18.mlp.gate_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.18.mlp.up_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.18.post_attention_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.18.self_attn.dkv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.18.self_attn.kr_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.18.self_attn.kv_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.18.self_attn.o_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.18.self_attn.q_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.18.self_attn.qk_k_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.18.self_attn.qk_q_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.18.self_attn.uk_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.18.self_attn.uv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.19.input_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.19.mlp.down_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.19.mlp.gate_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.19.mlp.up_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.19.post_attention_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.19.self_attn.dkv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.19.self_attn.kr_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.19.self_attn.kv_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.19.self_attn.o_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.19.self_attn.q_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.19.self_attn.qk_k_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.19.self_attn.qk_q_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.19.self_attn.uk_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.19.self_attn.uv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.2.input_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.2.mlp.down_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.2.mlp.gate_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.2.mlp.up_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.2.post_attention_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.2.self_attn.dkv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.2.self_attn.kr_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.2.self_attn.kv_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.2.self_attn.o_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.2.self_attn.q_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.2.self_attn.qk_k_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.2.self_attn.qk_q_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.2.self_attn.uk_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.2.self_attn.uv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.20.input_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.20.mlp.down_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.20.mlp.gate_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.20.mlp.up_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.20.post_attention_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.20.self_attn.dkv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.20.self_attn.kr_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.20.self_attn.kv_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.20.self_attn.o_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.20.self_attn.q_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.20.self_attn.qk_k_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.20.self_attn.qk_q_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.20.self_attn.uk_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.20.self_attn.uv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.21.input_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.21.mlp.down_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.21.mlp.gate_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.21.mlp.up_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.21.post_attention_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.21.self_attn.dkv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.21.self_attn.kr_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.21.self_attn.kv_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.21.self_attn.o_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.21.self_attn.q_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.21.self_attn.qk_k_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.21.self_attn.qk_q_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.21.self_attn.uk_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.21.self_attn.uv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.22.input_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.22.mlp.down_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.22.mlp.gate_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.22.mlp.up_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.22.post_attention_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.22.self_attn.dkv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.22.self_attn.kr_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.22.self_attn.kv_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.22.self_attn.o_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.22.self_attn.q_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.22.self_attn.qk_k_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.22.self_attn.qk_q_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.22.self_attn.uk_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.22.self_attn.uv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.23.input_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.23.mlp.down_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.23.mlp.gate_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.23.mlp.up_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.23.post_attention_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.23.self_attn.dkv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.23.self_attn.kr_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.23.self_attn.kv_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.23.self_attn.o_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.23.self_attn.q_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.23.self_attn.qk_k_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.23.self_attn.qk_q_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.23.self_attn.uk_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.23.self_attn.uv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.24.input_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.24.mlp.down_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.24.mlp.gate_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.24.mlp.up_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.24.post_attention_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.24.self_attn.dkv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.24.self_attn.kr_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.24.self_attn.kv_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.24.self_attn.o_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.24.self_attn.q_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.24.self_attn.qk_k_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.24.self_attn.qk_q_norm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.24.self_attn.uk_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.24.self_attn.uv_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.25.input_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.25.mlp.down_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.25.mlp.gate_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.25.mlp.up_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.25.post_attention_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.25.self_attn.dkv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.25.self_attn.kr_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.25.self_attn.kv_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.25.self_attn.o_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.25.self_attn.q_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.25.self_attn.qk_k_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.25.self_attn.qk_q_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.25.self_attn.uk_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.25.self_attn.uv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.26.input_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.26.mlp.down_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.26.mlp.gate_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.26.mlp.up_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.26.post_attention_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.26.self_attn.dkv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.26.self_attn.kr_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.26.self_attn.kv_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.26.self_attn.o_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.26.self_attn.q_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.26.self_attn.qk_k_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.26.self_attn.qk_q_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.26.self_attn.uk_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.26.self_attn.uv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.27.input_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.27.mlp.down_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.27.mlp.gate_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.27.mlp.up_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.27.post_attention_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.27.self_attn.dkv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.27.self_attn.kr_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.27.self_attn.kv_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.27.self_attn.o_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.27.self_attn.q_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.27.self_attn.qk_k_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.27.self_attn.qk_q_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.27.self_attn.uk_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.27.self_attn.uv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.28.input_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.28.mlp.down_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.28.mlp.gate_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.28.mlp.up_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.28.post_attention_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.28.self_attn.dkv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.28.self_attn.kr_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.28.self_attn.kv_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.28.self_attn.o_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.28.self_attn.q_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.28.self_attn.qk_k_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.28.self_attn.qk_q_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.28.self_attn.uk_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.28.self_attn.uv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.29.input_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.29.mlp.down_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.29.mlp.gate_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.29.mlp.up_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.29.post_attention_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.29.self_attn.dkv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.29.self_attn.kr_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.29.self_attn.kv_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.29.self_attn.o_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.29.self_attn.q_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.29.self_attn.qk_k_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.29.self_attn.qk_q_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.29.self_attn.uk_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.29.self_attn.uv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.3.input_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.3.mlp.down_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.3.mlp.gate_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.3.mlp.up_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.3.post_attention_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.3.self_attn.dkv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.3.self_attn.kr_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.3.self_attn.kv_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.3.self_attn.o_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.3.self_attn.q_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.3.self_attn.qk_k_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.3.self_attn.qk_q_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.3.self_attn.uk_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.3.self_attn.uv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.30.input_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.30.mlp.down_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.30.mlp.gate_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.30.mlp.up_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.30.post_attention_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.30.self_attn.dkv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.30.self_attn.kr_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.30.self_attn.kv_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.30.self_attn.o_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.30.self_attn.q_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.30.self_attn.qk_k_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.30.self_attn.qk_q_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.30.self_attn.uk_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.30.self_attn.uv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.31.input_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.31.mlp.down_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.31.mlp.gate_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.31.mlp.up_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.31.post_attention_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.31.self_attn.dkv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.31.self_attn.kr_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.31.self_attn.kv_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.31.self_attn.o_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.31.self_attn.q_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.31.self_attn.qk_k_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.31.self_attn.qk_q_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.31.self_attn.uk_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.31.self_attn.uv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.32.input_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.32.mlp.down_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.32.mlp.gate_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.32.mlp.up_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.32.post_attention_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.32.self_attn.dkv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.32.self_attn.kr_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.32.self_attn.kv_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.32.self_attn.o_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.32.self_attn.q_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.32.self_attn.qk_k_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.32.self_attn.qk_q_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.32.self_attn.uk_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.32.self_attn.uv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.33.input_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.33.mlp.down_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.33.mlp.gate_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.33.mlp.up_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.33.post_attention_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.33.self_attn.dkv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.33.self_attn.kr_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.33.self_attn.kv_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.33.self_attn.o_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.33.self_attn.q_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.33.self_attn.qk_k_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.33.self_attn.qk_q_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.33.self_attn.uk_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.33.self_attn.uv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.34.input_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.34.mlp.down_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.34.mlp.gate_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.34.mlp.up_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.34.post_attention_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.34.self_attn.dkv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.34.self_attn.kr_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.34.self_attn.kv_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.34.self_attn.o_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.34.self_attn.q_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.34.self_attn.qk_k_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.34.self_attn.qk_q_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.34.self_attn.uk_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.34.self_attn.uv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.35.input_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.35.mlp.down_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.35.mlp.gate_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.35.mlp.up_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.35.post_attention_layernorm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.35.self_attn.dkv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.35.self_attn.kr_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.35.self_attn.kv_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.35.self_attn.o_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.35.self_attn.q_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.35.self_attn.qk_k_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.35.self_attn.qk_q_norm.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.35.self_attn.uk_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.35.self_attn.uv_proj.weight\": \"model-00003-of-00003.safetensors\",\n    \"model.layers.4.input_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.4.mlp.down_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.4.mlp.gate_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.4.mlp.up_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.4.post_attention_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.4.self_attn.dkv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.4.self_attn.kr_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.4.self_attn.kv_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.4.self_attn.o_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.4.self_attn.q_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.4.self_attn.qk_k_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.4.self_attn.qk_q_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.4.self_attn.uk_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.4.self_attn.uv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.5.input_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.5.mlp.down_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.5.mlp.gate_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.5.mlp.up_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.5.post_attention_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.5.self_attn.dkv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.5.self_attn.kr_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.5.self_attn.kv_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.5.self_attn.o_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.5.self_attn.q_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.5.self_attn.qk_k_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.5.self_attn.qk_q_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.5.self_attn.uk_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.5.self_attn.uv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.6.input_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.6.mlp.down_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.6.mlp.gate_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.6.mlp.up_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.6.post_attention_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.6.self_attn.dkv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.6.self_attn.kr_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.6.self_attn.kv_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.6.self_attn.o_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.6.self_attn.q_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.6.self_attn.qk_k_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.6.self_attn.qk_q_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.6.self_attn.uk_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.6.self_attn.uv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.7.input_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.7.mlp.down_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.7.mlp.gate_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.7.mlp.up_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.7.post_attention_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.7.self_attn.dkv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.7.self_attn.kr_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.7.self_attn.kv_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.7.self_attn.o_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.7.self_attn.q_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.7.self_attn.qk_k_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.7.self_attn.qk_q_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.7.self_attn.uk_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.7.self_attn.uv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.8.input_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.8.mlp.down_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.8.mlp.gate_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.8.mlp.up_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.8.post_attention_layernorm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.8.self_attn.dkv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.8.self_attn.kr_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.8.self_attn.kv_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.8.self_attn.o_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.8.self_attn.q_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.8.self_attn.qk_k_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.8.self_attn.qk_q_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.8.self_attn.uk_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.8.self_attn.uv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.9.input_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.9.mlp.down_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.9.mlp.gate_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.9.mlp.up_proj.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.9.post_attention_layernorm.weight\": \"model-00002-of-00003.safetensors\",\n    \"model.layers.9.self_attn.dkv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.9.self_attn.kr_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.9.self_attn.kv_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.9.self_attn.o_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.9.self_attn.q_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.9.self_attn.qk_k_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.9.self_attn.qk_q_norm.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.9.self_attn.uk_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.layers.9.self_attn.uv_proj.weight\": \"model-00001-of-00003.safetensors\",\n    \"model.norm.weight\": \"model-00003-of-00003.safetensors\"\n  }\n}\n"
    },
    {
      "path": "weights/Giga-Embeddings-instruct/modeling_gigarembed.py",
      "language": "python",
      "size_bytes": 44149,
      "sha256": "d75a06519ffacd39bce2608ceedc56818c072430e1072f26f7aa0b570b3f7ba2",
      "content": "import copy\nimport logging\nfrom typing import Callable, List, Optional, Tuple, Union, Mapping\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\n\nfrom einops import rearrange, repeat\nfrom transformers import AutoModel, AutoTokenizer\n\nfrom transformers.cache_utils import Cache\nfrom transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import DynamicCache, StaticCache\nfrom transformers.generation import GenerationMixin\nfrom transformers.modeling_attn_mask_utils import AttentionMaskConverter\nfrom transformers.modeling_flash_attention_utils import FlashAttentionKwargs\nfrom transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\nfrom transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.processing_utils import Unpack\nfrom transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings\n\nfrom .configuration_gigarembed import GigarConfig, GigarEmbedConfig, LatentAttentionConfig\n\n\nlogger = logging.getLogger(__name__)\n_CONFIG_FOR_DOC = \"GigarEmbedConfig\"\n\n\nclass GigarMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, x):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n        return down_proj\n\n\nclass GigarRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        GigarRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n    def extra_repr(self):\n        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\ndef eager_attention_forward(\n    module: nn.Module,\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    attention_mask: Optional[torch.Tensor],\n    scaling: float,\n    dropout: float = 0.0,\n    **kwargs,\n):\n    key_states = repeat_kv(key, module.num_key_value_groups)\n    value_states = repeat_kv(value, module.num_key_value_groups)\n\n    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n    if attention_mask is not None:\n        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n        attn_weights = attn_weights + causal_mask\n\n    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n    attn_output = torch.matmul(attn_weights, value_states)\n    attn_output = attn_output.transpose(1, 2).contiguous()\n\n    return attn_output, attn_weights\n\n\nclass GigarLatentAttention(nn.Module):\n    \"\"\"\n    Multi-headed Latent Attention (MLA)\n\n    Check out the original paper: https://arxiv.org/pdf/2405.04434,\n    and the reference implementation: https://github.com/deepseek-ai/DeepSeek-V3/blob/main/inference/model.py\n    \"\"\"\n\n    def __init__(self, config: GigarConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.layer_idx = layer_idx\n        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n\n        assert config.num_attention_heads == config.num_key_value_heads, (\n            \"GQA for MLA is not supported (does it even make sense?)\"\n        )\n        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.apply_qk_norm = config.apply_qk_norm\n        self.attention_dropout = config.attention_dropout\n\n        assert config.mla_config is not None\n        self.qk_nope_head_dim = config.mla_config[\"qk_nope_head_dim\"]\n        self.qk_rope_head_dim = config.mla_config[\"qk_rope_head_dim\"]\n        self.v_head_dim = config.mla_config[\"v_head_dim\"]  # V has no rope part\n        self.kv_lora_rank = config.mla_config[\"kv_lora_rank\"]\n        self.q_lora_rank = config.mla_config[\"q_lora_rank\"]\n\n        self.qk_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim\n\n        self.scaling = self.qk_head_dim**-0.5\n\n        if self.q_lora_rank == 0:\n            self.q_proj = nn.Linear(\n                self.hidden_size,\n                self.num_heads * self.qk_head_dim,\n                bias=config.attention_bias,\n            )\n        else:\n            self.dq_proj = nn.Linear(\n                self.hidden_size,\n                self.q_lora_rank,\n                bias=config.attention_bias,\n            )\n            self.q_norm = GigarRMSNorm(self.q_lora_rank)\n            self.uq_proj = nn.Linear(\n                self.q_lora_rank,\n                self.num_heads * self.qk_head_dim,\n                bias=config.attention_bias,\n            )\n\n        self.kv_norm = GigarRMSNorm(self.kv_lora_rank)\n        self.dkv_proj = nn.Linear(\n            self.hidden_size,\n            self.kv_lora_rank,\n            bias=config.attention_bias,\n        )\n        self.uk_proj = nn.Linear(\n            config.kv_lora_rank,\n            self.num_heads * self.qk_nope_head_dim,\n            bias=config.attention_bias,\n        )\n        self.uv_proj = nn.Linear(\n            config.kv_lora_rank,\n            self.num_heads * self.v_head_dim,\n            bias=config.attention_bias,\n        )\n        self.kr_proj = nn.Linear(\n            self.hidden_size,\n            self.num_heads * self.qk_rope_head_dim,\n            bias=config.attention_bias,\n        )\n\n        self.o_proj = nn.Linear(\n            self.num_heads * self.v_head_dim,\n            self.hidden_size,\n            bias=config.attention_bias,\n        )\n\n        if self.apply_qk_norm:\n            self.qk_q_norm = nn.LayerNorm(self.num_heads * self.qk_head_dim, bias=False)\n            self.qk_k_norm = nn.LayerNorm(self.num_heads * self.qk_head_dim, bias=False)\n\n        config_for_rope = copy.copy(self.config)\n        config_for_rope.head_dim = self.config.qk_rope_head_dim\n\n        self.is_causal = False\n\n    def _compute_qkv(\n        self,\n        hidden_states: torch.Tensor,\n    ):\n        \"\"\"Compute query, key, and value tensors from hidden states.\"\"\"\n        bsz, seq_len, _ = hidden_states.size()\n\n        if self.q_lora_rank == 0:\n            query = self.q_proj(hidden_states)\n        else:\n            query = self.uq_proj(self.q_norm(self.dq_proj(hidden_states)))\n\n        latent = self.dkv_proj(hidden_states)\n        latent = self.kv_norm(latent)\n        k_rope = self.kr_proj(hidden_states)\n\n        k_nope = self.uk_proj(latent)\n        value = self.uv_proj(latent)\n\n        if self.apply_qk_norm:\n            query = self.qk_q_norm(query).to(query.dtype)\n            key = self.qk_k_norm(torch.cat([k_nope, k_rope], dim=-1)).to(k_nope.dtype)\n            k_nope, k_rope = torch.split(key, [k_nope.shape[-1], k_rope.shape[-1]], dim=-1)\n\n        # Reshape tensors\n        query = query.view(bsz, seq_len, self.num_heads, self.qk_head_dim).transpose(1, 2)\n        k_nope = k_nope.view(bsz, seq_len, self.num_heads, self.qk_nope_head_dim).transpose(1, 2)\n        k_rope = k_rope.view(bsz, seq_len, self.num_heads, self.qk_rope_head_dim).transpose(1, 2)\n        value = value.view(bsz, seq_len, self.num_heads, self.v_head_dim).transpose(1, 2)\n\n        q_nope, q_rope = torch.split(query, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n\n        return q_nope, q_rope, k_nope, k_rope, value\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n        attention_mask: Optional[torch.Tensor],\n        past_key_value: Optional[Cache] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n        \"\"\"\n        hidden_states: [bsz, seq_len, hidden_size]\n        attention_mask: [bsz, seq_len]\n        \"\"\"\n        batch_size, seq_len, _ = hidden_states.size()\n\n        q_nope, q_rope, k_nope, k_rope, value_states = self._compute_qkv(hidden_states)\n\n        # cos, sin = self.rotary_emb(q_rope, seq_len=seq_len)\n        cos, sin = position_embeddings\n        q_rope, k_rope = apply_rotary_pos_emb(q_rope, k_rope, cos, sin)\n        query_states = torch.cat([q_nope, q_rope], dim=-1)\n        key_states = torch.cat([k_nope, k_rope], dim=-1)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        attention_interface: Callable = eager_attention_forward\n        if self.config._attn_implementation != \"eager\":\n            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n\n        attn_output, attn_weights = attention_interface(\n            self,\n            query_states,\n            key_states,\n            value_states,\n            attention_mask,\n            dropout=0.0 if not self.training else self.attention_dropout,\n            scaling=self.scaling,\n            **kwargs,\n        )\n\n        attn_output = attn_output.reshape(batch_size, seq_len, -1).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        return attn_output, attn_weights\n\n\nclass GigarDecoderLayer(nn.Module):\n    def __init__(self, config: GigarConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n\n        self.self_attn = GigarLatentAttention(config, layer_idx)\n        self.mlp = GigarMLP(config)\n        self.input_layernorm = GigarRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = GigarRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n        **kwargs: Unpack[FlashAttentionKwargs],\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n            position_embeddings=position_embeddings,\n            **kwargs,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        return outputs\n\n\nclass GigarRotaryEmbedding(nn.Module):\n    def __init__(self, config: GigarConfig, device=None):\n        super().__init__()\n        # BC: \"rope_type\" was originally \"type\"\n        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n        else:\n            self.rope_type = \"default\"\n        self.max_seq_len_cached = config.max_position_embeddings\n        self.original_max_seq_len = config.max_position_embeddings\n\n        self.config = config\n        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n\n        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        self.original_inv_freq = self.inv_freq\n\n    def _dynamic_frequency_update(self, position_ids, device):\n        \"\"\"\n        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n        1 - growing beyond the cached sequence length (allow scaling)\n        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n        \"\"\"\n        seq_len = torch.max(position_ids) + 1\n        if seq_len > self.max_seq_len_cached:  # growth\n            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n            self.max_seq_len_cached = seq_len\n\n        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n            self.max_seq_len_cached = self.original_max_seq_len\n\n    @torch.no_grad()\n    def forward(self, x, position_ids):\n        if \"dynamic\" in self.rope_type:\n            self._dynamic_frequency_update(position_ids, device=x.device)\n\n        # Core RoPE block\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n\n        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n        cos = cos * self.attention_scaling\n        sin = sin * self.attention_scaling\n\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\nGIGAR_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`GigarConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare Gigar Model outputting raw hidden-states without any specific head on top.\",\n    GIGAR_START_DOCSTRING,\n)\nclass GigarPreTrainedModel(PreTrainedModel):\n    config_class = GigarConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"GigarDecoderLayer\"]\n    _skip_keys_device_placement = [\"past_key_values\"]\n    _supports_flash_attn_2 = True\n    _supports_sdpa = True\n    _supports_flex_attn = True\n    _supports_cache_class = True\n    _supports_quantized_cache = True\n    _supports_static_cache = True\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n\nGIGAR_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.n_positions - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n\n            Two formats are allowed:\n            - a [`~cache_utils.Cache`] instance, see our\n            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n            cache format.\n\n            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n            legacy cache format will be returned.\n\n            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n            of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n            the complete sequence length.\n\"\"\"\n\n@add_start_docstrings(\n    \"The bare Gigar Model outputting raw hidden-states without any specific head on top.\",\n    GIGAR_START_DOCSTRING,\n)\nclass GigarModel(GigarPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`GigarDecoderLayer`]\n\n    Args:\n        config: GigarConfig\n    \"\"\"\n\n    def __init__(self, config: GigarConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList(\n            [GigarDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n        )\n        self.norm = GigarRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.rotary_emb = GigarRotaryEmbedding(config=config)\n        self.gradient_checkpointing = False\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(GIGAR_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Cache] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n\n        if self.gradient_checkpointing and self.training and use_cache:\n            logger.warning_once(\n                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n            )\n            use_cache = False\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        if use_cache and past_key_values is None:\n            past_key_values = DynamicCache()\n\n        if cache_position is None:\n            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n            cache_position = torch.arange(\n                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n            )\n\n        if position_ids is None:\n            position_ids = cache_position.unsqueeze(0)\n        \n        attention_mask = self._update_encoder_mask(attention_mask, inputs_embeds)\n\n        hidden_states = inputs_embeds\n\n        # create position embeddings to be shared across the decoder layers\n        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n\n        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    attention_mask,  # causal_mask\n                    position_ids,\n                    past_key_values,\n                    output_attentions,\n                    use_cache,\n                    cache_position,\n                    position_embeddings,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,  # causal_mask\n                    position_ids=position_ids,\n                    past_key_value=past_key_values,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    cache_position=cache_position,\n                    position_embeddings=position_embeddings,\n                    **flash_attn_kwargs,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        output = BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=past_key_values if use_cache else None,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )\n        return output if return_dict else output.to_tuple()\n    \n    def _update_encoder_mask(\n        self,\n        attention_mask: torch.Tensor,\n        input_tensor: torch.Tensor,\n    ):\n        # Для flash_attention_2 возвращаем исходную маску\n        if self.config._attn_implementation == \"flash_attention_2\":\n            if attention_mask is not None and (attention_mask == 0).any():\n                return attention_mask\n            return None\n\n        dtype, device = input_tensor.dtype, input_tensor.device\n        batch_size, sequence_length = input_tensor.shape[:2]\n\n        # 1. Создаём базовую маску без ограничений (все токены видят друг друга)\n        encoder_mask = torch.full(\n            (batch_size, 1, sequence_length, sequence_length),\n            fill_value=1.0,\n            dtype=dtype,\n            device=device\n        )\n\n        # 2. Применяем padding-маску если есть\n        if attention_mask is not None:\n            # Создаём 4D padding-маску [batch, 1, 1, seq_len]\n            padding_mask = attention_mask[:, None, None, :].to(dtype=dtype)\n\n            # Комбинируем: обнуляем позиции где padding_mask == 0\n            encoder_mask = encoder_mask * padding_mask\n\n            # Конвертируем в формат для softmax (0 = -inf)\n            min_dtype = torch.finfo(dtype).min\n            encoder_mask = encoder_mask.masked_fill(encoder_mask == 0.0, min_dtype)\n\n        return encoder_mask\n\n    def _update_causal_mask(\n        self,\n        attention_mask: torch.Tensor,\n        input_tensor: torch.Tensor,\n        cache_position: torch.Tensor,\n        past_key_values: Cache,\n        output_attentions: bool,\n    ):\n        if self.config._attn_implementation == \"flash_attention_2\":\n            if attention_mask is not None and (attention_mask == 0.0).any():\n                return attention_mask\n            return None\n\n        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n        # to infer the attention mask.\n        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n        using_static_cache = isinstance(past_key_values, StaticCache)\n\n        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                attention_mask,\n                inputs_embeds=input_tensor,\n                past_key_values_length=past_seen_tokens,\n                is_training=self.training,\n            ):\n                return None\n\n        dtype, device = input_tensor.dtype, input_tensor.device\n        sequence_length = input_tensor.shape[1]\n        if using_static_cache:\n            target_length = past_key_values.get_max_cache_shape()\n        else:\n            target_length = (\n                attention_mask.shape[-1]\n                if isinstance(attention_mask, torch.Tensor)\n                else past_seen_tokens + sequence_length + 1\n            )\n\n        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n            attention_mask,\n            sequence_length=sequence_length,\n            target_length=target_length,\n            dtype=dtype,\n            device=device,\n            cache_position=cache_position,\n            batch_size=input_tensor.shape[0],\n        )\n\n        if (\n            self.config._attn_implementation == \"sdpa\"\n            and attention_mask is not None\n            and attention_mask.device.type == \"cuda\"\n            and not output_attentions\n        ):\n            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n            # Details: https://github.com/pytorch/pytorch/issues/110213\n            min_dtype = torch.finfo(dtype).min\n            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n\n        return causal_mask\n\n    @staticmethod\n    def _prepare_4d_causal_attention_mask_with_cache_position(\n        attention_mask: torch.Tensor,\n        sequence_length: int,\n        target_length: int,\n        dtype: torch.dtype,\n        device: torch.device,\n        cache_position: torch.Tensor,\n        batch_size: int,\n        **kwargs,\n    ):\n        \"\"\"\n        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n\n        Args:\n            attention_mask (`torch.Tensor`):\n                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n                `(batch_size, 1, query_length, key_value_length)`.\n            sequence_length (`int`):\n                The sequence length being processed.\n            target_length (`int`):\n                The target length: when generating with static cache, the mask should be as long as the static cache,\n                to account for the 0 padding, the part of the cache that is not filled yet.\n            dtype (`torch.dtype`):\n                The dtype to use for the 4D attention mask.\n            device (`torch.device`):\n                The device to plcae the 4D attention mask on.\n            cache_position (`torch.Tensor`):\n                Indices depicting the position of the input sequence tokens in the sequence.\n            batch_size (`torch.Tensor`):\n                Batch size.\n        \"\"\"\n        if attention_mask is not None and attention_mask.dim() == 4:\n            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n            causal_mask = attention_mask\n        else:\n            min_dtype = torch.finfo(dtype).min\n            causal_mask = torch.full(\n                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n            )\n            if sequence_length != 1:\n                causal_mask = torch.triu(causal_mask, diagonal=1)\n            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n            if attention_mask is not None:\n                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                mask_length = attention_mask.shape[-1]\n                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n                padding_mask = padding_mask == 0\n                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                    padding_mask, min_dtype\n                )\n\n        return causal_mask\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, mult = 4):\n        super().__init__()\n        self.hidden_size = dim\n        self.intermediate_size = dim * mult\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n        self.act_fn = nn.SiLU()\n\n    def forward(self, x):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass Attention(nn.Module):\n    def __init__(self, query_dimension, context_dimension=None, num_heads=8, head_dim=64):\n        super().__init__()\n        inner_dimension = head_dim * num_heads\n        context_dimension = context_dimension if context_dimension is not None else query_dimension\n\n        self.scaling_factor = head_dim ** -0.5\n        self.num_heads = num_heads\n\n        self.to_q = nn.Linear(query_dimension, inner_dimension, bias=False)\n        self.to_kv = nn.Linear(context_dimension, inner_dimension * 2, bias=False)\n        self.to_out = nn.Linear(inner_dimension, query_dimension, bias=False)\n\n    def forward(self, input_tensor, context=None, attention_mask=None):\n        batch_size, seq_len, _ = input_tensor.shape\n        num_heads = self.num_heads\n\n        # Project input to query\n        query = self.to_q(input_tensor)\n\n        # Use input as context if not provided\n        context = input_tensor if context is None else context\n        key, value = self.to_kv(context).chunk(2, dim=-1)\n\n        # Rearrange for multi-head attention\n        query = rearrange(query, 'b n (h d) -> (b h) n d', h=num_heads)\n        key = rearrange(key, 'b n (h d) -> (b h) n d', h=num_heads)\n        value = rearrange(value, 'b n (h d) -> (b h) n d', h=num_heads)\n\n        # Compute scaled dot-product attention\n        with torch.backends.cuda.sdp_kernel(\n            enable_flash=True, \n            enable_math=True, \n            enable_mem_efficient=True\n        ):\n            attention_output = F.scaled_dot_product_attention(query, key, value)\n\n        # Rearrange back to original shape\n        attention_output = rearrange(attention_output, '(b h) n d -> b n (h d)', h=num_heads)\n\n        return self.to_out(attention_output)\n\n\nclass LatentAttentionModel(PreTrainedModel):\n    config_class = LatentAttentionConfig\n\n    def __init__(self, configuration: LatentAttentionConfig):\n        super().__init__(configuration)\n\n        # Extract configuration parameters\n        num_latents = configuration.num_latents_value\n        latent_dimension = configuration.latent_dim\n        cross_attention_heads = configuration.num_cross_heads\n        cross_head_dimension = configuration.cross_dim_head\n        hidden_dimension = configuration.hidden_dim\n\n        # Initialize cross-attention components\n        self.cross_attend_blocks = nn.ModuleList([\n            Attention(\n                query_dimension=latent_dimension,\n                context_dimension=hidden_dimension,\n                num_heads=cross_attention_heads,\n                head_dim=cross_head_dimension\n            ),\n            FeedForward(latent_dimension)\n        ])\n\n        # Register learnable latents as model parameter\n        self.latents = nn.Parameter(torch.randn(num_latents, latent_dimension))\n\n    def forward(self, hidden_states, attention_mask: Optional[torch.Tensor] = None):\n        cross_attention, feed_forward = self.cross_attend_blocks\n        \n        batch_size, device = hidden_states.size(0), hidden_states.device\n        \n        # Expand latents to match batch size\n        expanded_latents = self.latents.repeat(batch_size, 1, 1)\n        \n        # Apply cross-attention with residual connection\n        attended_output = cross_attention(\n            hidden_states, context=expanded_latents, attention_mask=attention_mask) + hidden_states\n        \n        # Apply feed-forward with residual connection\n        processed_output = feed_forward(attended_output) + attended_output\n        \n        return processed_output\n\n\nclass GigarEmbedModel(PreTrainedModel):\n    config_class = GigarEmbedConfig\n    _supports_flash_attn_2 = True\n    _no_split_modules = [\"GigarDecoderLayer\", \"LatentAttentionModel\"]\n    \n    def __init__(self, configuration: GigarEmbedConfig):\n        super().__init__(configuration)\n        \n        # Initialize latent attention model\n        self.latent_attention_model = AutoModel.from_config(\n            configuration.latent_attention_config\n        )\n\n        self.tokenizer, self.text_encoder = None, None\n        if configuration.text_config is not None:\n            # Initialize text model if provided in config\n            self.model = AutoModel.from_config(configuration.text_config)\n\n            # Initialize tokenizer if text config is available\n            self.tokenizer = AutoTokenizer.from_pretrained(\n                configuration.name_or_path\n            )\n        \n        # Set configuration parameters\n        self.padding_side = configuration.padding_side\n        self.add_eos = configuration.add_eos\n        self.mask_type = configuration.mask_type\n        \n        # Add padding token if configured\n        if configuration.add_pad_token and self.tokenizer is not None:\n            self.add_pad_token()\n\n    def add_pad_token(self):\n        self.tokenizer.pad_token_id = 0\n        self.tokenizer.padding_side = self.padding_side\n\n    def gradient_checkpointing_enable(self, *args, **kwargs):\n        self.model.gradient_checkpointing_enable(*args, **kwargs)\n\n    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, \n                return_embeddings: bool = False, **kwargs):\n        kwargs.pop('token_type_ids', None)\n\n        with torch.autocast('cuda', dtype=torch.bfloat16):\n            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n\n            last_hidden = self.latent_attention_model(outputs.last_hidden_state, attention_mask)\n\n        if return_embeddings:\n            return self.mean_pool(last_hidden, attention_mask)\n\n        return BaseModelOutputWithPast(last_hidden_state=last_hidden)\n\n    def mean_pool(self, last_hidden: torch.Tensor, attention_mask: torch.Tensor):\n        last_hidden = last_hidden.masked_fill(~attention_mask[..., None].bool(), 0.0)\n        embeddings = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n        return F.normalize(embeddings, p=2, dim=-1)\n\n\n## AutoModel Register\nAutoModel.register(GigarConfig, GigarModel)\nAutoModel.register(GigarEmbedConfig, GigarEmbedModel)\nAutoModel.register(LatentAttentionConfig, LatentAttentionModel)\n\n## Register for auto class\nGigarModel.register_for_auto_class(\"AutoModel\")\nGigarEmbedModel.register_for_auto_class(\"AutoModel\")\nLatentAttentionModel.register_for_auto_class(\"AutoModel\")\n"
    },
    {
      "path": "weights/Giga-Embeddings-instruct/modules.json",
      "language": "json",
      "size_bytes": 349,
      "sha256": "84e40c8e006c9b1d6c122e02cba9b02458120b5fb0c87b746c41e0207cf642cf",
      "content": "[\n  {\n    \"idx\": 0,\n    \"name\": \"0\",\n    \"path\": \"\",\n    \"type\": \"sentence_transformers.models.Transformer\"\n  },\n  {\n    \"idx\": 1,\n    \"name\": \"1\",\n    \"path\": \"1_Pooling\",\n    \"type\": \"sentence_transformers.models.Pooling\"\n  },\n  {\n    \"idx\": 2,\n    \"name\": \"2\",\n    \"path\": \"2_Normalize\",\n    \"type\": \"sentence_transformers.models.Normalize\"\n  }\n]"
    },
    {
      "path": "weights/Giga-Embeddings-instruct/sentence_bert_config.json",
      "language": "json",
      "size_bytes": 58,
      "sha256": "82236913f81b129ce9a63581f29676c37ec71d7ba2ab1839f24dc03827e14aab",
      "content": "{\n    \"max_seq_length\": 4096,\n    \"do_lower_case\": false\n}"
    },
    {
      "path": "weights/Giga-Embeddings-instruct/special_tokens_map.json",
      "language": "json",
      "size_bytes": 690,
      "sha256": "7df9ebc543a0f1a8ac54fd979387f97e324f99c8339da718243fd58889ee8060",
      "content": "{\n  \"bos_token\": {\n    \"content\": \"<s>\",\n    \"lstrip\": false,\n    \"normalized\": false,\n    \"rstrip\": false,\n    \"single_word\": false\n  },\n  \"eos_token\": {\n    \"content\": \"</s>\",\n    \"lstrip\": false,\n    \"normalized\": false,\n    \"rstrip\": false,\n    \"single_word\": false\n  },\n  \"pad_token\": {\n    \"content\": \"<unk>\",\n    \"lstrip\": false,\n    \"normalized\": false,\n    \"rstrip\": false,\n    \"single_word\": false\n  },\n  \"sep_token\": {\n    \"content\": \"<unk>\",\n    \"lstrip\": false,\n    \"normalized\": false,\n    \"rstrip\": false,\n    \"single_word\": false\n  },\n  \"unk_token\": {\n    \"content\": \"<unk>\",\n    \"lstrip\": false,\n    \"normalized\": false,\n    \"rstrip\": false,\n    \"single_word\": false\n  }\n}\n"
    },
    {
      "path": "weights/Giga-Embeddings-instruct/tokenizer_config.json",
      "language": "json",
      "size_bytes": 48144,
      "sha256": "3e0e79212f9ff591db2fb29e3820c8fc19d585232274c77dab496e1ba98eaf4c",
      "content": "{\n  \"added_tokens_decoder\": {\n    \"0\": {\n      \"content\": \"<unk>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"1\": {\n      \"content\": \"<s>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"2\": {\n      \"content\": \"</s>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128000\": {\n      \"content\": \"<|gigatoken_1|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128001\": {\n      \"content\": \"<|gigatoken_2|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128002\": {\n      \"content\": \"<|gigatoken_3|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128003\": {\n      \"content\": \"<|gigatoken_4|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128004\": {\n      \"content\": \"<|gigatoken_5|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128005\": {\n      \"content\": \"<|gigatoken_6|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128006\": {\n      \"content\": \"<|gigatoken_7|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128007\": {\n      \"content\": \"<|gigatoken_8|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128008\": {\n      \"content\": \"<|gigatoken_9|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128009\": {\n      \"content\": \"<|gigatoken_10|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128010\": {\n      \"content\": \"<|gigatoken_11|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128011\": {\n      \"content\": \"<|gigatoken_12|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128012\": {\n      \"content\": \"<|gigatoken_13|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128013\": {\n      \"content\": \"<|gigatoken_14|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128014\": {\n      \"content\": \"<|gigatoken_15|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128015\": {\n      \"content\": \"<|gigatoken_16|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128016\": {\n      \"content\": \"<|gigatoken_17|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128017\": {\n      \"content\": \"<|gigatoken_18|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128018\": {\n      \"content\": \"<|gigatoken_19|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128019\": {\n      \"content\": \"<|gigatoken_20|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128020\": {\n      \"content\": \"<|gigatoken_21|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128021\": {\n      \"content\": \"<|gigatoken_22|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128022\": {\n      \"content\": \"<|gigatoken_23|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128023\": {\n      \"content\": \"<|gigatoken_24|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128024\": {\n      \"content\": \"<|gigatoken_25|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128025\": {\n      \"content\": \"<|gigatoken_26|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128026\": {\n      \"content\": \"<|gigatoken_27|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128027\": {\n      \"content\": \"<|gigatoken_28|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128028\": {\n      \"content\": \"<|gigatoken_29|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128029\": {\n      \"content\": \"<|gigatoken_30|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128030\": {\n      \"content\": \"<|gigatoken_31|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128031\": {\n      \"content\": \"<|gigatoken_32|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128032\": {\n      \"content\": \"<|gigatoken_33|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128033\": {\n      \"content\": \"<|gigatoken_34|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128034\": {\n      \"content\": \"<|gigatoken_35|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128035\": {\n      \"content\": \"<|gigatoken_36|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128036\": {\n      \"content\": \"<|gigatoken_37|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128037\": {\n      \"content\": \"<|gigatoken_38|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128038\": {\n      \"content\": \"<|gigatoken_39|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128039\": {\n      \"content\": \"<|gigatoken_40|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128040\": {\n      \"content\": \"<|gigatoken_41|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128041\": {\n      \"content\": \"<|gigatoken_42|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128042\": {\n      \"content\": \"<|gigatoken_43|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128043\": {\n      \"content\": \"<|gigatoken_44|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128044\": {\n      \"content\": \"<|gigatoken_45|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128045\": {\n      \"content\": \"<|gigatoken_46|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128046\": {\n      \"content\": \"<|gigatoken_47|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128047\": {\n      \"content\": \"<|gigatoken_48|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128048\": {\n      \"content\": \"<|gigatoken_49|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128049\": {\n      \"content\": \"<|gigatoken_50|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128050\": {\n      \"content\": \"<|gigatoken_51|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128051\": {\n      \"content\": \"<|gigatoken_52|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128052\": {\n      \"content\": \"<|gigatoken_53|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128053\": {\n      \"content\": \"<|gigatoken_54|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128054\": {\n      \"content\": \"<|gigatoken_55|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128055\": {\n      \"content\": \"<|gigatoken_56|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128056\": {\n      \"content\": \"<|gigatoken_57|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128057\": {\n      \"content\": \"<|gigatoken_58|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128058\": {\n      \"content\": \"<|gigatoken_59|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128059\": {\n      \"content\": \"<|gigatoken_60|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128060\": {\n      \"content\": \"<|gigatoken_61|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128061\": {\n      \"content\": \"<|gigatoken_62|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128062\": {\n      \"content\": \"<|gigatoken_63|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128063\": {\n      \"content\": \"<|gigatoken_64|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128064\": {\n      \"content\": \"<|gigatoken_65|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128065\": {\n      \"content\": \"<|gigatoken_66|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128066\": {\n      \"content\": \"<|gigatoken_67|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128067\": {\n      \"content\": \"<|gigatoken_68|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128068\": {\n      \"content\": \"<|gigatoken_69|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128069\": {\n      \"content\": \"<|gigatoken_70|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128070\": {\n      \"content\": \"<|gigatoken_71|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128071\": {\n      \"content\": \"<|gigatoken_72|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128072\": {\n      \"content\": \"<|gigatoken_73|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128073\": {\n      \"content\": \"<|gigatoken_74|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128074\": {\n      \"content\": \"<|gigatoken_75|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128075\": {\n      \"content\": \"<|gigatoken_76|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128076\": {\n      \"content\": \"<|gigatoken_77|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128077\": {\n      \"content\": \"<|gigatoken_78|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128078\": {\n      \"content\": \"<|gigatoken_79|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128079\": {\n      \"content\": \"<|gigatoken_80|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128080\": {\n      \"content\": \"<|gigatoken_81|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128081\": {\n      \"content\": \"<|gigatoken_82|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128082\": {\n      \"content\": \"<|gigatoken_83|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128083\": {\n      \"content\": \"<|gigatoken_84|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128084\": {\n      \"content\": \"<|gigatoken_85|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128085\": {\n      \"content\": \"<|gigatoken_86|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128086\": {\n      \"content\": \"<|gigatoken_87|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128087\": {\n      \"content\": \"<|gigatoken_88|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128088\": {\n      \"content\": \"<|gigatoken_89|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128089\": {\n      \"content\": \"<|gigatoken_90|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128090\": {\n      \"content\": \"<|gigatoken_91|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128091\": {\n      \"content\": \"<|gigatoken_92|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128092\": {\n      \"content\": \"<|gigatoken_93|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128093\": {\n      \"content\": \"<|gigatoken_94|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128094\": {\n      \"content\": \"<|gigatoken_95|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128095\": {\n      \"content\": \"<|gigatoken_96|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128096\": {\n      \"content\": \"<|gigatoken_97|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128097\": {\n      \"content\": \"<|gigatoken_98|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128098\": {\n      \"content\": \"<|gigatoken_99|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128099\": {\n      \"content\": \"<|gigatoken_100|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128100\": {\n      \"content\": \"<|gigatoken_101|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128101\": {\n      \"content\": \"<|gigatoken_102|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128102\": {\n      \"content\": \"<|gigatoken_103|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128103\": {\n      \"content\": \"<|gigatoken_104|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128104\": {\n      \"content\": \"<|gigatoken_105|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128105\": {\n      \"content\": \"<|gigatoken_106|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128106\": {\n      \"content\": \"<|gigatoken_107|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128107\": {\n      \"content\": \"<|gigatoken_108|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128108\": {\n      \"content\": \"<|gigatoken_109|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128109\": {\n      \"content\": \"<|gigatoken_110|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128110\": {\n      \"content\": \"<|gigatoken_111|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128111\": {\n      \"content\": \"<|gigatoken_112|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128112\": {\n      \"content\": \"<|gigatoken_113|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128113\": {\n      \"content\": \"<|gigatoken_114|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128114\": {\n      \"content\": \"<|gigatoken_115|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128115\": {\n      \"content\": \"<|gigatoken_116|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128116\": {\n      \"content\": \"<|gigatoken_117|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128117\": {\n      \"content\": \"<|gigatoken_118|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128118\": {\n      \"content\": \"<|gigatoken_119|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128119\": {\n      \"content\": \"<|gigatoken_120|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128120\": {\n      \"content\": \"<|gigatoken_121|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128121\": {\n      \"content\": \"<|gigatoken_122|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128122\": {\n      \"content\": \"<|gigatoken_123|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128123\": {\n      \"content\": \"<|gigatoken_124|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128124\": {\n      \"content\": \"<|gigatoken_125|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128125\": {\n      \"content\": \"<|gigatoken_126|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128126\": {\n      \"content\": \"<|gigatoken_127|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128127\": {\n      \"content\": \"<|gigatoken_128|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128128\": {\n      \"content\": \"<|gigatoken_129|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128129\": {\n      \"content\": \"<|gigatoken_130|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128130\": {\n      \"content\": \"<|gigatoken_131|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128131\": {\n      \"content\": \"<|gigatoken_132|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128132\": {\n      \"content\": \"<|gigatoken_133|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128133\": {\n      \"content\": \"<|gigatoken_134|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128134\": {\n      \"content\": \"<|gigatoken_135|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128135\": {\n      \"content\": \"<|gigatoken_136|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128136\": {\n      \"content\": \"<|gigatoken_137|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128137\": {\n      \"content\": \"<|gigatoken_138|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128138\": {\n      \"content\": \"<|gigatoken_139|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128139\": {\n      \"content\": \"<|gigatoken_140|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128140\": {\n      \"content\": \"<|gigatoken_141|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128141\": {\n      \"content\": \"<|gigatoken_142|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128142\": {\n      \"content\": \"<|gigatoken_143|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128143\": {\n      \"content\": \"<|gigatoken_144|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128144\": {\n      \"content\": \"<|gigatoken_145|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128145\": {\n      \"content\": \"<|gigatoken_146|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128146\": {\n      \"content\": \"<|gigatoken_147|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128147\": {\n      \"content\": \"<|gigatoken_148|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128148\": {\n      \"content\": \"<|gigatoken_149|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128149\": {\n      \"content\": \"<|gigatoken_150|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128150\": {\n      \"content\": \"<|gigatoken_151|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128151\": {\n      \"content\": \"<|gigatoken_152|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128152\": {\n      \"content\": \"<|gigatoken_153|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128153\": {\n      \"content\": \"<|gigatoken_154|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128154\": {\n      \"content\": \"<|gigatoken_155|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128155\": {\n      \"content\": \"<|gigatoken_156|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128156\": {\n      \"content\": \"<|gigatoken_157|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128157\": {\n      \"content\": \"<|gigatoken_158|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128158\": {\n      \"content\": \"<|gigatoken_159|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128159\": {\n      \"content\": \"<|gigatoken_160|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128160\": {\n      \"content\": \"<|gigatoken_161|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128161\": {\n      \"content\": \"<|gigatoken_162|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128162\": {\n      \"content\": \"<|gigatoken_163|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128163\": {\n      \"content\": \"<|gigatoken_164|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128164\": {\n      \"content\": \"<|gigatoken_165|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128165\": {\n      \"content\": \"<|gigatoken_166|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128166\": {\n      \"content\": \"<|gigatoken_167|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128167\": {\n      \"content\": \"<|gigatoken_168|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128168\": {\n      \"content\": \"<|gigatoken_169|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128169\": {\n      \"content\": \"<|gigatoken_170|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128170\": {\n      \"content\": \"<|gigatoken_171|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128171\": {\n      \"content\": \"<|gigatoken_172|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128172\": {\n      \"content\": \"<|gigatoken_173|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128173\": {\n      \"content\": \"<|gigatoken_174|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128174\": {\n      \"content\": \"<|gigatoken_175|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128175\": {\n      \"content\": \"<|gigatoken_176|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128176\": {\n      \"content\": \"<|gigatoken_177|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128177\": {\n      \"content\": \"<|gigatoken_178|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128178\": {\n      \"content\": \"<|gigatoken_179|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128179\": {\n      \"content\": \"<|gigatoken_180|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128180\": {\n      \"content\": \"<|gigatoken_181|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128181\": {\n      \"content\": \"<|gigatoken_182|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128182\": {\n      \"content\": \"<|gigatoken_183|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128183\": {\n      \"content\": \"<|gigatoken_184|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128184\": {\n      \"content\": \"<|gigatoken_185|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128185\": {\n      \"content\": \"<|gigatoken_186|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128186\": {\n      \"content\": \"<|gigatoken_187|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128187\": {\n      \"content\": \"<|gigatoken_188|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128188\": {\n      \"content\": \"<|gigatoken_189|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128189\": {\n      \"content\": \"<|gigatoken_190|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128190\": {\n      \"content\": \"<|gigatoken_191|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128191\": {\n      \"content\": \"<|gigatoken_192|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128192\": {\n      \"content\": \"<|gigatoken_193|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128193\": {\n      \"content\": \"<|gigatoken_194|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128194\": {\n      \"content\": \"<|gigatoken_195|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128195\": {\n      \"content\": \"<|gigatoken_196|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128196\": {\n      \"content\": \"<|gigatoken_197|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128197\": {\n      \"content\": \"<|gigatoken_198|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128198\": {\n      \"content\": \"<|gigatoken_199|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128199\": {\n      \"content\": \"<|gigatoken_200|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128200\": {\n      \"content\": \"<|gigatoken_201|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128201\": {\n      \"content\": \"<|gigatoken_202|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128202\": {\n      \"content\": \"<|gigatoken_203|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128203\": {\n      \"content\": \"<|gigatoken_204|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128204\": {\n      \"content\": \"<|gigatoken_205|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128205\": {\n      \"content\": \"<|gigatoken_206|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128206\": {\n      \"content\": \"<|gigatoken_207|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128207\": {\n      \"content\": \"<|gigatoken_208|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128208\": {\n      \"content\": \"<|gigatoken_209|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128209\": {\n      \"content\": \"<|gigatoken_210|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128210\": {\n      \"content\": \"<|gigatoken_211|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128211\": {\n      \"content\": \"<|gigatoken_212|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128212\": {\n      \"content\": \"<|gigatoken_213|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128213\": {\n      \"content\": \"<|gigatoken_214|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128214\": {\n      \"content\": \"<|gigatoken_215|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128215\": {\n      \"content\": \"<|gigatoken_216|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128216\": {\n      \"content\": \"<|gigatoken_217|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128217\": {\n      \"content\": \"<|gigatoken_218|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128218\": {\n      \"content\": \"<|gigatoken_219|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128219\": {\n      \"content\": \"<|gigatoken_220|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128220\": {\n      \"content\": \"<|gigatoken_221|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128221\": {\n      \"content\": \"<|gigatoken_222|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128222\": {\n      \"content\": \"<|gigatoken_223|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128223\": {\n      \"content\": \"<|gigatoken_224|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128224\": {\n      \"content\": \"<|gigatoken_225|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128225\": {\n      \"content\": \"<|gigatoken_226|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128226\": {\n      \"content\": \"<|gigatoken_227|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128227\": {\n      \"content\": \"<|gigatoken_228|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128228\": {\n      \"content\": \"<|gigatoken_229|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128229\": {\n      \"content\": \"<|gigatoken_230|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128230\": {\n      \"content\": \"<|gigatoken_231|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128231\": {\n      \"content\": \"<|gigatoken_232|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128232\": {\n      \"content\": \"<|gigatoken_233|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128233\": {\n      \"content\": \"<|gigatoken_234|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128234\": {\n      \"content\": \"<|gigatoken_235|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128235\": {\n      \"content\": \"<|gigatoken_236|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128236\": {\n      \"content\": \"<|gigatoken_237|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128237\": {\n      \"content\": \"<|gigatoken_238|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128238\": {\n      \"content\": \"<|gigatoken_239|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128239\": {\n      \"content\": \"<|gigatoken_240|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128240\": {\n      \"content\": \"<|gigatoken_241|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128241\": {\n      \"content\": \"<|gigatoken_242|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128242\": {\n      \"content\": \"<|gigatoken_243|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128243\": {\n      \"content\": \"<|gigatoken_244|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128244\": {\n      \"content\": \"<|gigatoken_245|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128245\": {\n      \"content\": \"<|gigatoken_246|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128246\": {\n      \"content\": \"<|gigatoken_247|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128247\": {\n      \"content\": \"<|gigatoken_248|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128248\": {\n      \"content\": \"<|gigatoken_249|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128249\": {\n      \"content\": \"<|gigatoken_250|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128250\": {\n      \"content\": \"<|gigatoken_251|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128251\": {\n      \"content\": \"<|gigatoken_252|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128252\": {\n      \"content\": \"<|gigatoken_253|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128253\": {\n      \"content\": \"<|gigatoken_254|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128254\": {\n      \"content\": \"<|gigatoken_255|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    },\n    \"128255\": {\n      \"content\": \"<|gigatoken_256|>\",\n      \"lstrip\": false,\n      \"normalized\": false,\n      \"rstrip\": false,\n      \"single_word\": false,\n      \"special\": true\n    }\n  },\n  \"bos_token\": \"<s>\",\n  \"chat_template\": \"{%- for message in messages -%}{{ message['content'] }}{%- if not loop.last -%}{{ ' ' }}{%- endif -%}{%- endfor -%}\",\n  \"clean_up_tokenization_spaces\": true,\n  \"eos_token\": \"</s>\",\n  \"extra_special_tokens\": {},\n  \"max_length\": 512,\n  \"model_max_length\": 1000000000000000019884624838656,\n  \"pad_to_multiple_of\": null,\n  \"pad_token\": \"<unk>\",\n  \"pad_token_type_id\": 0,\n  \"padding_side\": \"right\",\n  \"sep_token\": \"<unk>\",\n  \"stride\": 0,\n  \"tokenizer_class\": \"PreTrainedTokenizer\",\n  \"truncation_side\": \"right\",\n  \"truncation_strategy\": \"longest_first\",\n  \"unk_token\": \"<unk>\"\n}\n"
    },
    {
      "path": "weights/hf-cache/modules/__init__.py",
      "language": "python",
      "size_bytes": 0,
      "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
      "content": ""
    },
    {
      "path": "weights/hf-cache/modules/transformers_modules/Giga_hyphen_Embeddings_hyphen_instruct/__init__.py",
      "language": "python",
      "size_bytes": 0,
      "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
      "content": ""
    },
    {
      "path": "weights/hf-cache/modules/transformers_modules/Giga_hyphen_Embeddings_hyphen_instruct/configuration_gigarembed.py",
      "language": "python",
      "size_bytes": 14426,
      "sha256": "0b34786d1c8ee25c2df02d427657a89b2bfad4c4cf8f4858f2007bb223668962",
      "content": "import warnings\n\nfrom typing import Literal\nfrom transformers import AutoConfig\nfrom transformers.models.auto import CONFIG_MAPPING\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.modeling_rope_utils import rope_config_validation\n\nGIGAREMBED_TYPE = \"gigarembed\"\nLATENT_ATTENTION_TYPE = \"latent_attention\"\n\n\nclass GigarConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`GigarModel`]. It is used to instantiate an Gigar\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the Gigar-7B.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 32000):\n            Vocabulary size of the Gigar model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`GigarModel`]\n        hidden_size (`int`, *optional*, defaults to 4096):\n            Dimension of the hidden representations.\n        intermediate_size (`int`, *optional*, defaults to 11008):\n            Dimension of the MLP representations.\n        num_hidden_layers (`int`, *optional*, defaults to 32):\n            Number of hidden layers in the Transformer decoder.\n        num_attention_heads (`int`, *optional*, defaults to 32):\n            Number of attention heads for each attention layer in the Transformer decoder.\n        num_key_value_heads (`int`, *optional*):\n            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n            by meanpooling all the original heads within that group. For more details checkout [this\n            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n            `num_attention_heads`.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n            The non-linear activation function (function or string) in the decoder.\n        max_position_embeddings (`int`, *optional*, defaults to 2048):\n            The maximum sequence length that this model might ever be used with. Gigar 1 supports up to 2048 tokens,\n            Gigar 2 up to 4096, CodeLlama up to 16384.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n            The epsilon used by the rms normalization layers.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        pad_token_id (`int`, *optional*):\n            Padding token id.\n        bos_token_id (`int`, *optional*, defaults to 1):\n            Beginning of stream token id.\n        eos_token_id (`int`, *optional*, defaults to 2):\n            End of stream token id.\n        pretraining_tp (`int`, *optional*, defaults to 1):\n            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to\n            understand more about it. This value is necessary to ensure exact reproducibility of the pretraining\n            results. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/76232).\n        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n            Whether to tie weight embeddings\n        rope_theta (`float`, *optional*, defaults to 10000.0):\n            The base period of the RoPE embeddings.\n        rope_scaling (`Dict`, *optional*):\n            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n            accordingly.\n            Expected contents:\n                `rope_type` (`str`):\n                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n                    'gigar3'], with 'default' being the original RoPE implementation.\n                `factor` (`float`, *optional*):\n                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n                    original maximum pre-trained length.\n                `original_max_position_embeddings` (`int`, *optional*):\n                    Used with 'dynamic', 'longrope' and 'gigar3'. The original max position embeddings used during\n                    pretraining.\n                `attention_factor` (`float`, *optional*):\n                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n                    `factor` field to infer the suggested value.\n                `beta_fast` (`float`, *optional*):\n                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n                    ramp function. If unspecified, it defaults to 32.\n                `beta_slow` (`float`, *optional*):\n                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n                    ramp function. If unspecified, it defaults to 1.\n                `short_factor` (`List[float]`, *optional*):\n                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n                    size divided by the number of attention heads divided by 2\n                `long_factor` (`List[float]`, *optional*):\n                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n                    size divided by the number of attention heads divided by 2\n                `low_freq_factor` (`float`, *optional*):\n                    Only used with 'gigar3'. Scaling factor applied to low frequency components of the RoPE\n                `high_freq_factor` (`float`, *optional*):\n                    Only used with 'gigar3'. Scaling factor applied to high frequency components of the RoPE\n        attention_bias (`bool`, *optional*, defaults to `False`):\n            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n        attention_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for the attention probabilities.\n        mlp_bias (`bool`, *optional*, defaults to `False`):\n            Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n        head_dim (`int`, *optional*):\n            The attention head dimension. If None, it will default to hidden_size // num_attention_heads\n\n    ```python\n    >>> from transformers import GigarModel, GigarConfig\n\n    >>> # Initializing a Gigar gigar-7b style configuration\n    >>> configuration = GigarConfig()\n\n    >>> # Initializing a model from the gigar-7b style configuration\n    >>> model = GigarModel(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n\n    model_type = \"gigar\"\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n    # Default tensor parallel plan for base model `GigarModel`\n    base_model_tp_plan = {\n        \"layers.*.self_attn.q_proj\": \"colwise\",\n        \"layers.*.self_attn.k_proj\": \"colwise\",\n        \"layers.*.self_attn.v_proj\": \"colwise\",\n        \"layers.*.self_attn.o_proj\": \"rowwise\",\n        \"layers.*.mlp.gate_proj\": \"colwise\",\n        \"layers.*.mlp.up_proj\": \"colwise\",\n        \"layers.*.mlp.down_proj\": \"rowwise\",\n    }\n\n    def __init__(\n        self,\n        vocab_size=32000,\n        hidden_size=4096,\n        intermediate_size=11008,\n        num_hidden_layers=32,\n        num_attention_heads=32,\n        num_key_value_heads=None,\n        hidden_act=\"silu\",\n        max_position_embeddings=2048,\n        initializer_range=0.02,\n        rms_norm_eps=1e-6,\n        use_cache=True,\n        pad_token_id=None,\n        bos_token_id=1,\n        eos_token_id=2,\n        pretraining_tp=1,\n        tie_word_embeddings=False,\n        rope_theta=10000.0,\n        rope_scaling=None,\n        attention_bias=False,\n        attention_dropout=0.0,\n        mlp_bias=False,\n        head_dim=None,\n        apply_qk_norm=False,\n        mla_config=None,\n        **kwargs,\n    ):\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.pretraining_tp = pretraining_tp\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.rope_scaling = rope_scaling\n        self.attention_bias = attention_bias\n        self.attention_dropout = attention_dropout\n        self.mlp_bias = mlp_bias\n        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n        # Validate the correctness of rotary position embeddings parameters\n        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n        rope_config_validation(self)\n\n        self.apply_qk_norm = apply_qk_norm\n        self.mla_config = mla_config\n\n        self._validate_mla_config()\n\n    def _validate_mla_config(self):\n        if self.mla_config is None:\n            warnings.warn(\"MLA config is None!\")\n            return\n\n        EXPECTED_KEYS = [\n            \"qk_nope_head_dim\",\n            \"qk_rope_head_dim\",\n            \"v_head_dim\",\n            \"kv_lora_rank\",\n            \"q_lora_rank\",\n        ]\n        if not all((key in self.mla_config for key in EXPECTED_KEYS)):\n            raise ValueError(\n                f\"MLA config is expected to have the following keys {EXPECTED_KEYS} but got {self.mla_config.keys()}.\"\n            )\n\n        if self.mla_config[\"qk_nope_head_dim\"] + self.mla_config[\"qk_rope_head_dim\"] != self.mla_config[\"v_head_dim\"]:\n            err_msg = (\n                f\"QK and V head dims do not match! Got {self.mla_config['qk_nope_head_dim']} + {self.mla_config['qk_rope_head_dim']} \"\n                f\"= {self.mla_config['qk_rope_head_dim'] + self.mla_config['qk_nope_head_dim']} and {self.mla_config['v_head_dim']}.\"\n            )\n            raise ValueError(err_msg)\n\n\nclass GigarEmbedConfig(PretrainedConfig):\n    model_type = \"gigarembed\"\n    is_composition = False\n\n    def __init__(\n        self,\n        latent_attention_config=None,\n        text_config=None,\n        padding_side: Literal[\"right\", \"left\"]=\"right\",\n        add_pad_token: bool=True,\n        is_mask_instruction: bool = True,\n        add_eos: bool=True,\n        mask_type: str=\"b\",\n        **kwargs,\n    ):\n        if isinstance(latent_attention_config, dict):\n            latent_attention_config[\"model_type\"] = (\n                latent_attention_config[\"model_type\"] if \"model_type\" in latent_attention_config else LATENT_ATTENTION_TYPE\n            )\n            latent_attention_config = CONFIG_MAPPING[latent_attention_config[\"model_type\"]](**latent_attention_config)\n\n        self.latent_attention_config = latent_attention_config\n\n        if isinstance(text_config, dict):\n            text_config = GigarConfig(**text_config)\n        elif text_config is None:\n            text_config = None\n\n        self.text_config = text_config\n        self.padding_side = padding_side\n        self.is_mask_instruction = is_mask_instruction\n        self.add_pad_token = add_pad_token\n        self.add_eos = add_eos\n        self.mask_type = mask_type\n        if \"hidden_size\" in kwargs:\n            self.hidden_size = kwargs[\"hidden_size\"]\n\n        super().__init__(**kwargs)\n\n\nclass LatentAttentionConfig(PretrainedConfig):\n    model_type = LATENT_ATTENTION_TYPE\n    is_composition = False\n    _name_or_path = \"latent_attention\"\n\n    def __init__(\n        self,\n        num_latents_value: int,\n        num_cross_heads: int,\n        hidden_dim: int,\n        latent_dim: int,\n        cross_dim_head: int,\n        mult: int,\n        **kwargs,\n    ):\n        self.num_latents_value = num_latents_value\n        self.num_cross_heads = num_cross_heads\n        self.hidden_dim = hidden_dim\n        self.latent_dim = latent_dim\n        self.cross_dim_head = cross_dim_head\n        self.mult = mult\n\n        super().__init__(**kwargs)\n\n\nAutoConfig.register(GIGAREMBED_TYPE, GigarEmbedConfig)\nAutoConfig.register(LATENT_ATTENTION_TYPE, LatentAttentionConfig)\n\nGigarEmbedConfig.register_for_auto_class()\nLatentAttentionConfig.register_for_auto_class()\n"
    },
    {
      "path": "weights/hf-cache/modules/transformers_modules/Giga_hyphen_Embeddings_hyphen_instruct/modeling_gigarembed.py",
      "language": "python",
      "size_bytes": 44149,
      "sha256": "d75a06519ffacd39bce2608ceedc56818c072430e1072f26f7aa0b570b3f7ba2",
      "content": "import copy\nimport logging\nfrom typing import Callable, List, Optional, Tuple, Union, Mapping\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\n\nfrom einops import rearrange, repeat\nfrom transformers import AutoModel, AutoTokenizer\n\nfrom transformers.cache_utils import Cache\nfrom transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import DynamicCache, StaticCache\nfrom transformers.generation import GenerationMixin\nfrom transformers.modeling_attn_mask_utils import AttentionMaskConverter\nfrom transformers.modeling_flash_attention_utils import FlashAttentionKwargs\nfrom transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\nfrom transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.processing_utils import Unpack\nfrom transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings\n\nfrom .configuration_gigarembed import GigarConfig, GigarEmbedConfig, LatentAttentionConfig\n\n\nlogger = logging.getLogger(__name__)\n_CONFIG_FOR_DOC = \"GigarEmbedConfig\"\n\n\nclass GigarMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, x):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n        return down_proj\n\n\nclass GigarRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        GigarRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n    def extra_repr(self):\n        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\ndef eager_attention_forward(\n    module: nn.Module,\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    attention_mask: Optional[torch.Tensor],\n    scaling: float,\n    dropout: float = 0.0,\n    **kwargs,\n):\n    key_states = repeat_kv(key, module.num_key_value_groups)\n    value_states = repeat_kv(value, module.num_key_value_groups)\n\n    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n    if attention_mask is not None:\n        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n        attn_weights = attn_weights + causal_mask\n\n    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n    attn_output = torch.matmul(attn_weights, value_states)\n    attn_output = attn_output.transpose(1, 2).contiguous()\n\n    return attn_output, attn_weights\n\n\nclass GigarLatentAttention(nn.Module):\n    \"\"\"\n    Multi-headed Latent Attention (MLA)\n\n    Check out the original paper: https://arxiv.org/pdf/2405.04434,\n    and the reference implementation: https://github.com/deepseek-ai/DeepSeek-V3/blob/main/inference/model.py\n    \"\"\"\n\n    def __init__(self, config: GigarConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.layer_idx = layer_idx\n        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n\n        assert config.num_attention_heads == config.num_key_value_heads, (\n            \"GQA for MLA is not supported (does it even make sense?)\"\n        )\n        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.apply_qk_norm = config.apply_qk_norm\n        self.attention_dropout = config.attention_dropout\n\n        assert config.mla_config is not None\n        self.qk_nope_head_dim = config.mla_config[\"qk_nope_head_dim\"]\n        self.qk_rope_head_dim = config.mla_config[\"qk_rope_head_dim\"]\n        self.v_head_dim = config.mla_config[\"v_head_dim\"]  # V has no rope part\n        self.kv_lora_rank = config.mla_config[\"kv_lora_rank\"]\n        self.q_lora_rank = config.mla_config[\"q_lora_rank\"]\n\n        self.qk_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim\n\n        self.scaling = self.qk_head_dim**-0.5\n\n        if self.q_lora_rank == 0:\n            self.q_proj = nn.Linear(\n                self.hidden_size,\n                self.num_heads * self.qk_head_dim,\n                bias=config.attention_bias,\n            )\n        else:\n            self.dq_proj = nn.Linear(\n                self.hidden_size,\n                self.q_lora_rank,\n                bias=config.attention_bias,\n            )\n            self.q_norm = GigarRMSNorm(self.q_lora_rank)\n            self.uq_proj = nn.Linear(\n                self.q_lora_rank,\n                self.num_heads * self.qk_head_dim,\n                bias=config.attention_bias,\n            )\n\n        self.kv_norm = GigarRMSNorm(self.kv_lora_rank)\n        self.dkv_proj = nn.Linear(\n            self.hidden_size,\n            self.kv_lora_rank,\n            bias=config.attention_bias,\n        )\n        self.uk_proj = nn.Linear(\n            config.kv_lora_rank,\n            self.num_heads * self.qk_nope_head_dim,\n            bias=config.attention_bias,\n        )\n        self.uv_proj = nn.Linear(\n            config.kv_lora_rank,\n            self.num_heads * self.v_head_dim,\n            bias=config.attention_bias,\n        )\n        self.kr_proj = nn.Linear(\n            self.hidden_size,\n            self.num_heads * self.qk_rope_head_dim,\n            bias=config.attention_bias,\n        )\n\n        self.o_proj = nn.Linear(\n            self.num_heads * self.v_head_dim,\n            self.hidden_size,\n            bias=config.attention_bias,\n        )\n\n        if self.apply_qk_norm:\n            self.qk_q_norm = nn.LayerNorm(self.num_heads * self.qk_head_dim, bias=False)\n            self.qk_k_norm = nn.LayerNorm(self.num_heads * self.qk_head_dim, bias=False)\n\n        config_for_rope = copy.copy(self.config)\n        config_for_rope.head_dim = self.config.qk_rope_head_dim\n\n        self.is_causal = False\n\n    def _compute_qkv(\n        self,\n        hidden_states: torch.Tensor,\n    ):\n        \"\"\"Compute query, key, and value tensors from hidden states.\"\"\"\n        bsz, seq_len, _ = hidden_states.size()\n\n        if self.q_lora_rank == 0:\n            query = self.q_proj(hidden_states)\n        else:\n            query = self.uq_proj(self.q_norm(self.dq_proj(hidden_states)))\n\n        latent = self.dkv_proj(hidden_states)\n        latent = self.kv_norm(latent)\n        k_rope = self.kr_proj(hidden_states)\n\n        k_nope = self.uk_proj(latent)\n        value = self.uv_proj(latent)\n\n        if self.apply_qk_norm:\n            query = self.qk_q_norm(query).to(query.dtype)\n            key = self.qk_k_norm(torch.cat([k_nope, k_rope], dim=-1)).to(k_nope.dtype)\n            k_nope, k_rope = torch.split(key, [k_nope.shape[-1], k_rope.shape[-1]], dim=-1)\n\n        # Reshape tensors\n        query = query.view(bsz, seq_len, self.num_heads, self.qk_head_dim).transpose(1, 2)\n        k_nope = k_nope.view(bsz, seq_len, self.num_heads, self.qk_nope_head_dim).transpose(1, 2)\n        k_rope = k_rope.view(bsz, seq_len, self.num_heads, self.qk_rope_head_dim).transpose(1, 2)\n        value = value.view(bsz, seq_len, self.num_heads, self.v_head_dim).transpose(1, 2)\n\n        q_nope, q_rope = torch.split(query, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n\n        return q_nope, q_rope, k_nope, k_rope, value\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n        attention_mask: Optional[torch.Tensor],\n        past_key_value: Optional[Cache] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n        \"\"\"\n        hidden_states: [bsz, seq_len, hidden_size]\n        attention_mask: [bsz, seq_len]\n        \"\"\"\n        batch_size, seq_len, _ = hidden_states.size()\n\n        q_nope, q_rope, k_nope, k_rope, value_states = self._compute_qkv(hidden_states)\n\n        # cos, sin = self.rotary_emb(q_rope, seq_len=seq_len)\n        cos, sin = position_embeddings\n        q_rope, k_rope = apply_rotary_pos_emb(q_rope, k_rope, cos, sin)\n        query_states = torch.cat([q_nope, q_rope], dim=-1)\n        key_states = torch.cat([k_nope, k_rope], dim=-1)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        attention_interface: Callable = eager_attention_forward\n        if self.config._attn_implementation != \"eager\":\n            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n\n        attn_output, attn_weights = attention_interface(\n            self,\n            query_states,\n            key_states,\n            value_states,\n            attention_mask,\n            dropout=0.0 if not self.training else self.attention_dropout,\n            scaling=self.scaling,\n            **kwargs,\n        )\n\n        attn_output = attn_output.reshape(batch_size, seq_len, -1).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        return attn_output, attn_weights\n\n\nclass GigarDecoderLayer(nn.Module):\n    def __init__(self, config: GigarConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n\n        self.self_attn = GigarLatentAttention(config, layer_idx)\n        self.mlp = GigarMLP(config)\n        self.input_layernorm = GigarRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = GigarRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n        **kwargs: Unpack[FlashAttentionKwargs],\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n            position_embeddings=position_embeddings,\n            **kwargs,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        return outputs\n\n\nclass GigarRotaryEmbedding(nn.Module):\n    def __init__(self, config: GigarConfig, device=None):\n        super().__init__()\n        # BC: \"rope_type\" was originally \"type\"\n        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n        else:\n            self.rope_type = \"default\"\n        self.max_seq_len_cached = config.max_position_embeddings\n        self.original_max_seq_len = config.max_position_embeddings\n\n        self.config = config\n        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n\n        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        self.original_inv_freq = self.inv_freq\n\n    def _dynamic_frequency_update(self, position_ids, device):\n        \"\"\"\n        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n        1 - growing beyond the cached sequence length (allow scaling)\n        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n        \"\"\"\n        seq_len = torch.max(position_ids) + 1\n        if seq_len > self.max_seq_len_cached:  # growth\n            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n            self.max_seq_len_cached = seq_len\n\n        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n            self.max_seq_len_cached = self.original_max_seq_len\n\n    @torch.no_grad()\n    def forward(self, x, position_ids):\n        if \"dynamic\" in self.rope_type:\n            self._dynamic_frequency_update(position_ids, device=x.device)\n\n        # Core RoPE block\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n\n        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n        cos = cos * self.attention_scaling\n        sin = sin * self.attention_scaling\n\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\nGIGAR_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`GigarConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare Gigar Model outputting raw hidden-states without any specific head on top.\",\n    GIGAR_START_DOCSTRING,\n)\nclass GigarPreTrainedModel(PreTrainedModel):\n    config_class = GigarConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"GigarDecoderLayer\"]\n    _skip_keys_device_placement = [\"past_key_values\"]\n    _supports_flash_attn_2 = True\n    _supports_sdpa = True\n    _supports_flex_attn = True\n    _supports_cache_class = True\n    _supports_quantized_cache = True\n    _supports_static_cache = True\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n\nGIGAR_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.n_positions - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n\n            Two formats are allowed:\n            - a [`~cache_utils.Cache`] instance, see our\n            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n            cache format.\n\n            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n            legacy cache format will be returned.\n\n            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n            of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n            the complete sequence length.\n\"\"\"\n\n@add_start_docstrings(\n    \"The bare Gigar Model outputting raw hidden-states without any specific head on top.\",\n    GIGAR_START_DOCSTRING,\n)\nclass GigarModel(GigarPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`GigarDecoderLayer`]\n\n    Args:\n        config: GigarConfig\n    \"\"\"\n\n    def __init__(self, config: GigarConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList(\n            [GigarDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n        )\n        self.norm = GigarRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.rotary_emb = GigarRotaryEmbedding(config=config)\n        self.gradient_checkpointing = False\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(GIGAR_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Cache] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n\n        if self.gradient_checkpointing and self.training and use_cache:\n            logger.warning_once(\n                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n            )\n            use_cache = False\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        if use_cache and past_key_values is None:\n            past_key_values = DynamicCache()\n\n        if cache_position is None:\n            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n            cache_position = torch.arange(\n                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n            )\n\n        if position_ids is None:\n            position_ids = cache_position.unsqueeze(0)\n        \n        attention_mask = self._update_encoder_mask(attention_mask, inputs_embeds)\n\n        hidden_states = inputs_embeds\n\n        # create position embeddings to be shared across the decoder layers\n        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n\n        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    attention_mask,  # causal_mask\n                    position_ids,\n                    past_key_values,\n                    output_attentions,\n                    use_cache,\n                    cache_position,\n                    position_embeddings,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,  # causal_mask\n                    position_ids=position_ids,\n                    past_key_value=past_key_values,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    cache_position=cache_position,\n                    position_embeddings=position_embeddings,\n                    **flash_attn_kwargs,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        output = BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=past_key_values if use_cache else None,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )\n        return output if return_dict else output.to_tuple()\n    \n    def _update_encoder_mask(\n        self,\n        attention_mask: torch.Tensor,\n        input_tensor: torch.Tensor,\n    ):\n        # Для flash_attention_2 возвращаем исходную маску\n        if self.config._attn_implementation == \"flash_attention_2\":\n            if attention_mask is not None and (attention_mask == 0).any():\n                return attention_mask\n            return None\n\n        dtype, device = input_tensor.dtype, input_tensor.device\n        batch_size, sequence_length = input_tensor.shape[:2]\n\n        # 1. Создаём базовую маску без ограничений (все токены видят друг друга)\n        encoder_mask = torch.full(\n            (batch_size, 1, sequence_length, sequence_length),\n            fill_value=1.0,\n            dtype=dtype,\n            device=device\n        )\n\n        # 2. Применяем padding-маску если есть\n        if attention_mask is not None:\n            # Создаём 4D padding-маску [batch, 1, 1, seq_len]\n            padding_mask = attention_mask[:, None, None, :].to(dtype=dtype)\n\n            # Комбинируем: обнуляем позиции где padding_mask == 0\n            encoder_mask = encoder_mask * padding_mask\n\n            # Конвертируем в формат для softmax (0 = -inf)\n            min_dtype = torch.finfo(dtype).min\n            encoder_mask = encoder_mask.masked_fill(encoder_mask == 0.0, min_dtype)\n\n        return encoder_mask\n\n    def _update_causal_mask(\n        self,\n        attention_mask: torch.Tensor,\n        input_tensor: torch.Tensor,\n        cache_position: torch.Tensor,\n        past_key_values: Cache,\n        output_attentions: bool,\n    ):\n        if self.config._attn_implementation == \"flash_attention_2\":\n            if attention_mask is not None and (attention_mask == 0.0).any():\n                return attention_mask\n            return None\n\n        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n        # to infer the attention mask.\n        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n        using_static_cache = isinstance(past_key_values, StaticCache)\n\n        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                attention_mask,\n                inputs_embeds=input_tensor,\n                past_key_values_length=past_seen_tokens,\n                is_training=self.training,\n            ):\n                return None\n\n        dtype, device = input_tensor.dtype, input_tensor.device\n        sequence_length = input_tensor.shape[1]\n        if using_static_cache:\n            target_length = past_key_values.get_max_cache_shape()\n        else:\n            target_length = (\n                attention_mask.shape[-1]\n                if isinstance(attention_mask, torch.Tensor)\n                else past_seen_tokens + sequence_length + 1\n            )\n\n        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n            attention_mask,\n            sequence_length=sequence_length,\n            target_length=target_length,\n            dtype=dtype,\n            device=device,\n            cache_position=cache_position,\n            batch_size=input_tensor.shape[0],\n        )\n\n        if (\n            self.config._attn_implementation == \"sdpa\"\n            and attention_mask is not None\n            and attention_mask.device.type == \"cuda\"\n            and not output_attentions\n        ):\n            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n            # Details: https://github.com/pytorch/pytorch/issues/110213\n            min_dtype = torch.finfo(dtype).min\n            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n\n        return causal_mask\n\n    @staticmethod\n    def _prepare_4d_causal_attention_mask_with_cache_position(\n        attention_mask: torch.Tensor,\n        sequence_length: int,\n        target_length: int,\n        dtype: torch.dtype,\n        device: torch.device,\n        cache_position: torch.Tensor,\n        batch_size: int,\n        **kwargs,\n    ):\n        \"\"\"\n        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n\n        Args:\n            attention_mask (`torch.Tensor`):\n                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n                `(batch_size, 1, query_length, key_value_length)`.\n            sequence_length (`int`):\n                The sequence length being processed.\n            target_length (`int`):\n                The target length: when generating with static cache, the mask should be as long as the static cache,\n                to account for the 0 padding, the part of the cache that is not filled yet.\n            dtype (`torch.dtype`):\n                The dtype to use for the 4D attention mask.\n            device (`torch.device`):\n                The device to plcae the 4D attention mask on.\n            cache_position (`torch.Tensor`):\n                Indices depicting the position of the input sequence tokens in the sequence.\n            batch_size (`torch.Tensor`):\n                Batch size.\n        \"\"\"\n        if attention_mask is not None and attention_mask.dim() == 4:\n            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n            causal_mask = attention_mask\n        else:\n            min_dtype = torch.finfo(dtype).min\n            causal_mask = torch.full(\n                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n            )\n            if sequence_length != 1:\n                causal_mask = torch.triu(causal_mask, diagonal=1)\n            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n            if attention_mask is not None:\n                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                mask_length = attention_mask.shape[-1]\n                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n                padding_mask = padding_mask == 0\n                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                    padding_mask, min_dtype\n                )\n\n        return causal_mask\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, mult = 4):\n        super().__init__()\n        self.hidden_size = dim\n        self.intermediate_size = dim * mult\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n        self.act_fn = nn.SiLU()\n\n    def forward(self, x):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass Attention(nn.Module):\n    def __init__(self, query_dimension, context_dimension=None, num_heads=8, head_dim=64):\n        super().__init__()\n        inner_dimension = head_dim * num_heads\n        context_dimension = context_dimension if context_dimension is not None else query_dimension\n\n        self.scaling_factor = head_dim ** -0.5\n        self.num_heads = num_heads\n\n        self.to_q = nn.Linear(query_dimension, inner_dimension, bias=False)\n        self.to_kv = nn.Linear(context_dimension, inner_dimension * 2, bias=False)\n        self.to_out = nn.Linear(inner_dimension, query_dimension, bias=False)\n\n    def forward(self, input_tensor, context=None, attention_mask=None):\n        batch_size, seq_len, _ = input_tensor.shape\n        num_heads = self.num_heads\n\n        # Project input to query\n        query = self.to_q(input_tensor)\n\n        # Use input as context if not provided\n        context = input_tensor if context is None else context\n        key, value = self.to_kv(context).chunk(2, dim=-1)\n\n        # Rearrange for multi-head attention\n        query = rearrange(query, 'b n (h d) -> (b h) n d', h=num_heads)\n        key = rearrange(key, 'b n (h d) -> (b h) n d', h=num_heads)\n        value = rearrange(value, 'b n (h d) -> (b h) n d', h=num_heads)\n\n        # Compute scaled dot-product attention\n        with torch.backends.cuda.sdp_kernel(\n            enable_flash=True, \n            enable_math=True, \n            enable_mem_efficient=True\n        ):\n            attention_output = F.scaled_dot_product_attention(query, key, value)\n\n        # Rearrange back to original shape\n        attention_output = rearrange(attention_output, '(b h) n d -> b n (h d)', h=num_heads)\n\n        return self.to_out(attention_output)\n\n\nclass LatentAttentionModel(PreTrainedModel):\n    config_class = LatentAttentionConfig\n\n    def __init__(self, configuration: LatentAttentionConfig):\n        super().__init__(configuration)\n\n        # Extract configuration parameters\n        num_latents = configuration.num_latents_value\n        latent_dimension = configuration.latent_dim\n        cross_attention_heads = configuration.num_cross_heads\n        cross_head_dimension = configuration.cross_dim_head\n        hidden_dimension = configuration.hidden_dim\n\n        # Initialize cross-attention components\n        self.cross_attend_blocks = nn.ModuleList([\n            Attention(\n                query_dimension=latent_dimension,\n                context_dimension=hidden_dimension,\n                num_heads=cross_attention_heads,\n                head_dim=cross_head_dimension\n            ),\n            FeedForward(latent_dimension)\n        ])\n\n        # Register learnable latents as model parameter\n        self.latents = nn.Parameter(torch.randn(num_latents, latent_dimension))\n\n    def forward(self, hidden_states, attention_mask: Optional[torch.Tensor] = None):\n        cross_attention, feed_forward = self.cross_attend_blocks\n        \n        batch_size, device = hidden_states.size(0), hidden_states.device\n        \n        # Expand latents to match batch size\n        expanded_latents = self.latents.repeat(batch_size, 1, 1)\n        \n        # Apply cross-attention with residual connection\n        attended_output = cross_attention(\n            hidden_states, context=expanded_latents, attention_mask=attention_mask) + hidden_states\n        \n        # Apply feed-forward with residual connection\n        processed_output = feed_forward(attended_output) + attended_output\n        \n        return processed_output\n\n\nclass GigarEmbedModel(PreTrainedModel):\n    config_class = GigarEmbedConfig\n    _supports_flash_attn_2 = True\n    _no_split_modules = [\"GigarDecoderLayer\", \"LatentAttentionModel\"]\n    \n    def __init__(self, configuration: GigarEmbedConfig):\n        super().__init__(configuration)\n        \n        # Initialize latent attention model\n        self.latent_attention_model = AutoModel.from_config(\n            configuration.latent_attention_config\n        )\n\n        self.tokenizer, self.text_encoder = None, None\n        if configuration.text_config is not None:\n            # Initialize text model if provided in config\n            self.model = AutoModel.from_config(configuration.text_config)\n\n            # Initialize tokenizer if text config is available\n            self.tokenizer = AutoTokenizer.from_pretrained(\n                configuration.name_or_path\n            )\n        \n        # Set configuration parameters\n        self.padding_side = configuration.padding_side\n        self.add_eos = configuration.add_eos\n        self.mask_type = configuration.mask_type\n        \n        # Add padding token if configured\n        if configuration.add_pad_token and self.tokenizer is not None:\n            self.add_pad_token()\n\n    def add_pad_token(self):\n        self.tokenizer.pad_token_id = 0\n        self.tokenizer.padding_side = self.padding_side\n\n    def gradient_checkpointing_enable(self, *args, **kwargs):\n        self.model.gradient_checkpointing_enable(*args, **kwargs)\n\n    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, \n                return_embeddings: bool = False, **kwargs):\n        kwargs.pop('token_type_ids', None)\n\n        with torch.autocast('cuda', dtype=torch.bfloat16):\n            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n\n            last_hidden = self.latent_attention_model(outputs.last_hidden_state, attention_mask)\n\n        if return_embeddings:\n            return self.mean_pool(last_hidden, attention_mask)\n\n        return BaseModelOutputWithPast(last_hidden_state=last_hidden)\n\n    def mean_pool(self, last_hidden: torch.Tensor, attention_mask: torch.Tensor):\n        last_hidden = last_hidden.masked_fill(~attention_mask[..., None].bool(), 0.0)\n        embeddings = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n        return F.normalize(embeddings, p=2, dim=-1)\n\n\n## AutoModel Register\nAutoModel.register(GigarConfig, GigarModel)\nAutoModel.register(GigarEmbedConfig, GigarEmbedModel)\nAutoModel.register(LatentAttentionConfig, LatentAttentionModel)\n\n## Register for auto class\nGigarModel.register_for_auto_class(\"AutoModel\")\nGigarEmbedModel.register_for_auto_class(\"AutoModel\")\nLatentAttentionModel.register_for_auto_class(\"AutoModel\")\n"
    },
    {
      "path": "weights/hf-cache/modules/transformers_modules/__init__.py",
      "language": "python",
      "size_bytes": 0,
      "sha256": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",
      "content": ""
    }
  ]
}